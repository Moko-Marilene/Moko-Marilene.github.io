---
title: "Statistiques des risques extr√™mes"
author:
  - "DEBA WANDJI Franck Delma"
  - "KOUGOUM MOKO MANI Marilene"
format:
  html:
    code-fold: true
jupyter: python3
---

[‚¨Ö Retour](../index.qmd)

# Introduction

Ce projet a pour objectif d‚Äô√©tudier la mesure du risque de pertes extr√™mes √† partir d‚Äôune s√©rie historique de log-rendements boursiers, en combinant :

- des approches classiques de VaR non param√©trique et gaussienne,
- des m√©thodes plus avanc√©es bas√©es sur la pond√©ration EWMA,
- des lois de probabilit√© √† queues √©paisses (Skew-Student),
- la th√©orie des valeurs extr√™mes (TVE : GEV, GPD, POT),
- et enfin des mod√®les dynamiques de volatilit√© (AR(1)-GARCH(1,1)) pour construire une VaR dynamique et backtest√©e.

Plus sp√©cifiquement, il s‚Äôagira de :

comparer les diff√©rentes approches de VaR en termes de r√©alisme et de couverture du risque ;
mettre en place un protocole de backtesting adaptatif permettant de recalibrer les mod√®les lorsque ceux-ci ne d√©crivent plus correctement le risque observ√©.
***

[Lien vers l'application Dash (GitHub)](https://github.com/usenameFD/STATISTIQUES-RISQUES-EXTREMES.git)

***

***

## Recuperation des donn√©es et analyse des donn√©es


```{python}
import yfinance as yf
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.stats.diagnostic import acorr_ljungbox
from arch import arch_model
from scipy import stats
from scipy.stats import jarque_bera
from scipy.stats import shapiro

import functions
import importlib
import warnings
warnings.filterwarnings("ignore")


ticker_symbol = "^FCHI"
df_close = yf.download(ticker_symbol, start="2008-10-15", end = "2024-06-11")["Close"]
df_close.columns = ['Close']
df_close.head(2)
```

#### Exploration des donn√©es √† notre disposition


***
V√©rification des valeurs manquantes

```{python}

missing_values = df_close.isnull().sum()
print(f"Nombre de valeurs manquantes dans la colonne 'Close': {missing_values[0]}")
```

R√©sum√© descriptif des donn√©es

```{python}
summary = df_close['Close'].describe()
print("\nR√©sum√© descriptif des prix de cl√¥ture :")
print(summary)
```

Calcul des log return

```{python}
df_close['Log Return'] = np.log(df_close['Close'] / df_close['Close'].shift(1))
df_close = df_close.dropna()
df_close.head(2)
```

Visualisation de l'√©volution des prix de cl√¥ture ainsi que celle des log_return sur la p√©riode d'√©tude

```{python}
# Cr√©ation du graphique
fig, ax1 = plt.subplots(figsize=(12, 6))

# Evolution de "Close" sur l'axe primaire
ax1.plot(df_close.index, df_close['Close'], color='blue', label='Close', alpha=0.7)
ax1.set_xlabel('Date')
ax1.set_ylabel('Prix de Cl√¥ture', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Cr√©ation d'un deuxi√®me axe y pour les log returns
ax2 = ax1.twinx()
ax2.plot(df_close.index, df_close['Log Return'], color='red', label='Log Return', alpha=0.7)
ax2.set_ylabel('Log Return', color='red')
ax2.tick_params(axis='y', labelcolor='red')

# Ajout des titres et l√©gendes
plt.title("√âvolution des prix de cl√¥ture et des Log Returns (Eurostocks50)")
fig.tight_layout()  
plt.show()
```

On observe dej√† une tendance tr√®s marqu√©e et haussiere au niveau des prix de cl√¥ture, signe que cette variable n'est pas stationnaire.

Les rendements en revanche semblent pr√©senter une certaine regularit√© autour de la moyenne

***

Division de nos donn√©es en echantillon de train et de test

- Apprentissage : 15 octobre 2008 - 26 juillet 2022
- Test : 27 juillet 2022 - 11 juin 2024

```{python}

train_start, train_end = "2008-10-15", "2022-07-26"  # la BCE a commenc√© √† baisser ses taux directeurs
test_start, test_end = "2022-07-27", "2024-06-11"   # la BCE a commenc√© √† relever progressivement ses taux directeurs


train_data = df_close.loc[train_start:train_end]
test_data = df_close.loc[test_start:test_end]

# Afficher les tailles des ensembles
print(f"Taille de l'ensemble d'entra√Ænement: {train_data.shape[0]} jours")
print(f"Taille de l'ensemble de test: {test_data.shape[0]} jours")
```

***

#### Statistiques descriptives sur les 2 jeux de donn√©es

Nous nous int√©ressons aux caract√©ristiques de tendance centrale et de dispersion usuelles et, du fait du constat fait sur le graphique de l'√©volution des prix de cloture et des log-rendements sur la p√©riode d'√©tude, nous r√©alisons √©galement des test de stationnarit√© √† savoir : le test de Dick et Fuller augment√© et le test de KPSS.

***

##### Echantillon train

```{python}
importlib.reload(functions)
resultat = functions.descriptive_statistics(train_data, train_data.columns)
resultat
```

```{python}
functions.plot_histograms(train_data, train_data.columns)
```

**Commentaire**

Sur l'√©chantillon de train, les prix sont non stationnaires, et assez variables, avec un √©cart-type de 1022 pour une moyenne de 4601, soit pr√®s de 1/4 de la valeur moyenne. 

Les log rendements en revanche pr√©sentent une certaine r√©gularit√© autour de leur moyenne (0.000173) comme l'attestent les tests de stationnarit√© de Dick et Fuller augment√© et de KPSS. On note toute fois une asym√©trie marqu√©e (-0.26) et des queues lourdes (avec un excess kurtosis de 7.30) contrairement aux prix de cl√¥ture

***

##### Echantillon test

```{python}
stat_test = functions.descriptive_statistics(test_data, test_data.columns)
stat_test
```

```{python}
functions.plot_histograms(test_data, test_data.columns)
```

**Commentaire**

Les caract√©ristiques de stationnarit√© sont similaires entre l'√©chantillon de test et l'√©chantillon d'entra√Ænement.

Cependant, l'√©chantillon de test pr√©sente en moyenne des prix et des rendements logarithmiques plus √©lev√©s, tout en √©tant moins dispers√©s.

En ce qui concerne les distributions des rendements logarithmiques, les queues de l'√©chantillon de test sont moins lourdes que celles de l'√©chantillon d'entra√Ænement. Toutefois, elles restent plus √©paisses que celles d'une loi normale, comme l'indique un exc√®s de kurtosis de 1,96.

## 	VaR non param√©trique


### VaR Historique

Ecrire une fonction calculant la VaR historique d‚Äôun ensemble de log-rendements : VaR_Hist(x, alpha)

Calcul de la VaR historique √† horizon 1 jour  sur base d‚Äôapprentissage pour alpha = 99%.

```{python}
importlib.reload(functions)
var_99_hist = functions.VaR_Hist(train_data['Log Return'])
print(f"VaR historique √† 99% sur un  horizon d'un jour : {var_99_hist:.4%}")
```

### VaR Bootstrap



Ecrire une fonction calculant la VaR historique bootstrap d‚Äôun ensemble de log-rendements et donnant un IC de niveau alpha_IC de cette VaR

Nous choisissons de prendre la m√©diane des VaR des differents echantillons bootstrap√©s car elle est plus robuste que la moyenne.
Nous choississons un nombre d'it√©ration suffisamment grand pour nous assurer de la pertinence de la VaR estim√©e. On choisit √©galement N tel que NŒ± soit divisible par 2 afin de pouvoir extraire des indices exacts pour les quantiles d‚Äôordre
NŒ±/2 et 1‚àíNŒ±/2.

***
Calcul  de  la VaR historique bootstrap et l‚ÄôIC associ√© √† 90% sur base d‚Äôapprentissage pour alpha = 99%.

```{python}
importlib.reload(functions)
var_bootstrap_99, var_ci = functions.VaR_Hist_Bootstrap(train_data['Log Return'],confidence_level=0.99, n_iterations=10000, ci_level=0.90)

# Affichage des r√©sultats
print(f"VaR Bootstrap √† 99% sur un jour : {var_bootstrap_99:.4%}")
print(f"Intervalle de confiance √† 95% : [{var_ci[0]:.4%}, {var_ci[1]:.4%}]")
```


Calcul du nombre d‚Äôexceptions sur base de test associ√©es √† la VaR historique calcul√©e pr√©c√©demment

```{python}
importlib.reload(functions)
functions.var_exceptions(test_data, var_99_hist)
```


Comparer statistiquement ce pourcentage d‚Äôexceptions avec le niveau de risque attendu

Nous mettons ici en place le test statistique **d'unconditional coverage**.

### Test d'unconditionnal coverage : Pr√©sentation du principe 


__Pr√©sentation du principe du test d'unconditional coverage__




Le test d'__Unconditional Coverage__ √©value si la proportion d'exc√®s observ√©s (c'est-√†-dire les rendements inf√©rieurs √† la VaR) correspond √† la probabilit√© th√©orique de d√©passement, $\alpha$ (1% dans notre cas).

- **Hypoth√®se nulle (H‚ÇÄ)** : La proportion d'exc√®s observ√©e est √©gale √† $\alpha$.
  
- **Statistique de test** : Le test est bas√© sur le rapport de vraisemblance (Likelihood Ratio, LR) :
  
  $$
  LR_{\text{uc}} = -2 \cdot \log\left( (1 - \alpha)^{n - n_{\text{exc√®s}}} \cdot \alpha^{n_{\text{exc√®s}}} \right) + 2 \cdot \log\left( (1 - \hat{p}_{\text{emp}})^{n - n_{\text{exc√®s}}} \cdot \hat{p}_{\text{emp}}^{n_{\text{exc√®s}}} \right)
  $$

  o√π :
  - $n$ est le nombre total d'observations,
  - $n_{\text{exc√®s}}$ est le nombre d'exc√®s (jours o√π le rendement est inf√©rieur √† la VaR),
  - $\hat{p}_{\text{emp}}$ est la proportion empirique d'exc√®s observ√©e.

- **p-value** : La statistique suit une loi du chi-deux √† 1 degr√© de libert√©. La p-value est obtenue comme suit :
  
  $$
  p_{\text{uc}} = 1 - \chi^2_{\text{cdf}}(LR_{\text{uc}}, df=1)
  $$

- **Conclusion** : Si $p_{\text{uc}} > \text{seuil de significativit√©}$ (nous avons choisi 0.05), on ne rejette pas l'hypoth√®se nulle et la proportion d'exc√®s observ√©e est coh√©rente avec la probabilit√© th√©orique $\alpha$. Sinon, on rejette l'hypoth√®se nulle.

***

**Resultat du test**

```{python}
importlib.reload(functions)
LR_uc, p_value = functions.unconditional_coverage_test(test_data, var_99_hist, significance_level=0.01)
```

## VaR Gaussienne

***

#### Ecrire une fonction calculant la VaR gaussienne d‚Äôun ensemble de log-rendements

Calcul de la VaR gaussienne sur base d‚Äôapprentissage pour alpha = 99%.

```{python}
importlib.reload(functions)

var_gauss = functions.VaR_Gauss(train_data, alpha=0.99)
print(f"VaR Gaussienne √† 99% sur un jour : {var_gauss:.4%}")
```

#### Faire une validation ex-ante de cette VaR Gaussienne (analyses graphiques, QQ-plot, etc.)

***

**Comparaison de la courbe densit√© empirique des log return et de la courbe de densit√© th√©orique suppos√©e**

```{python}
importlib.reload(functions)
functions.plot_kde_vs_gauss(train_data)
```

On observe que la distribution r√©elle est plus concentr√©e au centre et pr√©sente des queues plus √©paisses que la loi normale suppos√©e. Ceci indique une fr√©quence plus √©lev√©e d‚Äô√©v√©nements extr√™mes que ne le pr√©voit la loi normale. Ainsi, A priori, notre VaR gaussienne √† 99% sous-estime le risque r√©el.

Nous utilisons un QQ-plot afin de conforter cette observation

***
**QQ-plot**

```{python}
importlib.reload(functions)
functions.plot_qq(train_data['Log Return'])
```

Le QQ-plot compare les quantiles empiriques des log-returns √† ceux d‚Äôune distribution normale. On observe que :

-   Au centre de la distribution, les points suivent approximativement la droite th√©orique, ce qui indique une l√©g√®re conformit√© √† la normalit√©.

-   Aux extr√©mit√©s, les points s‚Äô√©cartent fortement de la droite, r√©v√©lant des queues √©paisses

Il semblerait donc bien que la loi normale ne soit pas celle adapt√©e √† nos donn√©es. 

Nous confortons ces resultats visuels √† l'aide de tests statistiques. Nous utilisons un test statistique d'ad√©quation (Kolmogorov-smirnov)

**Test de Kolmogorov Smirnov**

```{python}
importlib.reload(functions)
functions.ks_test(train_data['Log Return'])
```

Le test d'ad√©quation corrobore les observations faites pr√©c√©demment. On s'attend donc √† ce que la Var ici calibr√©e ne passe pas le test d'unconditional coverage.

**Test d'unconditional coverage**

```{python}
functions.unconditional_coverage_test(train_data, var_gauss, significance_level=0.01)
```

Effectivemment , cette VaR ne couvre pas correctement les exceptions: **Elle sous-estime le risque de perte**

***

####  Calcul de  la VaR gaussienne √† 10j par la m√©thode de scaling

***

La VaR gaussienne √† 10 jours est estim√©e en multipliant la VaR quotidienne par la racine carr√©e du nombre de jours (10 jours dans ce cas)

$$
\text{VaR}_{10j} = \text{VaR}_{1j} \times \sqrt{10}
$$

o√π :
- $\text{VaR}_{1j}$ est la VaR calcul√©e sur une p√©riode de 1 jour,
- $\sqrt{10}$ est le facteur de scaling bas√© sur le nombre de jours (10 jours ici).

```{python}
importlib.reload(functions)
var_10j = functions.var_gauss_horizon(var_gauss, horizon=10)
```

***

#### Calcul de la VaR Gaussienne √† 10 jours par la m√©thode de diffusion d'un actif

***

La VaR gaussienne √† 10 jours peut √™tre calcul√©e en utilisant la m√©thode de diffusion d'un actif, bas√©e sur le mod√®le suivant pour l'√©volution du prix de l'actif :

$$
dS = S \cdot \mu \cdot dt + S \cdot \sigma \cdot Z \cdot \sqrt{dt}
$$

o√π :
- $Z \sim N(0, 1)$ (bruit blanc suivant une loi normale standard),
- $S_0$ est la valeur du cours de cl√¥ture de l'actif √† la derni√®re date de l'√©chantillon d'apprentissage,
- $dt = 1 \, \text{jour}$,
- $\mu$ et $\sigma$ sont les param√®tres estim√©s dans la section 2.b (moyenne et √©cart-type des rendements log-transform√©s).

Cette m√©thode permet d'estimer la VaR en tenant compte de la dynamique de diffusion du prix de l'actif.

***

La formule utilis√©e pour simuler les trajectoires du prix est obtenue en resolvant l'EDS pr√©sent√©e plus haut: 

$S_t = S_{t-1} \cdot \exp\left(\left(\mu - 0.5 \cdot \sigma^2\right) \cdot \Delta t + \sigma \cdot Z\right)$

Ici, $ \Delta t = 1 $ car la simulation est effectu√©e sur des √©tapes d'une unit√© de temps (1 jour).

```{python}
importlib.reload(functions)

## Param√®tres
S0 = float(train_data[['Close']].iloc[-1])
mu = np.mean(train_data['Log Return'])
sigma = np.std(train_data['Log Return'])
t = 10
num_simulations = 1000


St = functions.simulate_price_paths(t, S0, mu, sigma, num_simulations)
log_returns = functions.calculate_log_returns(St, S0)

# Calcul de la VaR  √† 99%
var_99 = functions.calculate_var(log_returns)
```

***
#####  Aper√ßu de quelques trajectoires de prix

***

```{python}
importlib.reload(functions)
num_trajectoires =10
functions.plot_simulations(St, num_trajectoires, t)
```

### Calcul de la VaR Gaussienne √† 1 jour avec EWMA

***
La **VaR Gaussienne** √† 1 jour est calcul√©e en surpond√©rant les observations les plus r√©centes avec la m√©thode **EWMA** (Exponential Weighted Moving Average).

Les poids sont d√©finis par :
- $ \omega_i (\lambda) = \lambda^i (1 - \lambda) $, o√π $ \lambda $ est le param√®tre de lissage et $ i $ l'indice de l'observation.

Les poids normalis√©s sont :
- $ \tilde{\omega}_i (\lambda) = \frac{\omega_i (\lambda)}{\sum_{i=0}^{T} \omega_i (\lambda)} $.

La moyenne et la variance sont calcul√©es comme suit :
- $ \hat{\mu}(\lambda) = \sum_{i=0}^{T} \tilde{\omega}_i (\lambda) r_{T-i} $,
- $ \hat{\sigma}^2 (\lambda) = \sum_{i=0}^{T} \tilde{\omega}_i (\lambda) (r_{T-i} - \hat{\mu} (\lambda))^2 $.

Cela permet de calculer la VaR en utilisant les rendements pond√©r√©s, avec une importance plus grande pour les rendements r√©cents.


***

**Calcul de mu et sigma ewma pour differentes valeurs de lambda**

```{python}
importlib.reload(functions)
lambdas = [ 0.9 , 0.95 , 0.99  ]
for lambda_ in lambdas:
    result = functions.calculate_mu_sigma_ewma(train_data['Log Return'], lambda_)

print("L'ecart-type non pond√©r√© des log rendements dans l'echantillon train est : ", train_data['Log Return'].std())
print('-'*50)
print("La moyenne non pond√©r√©e des log rendements dans l'echantillon train est : ", train_data['Log Return'].mean())
print('-'*50)
```

**Commentaire**

***

Plus Œª est grand, plus on lisse, et plus on se concentre sur les donn√©es r√©centes.

Et des r√©sultats pr√©c√©dents, en parcourant ces valeurs de Œª, on observe que les log-rendements r√©cents semblent moins n√©gatifs et moins volatils lorsque lambda grandit, ce qui se refl√®te dans une moyenne ŒºÃÇ qui remonte l√©g√®rement (de -0.0019 √† -0.0005) et une volatilit√© œÉÃÇ qui diminue (de 0.0409 √† 0.0273).

Cependant, ces √©carts-types pond√©r√©s restent nettement sup√©rieurs √† l‚Äô√©cart-type non pond√©r√© de l‚Äô√©chantillon (‚âà 0.0139).

De plus, alors que la moyenne non pond√©r√©e est  positive (‚âà 0.00017), les moyennes pond√©r√©es sont toutes n√©gatives, ce qui indique une tendance √† la perte dans les observations r√©centes .

On s‚Äôattend donc √† une VaR plus s√©v√®re que celle issue d‚Äôune approche non pond√©r√©e, car les r√©centes performances sont √† la fois plus volatiles et plus n√©gatives. Cela dit, √† mesure que Œª augmente, la VaR devrait d√©cro√Ætre, en coh√©rence avec une baisse de la volatilit√© estim√©e.


***

**Calcul de la VaR gaussienne selon la m√©thode EWMA**

***

Pour calculer la VaR gaussienne √† 99% selon la m√©thode EWMA, on utilise les param√®tres estim√©s $ \hat{\mu}(\lambda) $ et $ \hat{\sigma}^2(\lambda) $. La VaR est ensuite calcul√©e √† l'aide de la formule suivante :

$ \text{VaR}_{\text{EWMA}} = \hat{\mu}(\lambda) - \text{Quantile}_{1-\alpha} \times \hat{\sigma}(\lambda) $

```{python}
importlib.reload(functions)
for lambda_ in lambdas:
    var_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], lambda_, alpha=0.99)
```

Les resultats observ√©s corroborent bien notre analyse faite pr√©c√©demment. Les pertes decroissent √† mesure que lambda grandit. Mais, elles demeurent bien plus s√©v√®res  que celle observ√©e sans pond√©ration.

***
On constate par ailleurs que ces VaR pond√©r√©es sont toutes plus s√©v√®res que la VaR historique calcul√©e pr√©c√©demment, on s'attend donc √† ce qu'il n'y ait pas d'exception et que le test d'unconditionnal coverage ne soit pas satisfait.

Pour s'en convaincre, ex√©cutons le code qui calcule le nombre d'exception et fait le test.

```{python}
### Pour lambda = 0.9

var_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], 0.9, alpha=0.99)
LR_uc, p_value = functions.unconditional_coverage_test(test_data, var_ewma, significance_level=0.01)
print("-"*40)
```

```{python}
### Pour lambda = 0.95

var_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], 0.95, alpha=0.99)
LR_uc, p_value = functions.unconditional_coverage_test(test_data, var_ewma, significance_level=0.01)
print("-"*40)
```

```{python}
### Pour lambda = 0.99

var_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], 0.99, alpha=0.99)
LR_uc, p_value = functions.unconditional_coverage_test(test_data, var_ewma, significance_level=0.01)
print("-"*40)
```

***
##### Que peut-on retenir √† ce niveau?


La VaR gaussienne calcul√©e sans pond√©ration sous-estime le risque de perte. En effet, le taux de d√©passement observ√© est de 2.21 %, bien sup√©rieur au seuil th√©orique de 1 % attendu pour une VaR √† 99 %. 

Pour y rem√©dier, le recours √† une VaR gaussienne avec pond√©ration EWMA pourrait √™tre pertinent, car elle permet d‚Äôaccorder plus d‚Äôimportance aux observations r√©centes. Toutefois, lorsqu‚Äôon choisit un Œª trop √©lev√©, on observe une VaR excessivement conservatrice. Dans ce cas, la mesure de risque devient trop sensible aux rendements r√©cents, au point de perdre en repr√©sentativit√© globale.

**Conclusion** : Il est donc essentiel de trouver un Œª adapt√©, capable d‚Äôassurer un bon compromis entre r√©activit√© et stabilit√©. Un Œª trop faible dilue l'information r√©cente, tandis qu‚Äôun Œª trop √©lev√© amplifie les variations court terme. Le choix optimal de Œª permettrait ainsi une √©valuation plus r√©aliste et efficace du risque de perte.

## 	VaR skew-Student

***

Estimation des param√®tres d'une loi de Skew Student par maximum de vraisemblance


Nous allons estimer les param√®tres d'une loi de Skew Student en utilisant la m√©thode du maximum de vraisemblance. Nous proc√©derons par √©tapes :

- **D√©finition de la fonction de densit√©** 
- **Fonction de log-vraisemblance** 
- **Optimisation de la log-vraisemblance**

***

*Rappel*

Soit $X$ une variable al√©atoire suivant une loi Skew Student, avec les param√®tres $\\mu$ (moyenne), $\\sigma$ (√©cart-type), $\\gamma$ (param√®tre de skewness) et $\\nu$ (degr√©s de libert√©).

La fonction de densit√© est donn√©e par la formule suivante :

$$
f_{\\{skew-student}}(x) = 2 \times f(x) \times F(x)
$$

o√π :

- $f(x)$ est la fonction de densit√© de la loi de Student avec $\\nu$ degr√©s de libert√©,
- $F(x)$ est la fonction de r√©partition (CDF) de la loi de Student avec $\\nu + 1$ degr√©s de libert√©.

***

### Estimation des param√®tres de loi Skew Student sur base d‚Äôapprentissage.

```{python}

importlib.reload(functions)
log_returns = train_data['Log Return'].values  

# Estimation des param√®tres optimaux
params_optimaux = functions.optimize_parameters(log_returns)

if params_optimaux is not None:
    mu_opt, sigma_opt, gamma_opt, nu_opt = params_optimaux
    print(f"Param√®tres estim√©s de la loi Skew Student :")
    print(f" - Moyenne (mu) : {mu_opt}")
    print(f" - Ecart-type (sigma) : {sigma_opt}")
    print(f" - Skewness (gamma) : {gamma_opt}")
    print(f" - Degr√©s de libert√© (nu) : {nu_opt}")
else:
    print("L'optimisation a √©chou√©.")
```

####  Mise en place  d'une validation ex-ante de cet ajustement

***

**Comparaison des fonctions de densit√©**

```{python}
importlib.reload(functions)
functions.plot_skew_student_fit(log_returns, mu_opt, sigma_opt, gamma_opt, nu_opt, functions.f_skew_student)
```

La loi skew-student semble assez bien s'ajuster √† nos donn√©es. On peut cependant noter que, au niveau des queues de distribution, la densit√© th√©orique est l√©g√®rement en dessous de la densit√© empirique. 

Mais, ici, la qualit√© de l'ajustement semble bien meilleure que celle avec la loi normale. 

On s'int√©resse au QQ-plot.

***

**Evaluation de la qualit√© de l'ajustement aux donn√©es via des QQ-plot**

```{python}
importlib.reload(functions)
T = len(train_data['Log Return'])
df_simulated = functions.skew_student_sim(mu_opt, sigma_opt, gamma_opt, nu_opt, T)
df_observed = train_data['Log Return']

fig = functions.qqplot(df_observed, df_simulated)
plt.show()
```

L'observation du QQ-plot, vient confirmer le constat fait pr√©c√©demment avec les courbes de densit√©s supperpos√©es.

La loi skew student serait donc mieux adapt√© √† nos donn√©es qu'une loi gaussienne.

Pour que ceci soit plus visuel, nous allons afficher c√¥te √† c√¥te les QQ-plots pour ces deux lois.

***

#### Comparaison de la qualit√© de fit entre loi gaussienne et loi de skew Student par analyse graphique.

```{python}
importlib.reload(functions)

# Param√®tres des lois
mu_gauss, sigma_gauss = np.mean(train_data['Log Return']), np.std(train_data['Log Return'])
mu_skew, sigma_skew, gamma_skew, nu_skew = mu_opt, sigma_opt, gamma_opt, nu_opt

# G√©n√©ration des QQ plots
functions.qqplot_gaussian_skew(train_data['Log Return'], mu_gauss, sigma_gauss, mu_skew, sigma_skew, gamma_skew, nu_skew)
```

La loi de Skew Student colle beaucoup mieux aux rendements extr√™mes que la loi normale. Elle s‚Äôadapte mieux √† la forme r√©elle de nos donn√©es, surtout dans les queues.

***

#### Calcul de la VaR Skew Student sur base d‚Äôapprentissage pour alpha = 99%.

```{python}
importlib.reload(functions)
VaR_skew = functions.Var_param_student(train_data['Log Return'], confidence_level=0.99)
```

## Expected Shortfall

On s'int√©resse √† pr√©sent √† un indicateur plus prudent et informatif que la VaR afin de compl√©ter notre analyse : **l'Expected shortfall**. 

Il r√©pr√©sente la perte moyenne attendue en cas de d√©passement de la VaR, c‚Äôest-√†-dire la moyenne des pertes dans les pires cas

***

#### Calcul de l‚ÄôES empirique associ√© √† la mesure la VaR historique sur base d‚Äôapprentissage pour alpha = 99%.

```{python}
importlib.reload(functions)
print("La VaR historique (niveau 99%) est : ", round(var_99_hist, 4))
ES_hist =functions.ES_Hist(train_data['Log Return'])
```

***

####  Calcul de l‚ÄôES empirique associ√© √† la VaR gaussienne sur base d‚Äôapprentissage pour alpha = 99%.

```{python}
importlib.reload(functions)
print("La VaR gaussienne (niveau 99%) est : ", round(var_gauss, 4))
ES_emp_gauss = functions.ES_emp_gauss(train_data)
```

####  L'ES th√©orique bas√©e sur la VaR gaussienne

```{python}
importlib.reload(functions)
print("La VaR gaussienne (niveau 99%) est : ", round(var_gauss, 4))
ES_gauss = functions.ES_gauss(train_data, confidence_level=0.99)
```

***

#### Calcul de l'ES empirique bas√© sur la VaR skewed-student √† 99%

```{python}
importlib.reload(functions)
ES_emp_skew_student = functions.ES_emp_skew_student(train_data)
```

####  Calcul de l'ES th√©orique associ√©e √† la VaR Skew student

```{python}
importlib.reload(functions)
print("La VaR skew-student (niveau 99%) est : ", round(VaR_skew, 4))
ES_skew_student = functions.ES_skew_student(mu_opt, sigma_opt, gamma_opt, nu_opt, train_data, VaR_skew)
```

## 	Protocole de backtesting

***

#### Proposition d'un protocole de backtesting 

Nous proposons ci dessous un protocole de backtesting adaptatif pour une VaR gaussienne

***


**Protocole de backtesting adaptatif :**  
Le protocole de backtesting propos√© vise √† v√©rifier la robustesse de l'estimation de la VaR par une approche param√©trique gaussienne en suivant trois √©tapes principales :  

1. **Calibration initiale :**  
   Le mod√®le est initialement calibr√© sur une p√©riode d‚Äôapprentissage fixe pour estimer la VaR gaussienne.  

2. **Backtesting ex-post quotidien :**  
   Le backtesting commence √† partir du 30·µâ jour afin de s'assurer que les hypoth√®ses sous-jacentes des tests utilis√©s (notamment l'approximation des lois) sont satisfaites. √Ä partir de ce moment-l√†, la VaR estim√©e est test√©e chaque jour sur une p√©riode glissante de taille pr√©d√©finie (`window_size`).  
   Deux tests sont r√©alis√©s :  
   - **Unconditional Coverage Test (UC Test)** : V√©rifie si la proportion d‚Äôexception observ√©s est coh√©rente au seuil fix√©, ici 1%.  
   - **Independence Test (IND Test)** : V√©rifie si les exc√®s sont ind√©pendants dans le temps.  

3. **M√©canisme de recalibrage par fen√™tre glissante :**  
   - **Recalibrage d√©clench√© par exception :**  
     Si l'un des tests √©choue (`UC Test` ou `IND Test`), un recalibrage est imm√©diatement d√©clench√©.  
     La p√©riode d'entra√Ænement est alors mise √† jour par une fen√™tre glissante : les observations les plus anciennes sont supprim√©es tandis que les nouvelles observations r√©centes sont ajout√©es.  
     Par exemple, si une exception est d√©tect√©e au 100·µâ jour, les 100 premi√®res observations de la p√©riode d'entra√Ænement sont supprim√©es et remplac√©es par les 100 jours les plus r√©cents des donn√©es de test. Cela permet d'actualiser les param√®tres du mod√®le pour mieux refl√©ter les conditions actuelles du march√©.  

   - **Recalibrage en absence prolong√©e d‚Äôexception :**  
     Si aucune exception n'est d√©tect√©e sur une longue p√©riode (`max_no_recalib = 252 jours`), un recalibrage est √©galement effectu√© par une fen√™tre glissante.  
     Cela garantit que le mod√®le reste pertinent m√™me lorsqu‚Äôil semble bien fonctionner sur une longue p√©riode, √©vitant ainsi un ajustement excessivement conservateur.  

Ce protocole dynamique assure une adaptation continue du mod√®le, permettant d'estimer la VaR de mani√®re fiable en s'adaptant aux nouvelles conditions de march√©.

***

#### Mise en place ce protocole sur les donn√©es de test

Pour ce faire, dans la partie dedi√©e au code, on retrouvera : 

- la fonction : **perform_backtest*** qui fait conjointement le test d'unconditional coverage et le test d'independance

Ensuite la fonction 

- la fonction : **adaptive_backtesting** : Qui r√©alise le backtesting adaptatif


***

```{python}
importlib.reload(functions)

result_var, result_date_recalib, jours_recalibrage = functions.adaptive_backtesting(train_data, test_data)
```

***

La th√©orie des valeurs extr√™mes offre √©galement d'importants insights pour le calcul de la Value at Risk. 

Nous allons pr√©senter ci apr√®s les VaR obtenues via les m√©thodes dites :

    - Generalized Pareto Distribution (GPD)  qui est une approche par bloc Maxima et 
    -  Peak Over Threshold(POT)

##  VaR TVE : Approche Maxima par bloc

***

#### D√©termination d'une taille de bloc s et construction d'un √©chantillon de maxima sur la base d‚Äôapprentissage.


On choisit s suffisamment grand pour satisfaire les conditions asymptotiques du th√©or√®me de  Fisher - Tippet
mais √©galement suffisamment faible pour obtenir un √©chantillon de maxima de taille convenable

Dans notre cas, nous avons choisi comme taille de bloc 20

Compte tenu du fait que nous travaillons sur des donn√©es financi√®res, cette taille represente donc des blocs mensuels.

***

####    Trac√© de Gumbel plot pour juger de l‚Äôhypoth√®se Œæ=0 (i.e. GEV vs EV).

Le Gumbel plot nous permettra de determiner si la distribution adapt√©e aux donn√©es est celle de Gumbel

```{python}
importlib.reload(functions)
functions.gumbel_plot(-train_data['Log Return'])
```

La courbe observ√©e, pr√©sente une courbure mais, tr√®s peu prononc√©e. Pour √™tre pr√©cautionaux, nous allons mod√©liser une GEV.
 
 #### 	Estimation des param√®tres de loi GEV 

***

Nous avons opt√© pour des blocs de taille 20, qui correspondent dans notre cas √† une p√©riode mensuelle.

```{python}
importlib.reload(functions)
# Recuperons les param√®tres de la GEV
block_max = functions.block_maxima(-train_data)
shape, loc, scale, neg_log_likelihood_value = functions.fit_gev(block_max)

# Affichons les r√©sultats 
print(f"Shape (Œæ): {shape}\n{'-'*50}")
print(f"Location (Œº): {loc}\n{'-'*50}")
print(f"Scale (œÉ): {scale}\n{'-'*50}")
print(f"Negative Log-Likelihood: {neg_log_likelihood_value}\n{'-'*50}")
```

***

#### Faisons une validation ex-ante de notre ajustement

Nous allons pour ce faire, tester l'ajustement de nos donn√©es √† la loi ajust√©e en utilisant un QQ plot, un test d'ad√©quation de kolmogorov smirnov, et un LR test, 

Pour voir si le fait d'avoir pris en compre cette courbure tr√®s prononc√©e apporte quelque chose √† nos donn√©es approte quelque chose, nous allons fait un test de rapport de vraisemblance pour comparer la Gumble √† la loi ajust√©e

***

QQ-Plot

***

```{python}
importlib.reload(functions)
functions.gev_plot(block_max , shape, loc, scale)
```

Le QQ-plot sugg√®re que la loi GEV est globalement adapt√©e aux maxima extraits de nos blocs.
Les points soint assez bien align√©s sur la premiere bissectrice, ce qui indique que la distribution th√©orique capture bien le comportement empirique, y compris dans les queues, malgr√© quelques √©carts en tr√®s haute valeur.

Nous allons valider cel√† statistiquement avec un test de Kolmogorov-smirnov au seuil de 5%

***

```{python}
importlib.reload(functions)
functions.ks_test_gev(block_max, shape, loc, scale, alpha=0.05)
```

***

La courbure du Gumbel plot pr√©sent√© pr√©c√©demment etait tr√®s peu marqu√©e et nous avons choisi de  mod√©liser une GEV pour √™tre plus pr√©cautionneux. 

Nous voulons √† pr√©sent savoir si le gain qu'on obtient en faisant ce choix est significatif. Nous allons donc pour ce faire effectuer un test de comparaison de vraisemblance **(LR test)**:

On compare les log vraisemblance de la GEV ajust√©e sur nos donn√©es √† celle qu'aurait donn√© une Gumbel.


***

**Pour ce faire, on ajuste d'abord une Gumbel**

```{python}
importlib.reload(functions)
loc_g, scale_g, neg_log_likelihood_value_g = functions.fit_gumbel(block_max)
```

**On r√©alise ensuite notre test de rapport de vraisemblance**

```{python}
importlib.reload(functions)
functions.LR_test(neg_log_likelihood_value_g, neg_log_likelihood_value)
```

Statistiquement, il n'y a pas d'amelioration d'ajustement qui soit significatif en ajustant une GEV plut√¥t qu'une Gumbel.

L'effet d'une GEV est donc marginal.

Nous allons donc considerer la Gumbel pour le calcul de la VaR

***

#### Calcul de la VaR

```{python}
importlib.reload(functions)
VaR_Gumbel = functions.calcul_VaR_Gumbel (loc_g, scale_g, block_size=20)
```


***

## VaR TVE : Approche Peak over threshold

####  Ecrivons une fonction permettant d‚Äôobtenir le mean excess plot et determinons le seuil u par une analyse graphique

```{python}
importlib.reload(functions)
functions.mean_excess_plot(-train_data, u_min=0, u_max=None, step=0.001)
```

En observant le mean excess plot, on opte pour u = 0.028. 
Le comportement des mean excess au del√† de u, sont plus ou moins lin√©aires. Mais il est important que u soit suffisamment √©lev√© pour respecter les conditions asymptotiques. 

Nous optons donc pour cette valeur et voyons les caract√©ristiques de la loi ajust√©e

#### Estimation  des param√®tres de loi GPD

***

```{python}
importlib.reload(functions)
shape, loc, scale =functions.fit_gpd(-train_data['Log Return'].to_numpy(), u = 0.028)
print("-" * 50)
print("Param√®tres calibr√©s de la loi GPD\n")
print(f"Shape (Œæ)  : {shape:.4f}")
print(f"Loc        : {loc:.4f}")
print(f"Scale (œÉ)  : {scale:.4f}")
print("-" * 50)
```

####  Faire une validation ex-ante (analyse graphiques, QQ-plot, etc.)

Nous utilisons pour ce faire un QQ-plot, un PP-plot et un test de kolmogorov smirnov

***

```{python}
importlib.reload(functions)
u = 0.028
gpd_validation = functions.gpd_validation(-train_data['Log Return'].to_numpy(), u, shape, scale)
```

Le QQ plot et le PP plot affichent un plus ou moins bien ajustement de la loi GPD aux donn√©es. Bien qu'elle sous estime quelque peu les valeurs extr√™mes.

Cet assez bon ajustement est corrobor√©e par un test statistique de Kolmogorov smirnov qui soutient que la GPD arrive √† bien mod√©liser les exc√®s.

Test de Kolmogorov_smirnov

```{python}
importlib.reload(functions)
functions.gpd_ks_test(-train_data['Log Return'], u, shape, scale, alpha=0.05)
```

####  Calculer de la VaR TVE par PoT sur base d‚Äôapprentissage pour alpha = 99%.

```{python}
importlib.reload(functions)
var_tve_pot_result = functions.var_tve_pot(-train_data['Log Return'].to_numpy(), u, shape, scale)
print("-" * 50)
print("la VaR TVE par PoT √† un niveau de confiance 99 % est :\n")
print(f"VaR TVE  : {var_tve_pot_result :.4f}")
print("-" * 50)
```


Le choix du seuil u, √©tant tr√®s subjectif et  g√©n√©ralement pas ais√©, nous proposons ci apr√®s un protocole qui permet une selection automatique d'un bon seuil pour la m√©thode POT.

#### 	Proposition  d'un protocole permettant de calibrer u de mani√®re automatique, et le mettre en ≈ìuvre.

***

Afin de d√©terminer le seuil optimal, il ne doit pas √™tre trop bas pour √©viter d‚Äôinclure des donn√©es trop fr√©quentes, ni trop √©lev√© afin de conserver suffisamment d'observations pour une mod√©lisation fiable. Pour cette raison, un seuil minimal est fix√© au quantile d'ordre 90% et un seuil maximal au quantile 99%. Avec un pas ajustable de 0.0001, nous explorons la plage de valeurs comprises entre u_min et u_max. Pour chaque seuil, nous estimons les param√®tres de la loi GPD. Nous calculons ensuite les √©carts absolus entre les estimations successives des param√®tres scale et shape. Ces √©carts sont additionn√©s pour chaque seuil afin de mesurer la stabilit√© des estimations. Le seuil optimal correspond √† celui o√π la somme de ces √©carts est minimale, indiquant une stabilit√© maximale des param√®tres estim√©s.

##### Impl√©mentation du protocole

***

```{python}
importlib.reload(functions)
u = functions.calibrate_u(-train_data['Log Return'].to_numpy(), alpha=0.99, step=0.0001)
```

```{python}
print('-'*40)
print("Le param√®tre u optimal dans le cadre de notre exercice pr√©sent est", u)
print('-'*40)
```

## 	VaR  GARCH 


***

#### Etude de  l‚Äôapplicabilit√© d‚Äôun mod√®le AR[1]-GARCH[1,1] √† la s√©rie des log-rendements historiques sur base d‚Äôapprentissage.

On proc√©dera de mani√®re s√©quentielle : plausibilit√© d‚Äôun AR[1], √©tude de l‚Äôhomoscedasticit√© des r√©sidus de l‚ÄôAR[1]

***

##### Sp√©cification d'un AR(1) sur les rendements et analyse des r√©sidus

```{python}
### Sp√©cification d'un AR(1) et analyse des r√©sidus


# Analyse de  l'ACF et du PACF des rendements
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plot_acf(train_data['Log Return'], lags=20, ax=plt.gca())
plt.title('ACF of Returns')

plt.subplot(1, 2, 2)
plot_pacf(train_data['Log Return'], lags=20, ax=plt.gca())
plt.title('PACF of Returns')
plt.show()

# Test de stationnarit√© (ADF test)
adf_result = adfuller(train_data['Log Return'])
print(f"ADF Statistic: {adf_result[0]}")
print(f"p-value: {adf_result[1]}")
print(f"Critical Values: {adf_result[4]}")

# On ajuste un mod√®le AR(1) sur nos rendements
ar_model = ARIMA(train_data['Log Return'], order=(1, 0, 0)).fit()
print(ar_model.summary())
```

Les coefficients const et ar.L1 ne sont pas significatifs, ce qui indique qu‚Äôun mod√®le AR(1) ne capture pas bien la dynamique de la s√©rie. 

En s'int√©ressant notamment aux residus de cette mod√©lisation AR(1) (pr√©sent√©s ci-dessous), il en ressort qu'en effet, ce mod√®le n'est pas adapt√© √† la dynamique des rendements

```{python}
residuals = ar_model.resid

# Plot ACF and PACF of squared residuals
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plot_acf(residuals**2, lags=20, ax=plt.gca())
plt.title('ACF of Squared Residuals')

plt.subplot(1, 2, 2)
plot_pacf(residuals**2, lags=20, ax=plt.gca())
plt.title('PACF of Squared Residuals')
plt.show()
```

L'ACF ci dessus sugg√®re une pr√©sence subtantielle d'autocorellation dans les residus du mod√®le AR(1). Cette observation est valid√©e statistiquement par un test statistique : le test de **Ljung box**.

```{python}
ljung_box_pvalues_squared = acorr_ljungbox(residuals, lags=[10], return_df=True)
print("R√©sultats du test de Ljung-Box sur les r√©sidus  :")
print(ljung_box_pvalues_squared)
```

**Conclusion 1** : 

Le mod√®le AR(1) n'est clairement pas celui adapt√© pour mod√©liser la dynamique de nos log-rendements. Nous allons explorer d'autres approches.

***

Nous allons faire un ajustement GARCH[1,1] sur les residus du AR(1) pr√©c√©dent et allons analyser le comportement des residus obtenus

##### Ajustement d'un GARCH(1,1) sur les r√©sidus de l'AR(1) 

***

```{python}

garch_model = arch_model(residuals, vol='Garch', p=1, q=1)
garch_fit = garch_model.fit()
print(garch_fit.summary())

# Plot ACF of GARCH residuals
garch_residuals = garch_fit.resid
plot_acf(garch_residuals, lags=20)
plt.title('ACF of GARCH Residuals')
plt.show()
```

Les coefficients du GARCH(1,1) sont tous significatifs, de plus , et, en observant l'ACF, on n'observe pas d'autocorellation dans les residus du mod√®le mis en place.

Ceci, nous am√®ne donc √† postuler pour un mod√®le de type AR(1)- GARCH(1,1) pour mod√©liser la dynamique des log rendements

#### Estimation des param√®tres du mod√®le AR[1]-GARCH[1,1] sur base d‚Äôapprentissage 


***

```{python}
combined_model = arch_model(train_data['Log Return'], mean='AR', lags=1, vol='Garch', p=1, q=1)
combined_fit = combined_model.fit()
print(combined_fit.summary())
```

#### V√©rifions que les r√©sidus de l‚ÄôAR[1]-GARCH[1,1] sont bien repr√©sentatifs d‚Äôun bruit blanc i.i.d.

***

- On verifie si les r√©sidus sont d√©correl√©s

```{python}
# R√©cup√©rons les r√©sidus standardis√©s du mod√®le AR(1)-GARCH(1,1)

residuals = combined_fit.std_resid  
residuals = residuals.dropna()
# Test d'autocorr√©lation avec Ljung-Box
ljung_box_pvalues = acorr_ljungbox(residuals, lags=[10], return_df=True)
ljung_box_pvalues_squared = acorr_ljungbox(residuals**2, lags=[10], return_df=True)
print("R√©sultats du test de Ljung-Box sur les r√©sidus :")
print(ljung_box_pvalues)
```

La p-value du test de Ljung-box est superieure √† 0.05, ce qui signifie que au seuil de 5%, on peut rejetter l'hypoth√®se d'autocorrelation des residus du mod√®le postul√©.

***

- On s'assure aussi que la volatilit√© conditionnelle est bien mod√©lis√©e par le GARCH.

Pour ce faire, on va tester l'auto-correlation des residus studentis√©s √©l√©v√©s au carr√©

***

```{python}
ljung_box_pvalues_squared = acorr_ljungbox(residuals**2, lags=[10], return_df=True)
print("R√©sultats du test de Ljung-Box sur les r√©sidus au carr√© :")
print(ljung_box_pvalues_squared)
```

La volatilit√© conditionnelle est bien donc mod√©lis√©e par le GARCH

***

- On s'assure que ces residus sont identiquement distribu√©s

Pour ce faire, on essaie de mod√©liser la loi qui les r√©git : On effectue des test de normalit√© √† savoir le test de forme de Jacques Bera et le test de Shapiro-wilk

***

```{python}
# Tracer la densit√© (KDE) des r√©sidus
sns.kdeplot(residuals, shade=True, color='blue')
plt.title("Densit√© des r√©sidus")
plt.xlabel("Valeurs des r√©sidus")
plt.ylabel("Densit√©")
plt.grid(True)
plt.show()
```

```{python}
# Tracer le QQ plot
plt.figure(figsize=(6, 6))
stats.probplot(residuals, dist="norm", plot=plt)
plt.title("QQ Plot des r√©sidus vs Distribution Normale")
plt.grid(True)
plt.show()
```

```{python}

#Test de normalit√© avec Jarque-Bera
jb_stat, jb_pvalue = jarque_bera(residuals)

print("\nR√©sultat du test de Jarque-Bera :")
print(f"JB-stat = {jb_stat:.3f}, p-value = {jb_pvalue:.3f}")
```

```{python}
# Test de Shapiro-Wilk
shapiro_stat, shapiro_pvalue = shapiro(residuals)

print("\nR√©sultat du test de Shapiro-Wilk :")
print(f"Statistique W = {shapiro_stat:.3f}, p-value = {shapiro_pvalue:.3f}")
```


**Analyse des resultats relatifs √† la distribution des residus**


Bien que les outils graphiques sugg√®rent une distribution normale des residus, les tests statistiques effectu√©es ont condduit au rejet de cette hypoth√®se. Ceci sugg√®re que, a priori, les residus ne sont pas identiquement distribu√©s. 

Malgr√© cela, nous conservons le mod√®le AR(1)-GARCH(1,1) en raison de sa simplicit√© d'interpr√©tation et de sa pertinence p√©dagogique.  

Ce mod√®le constitue un bon point de d√©part pour la mod√©lisation de la VaR, avec une structure claire o√π :  
- **ùúî (omega)** repr√©sente la volatilit√©  de long terme,  
- **ùõº (alpha)** mesure la r√©action imm√©diate aux chocs r√©cents,  
- **ùõΩ (beta)** refl√®te la persistance de la volatilit√© √† travers le temps.  

***

```{python}
### On regarde la forme du combined plot pour notre mod√®le.
combined_fit.plot()
```

#### 	Reproduisons sur l‚Äôensemble de la p√©riode (apprentissage + test)  la dynamique historique de Œº_t et œÉ_t selon la dynamique et les valeurs initiales fournies dans le support de cours.


***

```{python}
mu, phi, omega, a, b = combined_fit.params
data = pd.concat([train_data, test_data])
std_residuals = combined_fit.std_resid 
alpha = 0.99
q = std_residuals.quantile(1-alpha) 

data["mu"] = mu + phi*data['Log Return'].shift()
data["mu"][0] = mu

data["vol"] = np.sqrt(omega/(1-a-b))

for t in range(1, len(data)):
    data["vol"][t] = np.sqrt(omega + 
                             a*(data['Log Return'][t-1] - data["mu"][t-1])**2 
                            + b*data["vol"][t-1]**2
                               )
data["VaR"] = data["mu"] + data["vol"]*q
data.head(2)
```

####  Pour les m√©thodes historique et GPD de calcul de VaR, nous allons

-   Estimer la VaR sur les r√©sidus
-   	Calculer la VaR dynamique et les exceptions associ√©es sur la base de test

***

***

##### Cas de la VaR historique

***

```{python}
plt.figure(figsize=(12, 6))

# Tracer la VaR et les rendements
plt.plot(data.index, data["VaR"], label="VaR", color="red", linestyle="--")
plt.plot(data.index, data['Log Return'], label="Rendements", color="blue")

# Identifier les points o√π la VaR exc√®de les rendements
exceedance_points = data[data["VaR"] > data['Log Return']]

# Marquer ces points avec des points rouges
plt.scatter(exceedance_points.index, exceedance_points['Log Return'], color="red", label="Exception", zorder=5)

# Annoter les points critiques
for date, return_value in exceedance_points['Log Return'].items():
    plt.annotate(date.strftime('%Y-%m-%d'), (date, return_value), textcoords="offset points", xytext=(0, 10), ha='center')

# Ajouter des labels et une l√©gende
plt.title("VaR vs Rendements")
plt.xlabel("Date")
plt.ylabel("Valeur")
plt.legend()
plt.grid()

# Afficher le graphique
plt.show()
```

##### Cas de la VaR GPD

```{python}
# On ajuste un AR(1)_GARCH(1,1)
combined_model = arch_model(train_data['Log Return'], mean='AR', lags=1, vol='Garch', p=1, q=1)
combined_fit = combined_model.fit()

# On recup√®re les residus
std_residuals = combined_fit.std_resid.dropna().to_numpy()
std_residuals = -std_residuals
```

```{python}
importlib.reload(functions)
functions.compute_and_plot_var_GPD(std_residuals, alpha, combined_fit, train_data, test_data)
```

## Conclusion

***

L'estimation de la Value at Risk (VaR) est une t√¢che essentielle pour nous qui sommes appel√©s √† √™tre des mod√©lisateur, car elle constitue un indicateur cl√© qui oriente la prise de decision, des banques, des prestataires de services d'investissement, et m√™me des  soci√©t√©s de gestion de portefeuilles.

Dans la litt√©rature, plusieurs m√©thodes permettent d'estimer la Value at Risk. Dans le cadre de ce cours, et plus particuli√®rement de ce projet, nous avons explor√© diverses approches, allant de la m√©thode gaussienne classique √† des mod√®les plus sophistiqu√©s tels que le mod√®le AR(1)-GARCH(1,1).

Chacune de ces m√©thodes a montr√© ses limites, tant au niveau du calcul du nombre d'exceptions que de l'application du protocole de backtesting.

En se concentrant plus sp√©cifiquement sur les mod√®les de type s√©rie temporelle, les tests statistiques r√©alis√©s dans le cadre de la mod√©lisation AR(1) ‚Äî notamment sur les r√©sidus, la distribution et la volatilit√© ‚Äî ont mis en √©vidence deux points cl√©s : les r√©sidus ne suivent pas la m√™me loi de distribution, et la volatilit√© n‚Äôest pas constante.

Ces observations nous ont naturellement conduit au passage √† un mod√®le conditionnel de type AR(1)-GARCH(1,1), plus adapt√© pour mod√©liser les variations de la volatilit√© dans le temps.

En combinant ce mod√®le avec une loi de queue plus souple (la GPD dans notre cas), nous avons pu affiner l‚Äôestimation de la VaR, notamment dans les zones extr√™mes.

La version dynamique de la VaR, fond√©e sur ce mod√®le, s‚Äôest r√©v√©l√©e la plus pertinente : elle s‚Äôajuste mieux aux fluctuations des donn√©es et permet d‚Äôidentifier plus efficacement les p√©riodes de stress.

Cela montre que, bien que ces mod√®les soient plus sophistiqu√©s et donc plus complexes √† impl√©menter, leur utilisation permet d‚Äôobtenir un gain significatif en termes de pr√©cision.


Par ailleurs, ce projet nous a permis d'approfondir notre compr√©hension des d√©fis associ√©s √† l'estimation de la VaR, ainsi que des difficult√©s rencontr√©es avec les diff√©rentes approches. Cela inclut, par exemple, la d√©termination de la taille des blocs pour l'approche des blocs maxima, ou encore le choix du seuil optimal dans le cadre de l'approche POT. Nous avons √©t√© amen√©s  √† pousser la  r√©flexion de mani√®re plus approfondie afin de concevoir nous m√™me des m√©canismes permettant de s√©lectionner automatiquement certains param√®tres, optimisant ainsi le processus d'estimation.