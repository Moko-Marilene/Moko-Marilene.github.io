[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Marilene KOUGOUM",
    "section": "",
    "text": "Je suis Marilene KOUGOUM, récemment diplômée de l’ENSAI Rennes en tant qu’ingénieure en data science et gestion des risques. Cette formation m’a permis d’acquérir une base solide en modélisation des risques financiers - notamment les risques de marché et de crédit - ainsi qu’une bonne compréhension des cadres comptables et réglementaires structurant de secteur bancaire (Bâle III, IFRS 9, etc.).\nJe poursuis actuellement mes études à l’Université Paris Cité au sein du master M2MO, spécialisé en modélisation aléatoire, finance quantitative et data science.\nJ’ai créé ce site pour partager ce que j’apprends au fil de mon parcours : d’une part, pour aider mes cadets académiques à avoir un aperçu des notions enseignées en école d’ingénieur et en master quantitatif ; d’autre part, afin de vulgariser des concepts parfois techniques en finance quantitative, en data science ou en gestion des risques.\nLes contenus présents sur ce site proviennent de mes travaux académiques, de projets personnels ou d’enseignements réalisés sous la supervision de professeurs et de professionnels experts dans leurs domaines. Je ne prétends pas que ces contenus soient exhaustifs : ils reflètent mon apprentissage continu et ma volonté de partager.\nMerci de votre visite, et bonne exploration !"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Apprendre, comprendre, partager.",
    "section": "",
    "text": "Modèles stochastiques, pricing, produits dérivés.\n\n→ Explorer\n\n\n\n\nRisque de marché, risque de crédit, conformité bancaire.\n\n→ Explorer\n\n\n\n\nModèles prédictifs, séries temporelles, applications financières.\n\n→ Explorer\n\n\n\n\n\n\n\n\n\n\n\nSimulation stochastique, variance reduction, convergence.\n\nLire\n\n\n\n\n\nPD, LGD, IFRS 9 et modèles statistiques appliqués.\n\nLire\n\n\n\n\n\nARIMA, Prophet, LSTM : comparaison et analyse.\n\nLire"
  },
  {
    "objectID": "index.html#les-grands-axes-du-site",
    "href": "index.html#les-grands-axes-du-site",
    "title": " ",
    "section": "Les grands axes du site",
    "text": "Les grands axes du site\n\n\n\nFinance quantitative\n\n\nCalibration de processus stochastiques, Modèles de courbe de taux, Pricing…\n\n→ Explorer\n\n\n\nRisque & Régulation\n\n\nRisque de marché, risque de crédit, conformité bancaire…\n\n→ Explorer\n\n\n\nModélisation stat\n\n\nAnalyse des séries financières, Théorie des valeurs extrêmes, Modèles de durée…\n\n→ Explorer"
  },
  {
    "objectID": "index.html#derniers-projets",
    "href": "index.html#derniers-projets",
    "title": " ",
    "section": "Derniers projets",
    "text": "Derniers projets\n\n\n\n\nPricing d’options via Monte-Carlo\n\n\nSimulation stochastique, variance reduction, convergence.\n\nLire\n\n\n\n\nModélisation du risque de crédit\n\n\nPD, LGD, IFRS 9 et modèles statistiques appliqués.\n\nLire\n\n\n\n\nPrévision de séries temporelles\n\n\nARIMA, Prophet, LSTM : comparaison et analyse.\n\nLire"
  },
  {
    "objectID": "index.html#publications-récentes",
    "href": "index.html#publications-récentes",
    "title": " ",
    "section": "Publications récentes",
    "text": "Publications récentes\n\n\n\n\nCourbe de taux & modèle de Hull–White\n\n\nBootstrapping, valorisation caplets/swap­tion, calibration Hull–White\n\nLire\n\n\n\n\nVECM & Pairs Trading\n\n\nCointégration,ruptures structurelles, extensions (GARCH, régimes), usages en stratégie pairs trading\n\nLire\n\n\n\n\nGestion de la liquidité des actifs\n\n\nProfondeur de marché, Calcul du profil d’écoulement,liquidation avec/sans déformation.\n\nLire"
  },
  {
    "objectID": "risque-regulation.html",
    "href": "risque-regulation.html",
    "title": "Risque & Régulation",
    "section": "",
    "text": "Dans cette première partie, nous présentons les fondements théoriques de la gestion des risques bancaires.\nElle rassemble les concepts, définitions et structures méthodologiques nécessaires pour comprendre le fonctionnement global du risk management dans les institutions financières."
  },
  {
    "objectID": "risque-regulation.html#la-gestion-des-risques-une-fonction-stratégique",
    "href": "risque-regulation.html#la-gestion-des-risques-une-fonction-stratégique",
    "title": "Risque & Régulation",
    "section": "La Gestion des Risques : Une Fonction Stratégique",
    "text": "La Gestion des Risques : Une Fonction Stratégique\nAu cœur du système économique, les banques assurent une mission essentielle de circulation des capitaux entre épargnants et investisseurs. Cette position centrale les expose naturellement à divers risques financiers qu’elles doivent apprendre à maîtriser.\nLa gestion des risques a considérablement évolué : d’une fonction traditionnelle de contrôle, elle est devenue un véritable levier de création de valeur. Elle permet aujourd’hui une allocation optimale du capital et une tarification plus fine des produits financiers, tout en guidant les décisions stratégiques.\nLe risk manager moderne endosse un rôle crucial à l’interface des métiers. Véritable vigie, il anticipe les scénarios défavorables, conseille les équipes opérationnelles et éclaire la direction générale sur l’exposition aux risques."
  },
  {
    "objectID": "risque-regulation.html#panorama-des-risques-bancaires",
    "href": "risque-regulation.html#panorama-des-risques-bancaires",
    "title": "Risque & Régulation",
    "section": "Panorama des Risques Bancaires",
    "text": "Panorama des Risques Bancaires\nPour appréhender correctement la gestion des risques, il est fondamental d’en connaître les différentes facettes. Quatre catégories principales structurent le paysage risk management.\n\nLe risque de marché\nLe risque de marché apparaît lorsque la valeur des positions détenues par une banque évolue sous l’effet des mouvements des marchés financiers. Ces mouvements concernent notamment :\n\nles taux d’intérêt,\n\nles taux de change,\n\nles cours des actions,\n\nles matières premières,\n\net, plus globalement, l’ensemble des conditions économiques et financières.\n\nCe risque ne se résume pas à une liste de menaces distinctes : il traduit la manière dont un portefeuille réagit aux évolutions de son environnement. Certains instruments sont sensibles à de petites variations, tandis que d’autres réagissent davantage à des mouvements plus marqués ou à des périodes de forte instabilité, où les prix peuvent changer rapidement et où les relations entre actifs deviennent moins prévisibles.\n\n\nLe risque de crédit\nLe risque de crédit, le plus significatif pour une banque, correspond à la probabilité qu’une contrepartie ne honore pas ses engagements financiers.\n\n\nLe risque de liquidité\nIl se manifeste sous deux angles : - liquidité de marché : capacité à céder des actifs sans perte de valeur significative ; - liquidité de financement : aptitude à se refinancer à un coût raisonnable.\n\n\nLe risque opérationnel\nIl englobe les pertes potentielles liées à des défaillances internes (processus, personnes, systèmes) ou à des événements externes, incluant le risque juridique.\n\n\nAutres risques\nParmi les autres risques notables : - le risque de réputation,\n- le risque de modèle,\n- le risque systémique.\n\nCes risques sont profondément interconnectés. Une approche en silos pourrait conduire à sous-estimer des menaces transversales aux conséquences potentiellement graves."
  },
  {
    "objectID": "risque-regulation.html#la-théorie-des-mesures-de-risque",
    "href": "risque-regulation.html#la-théorie-des-mesures-de-risque",
    "title": "Risque & Régulation",
    "section": "La Théorie des Mesures de Risque",
    "text": "La Théorie des Mesures de Risque\nLa quantification rigoureuse du risque s’appuie sur des outils mathématiques sophistiqués. Une mesure de risque, notée ( (L) ) où ( L ) représente la perte aléatoire, détermine le capital économique nécessaire à sa couverture.\n\nMesures de risque cohérentes\nArtzner et ses co-auteurs ont défini quatre axiomes qui caractérisent une mesure de risque robuste :\n\nInvariance par translation\n[ (L + c) = (L) + c ]\nHomogénéité positive\n[ (L) = (L) ]\nMonotonie\n[ L_1 L_2 (L_1) (L_2) ]\nSous-additivité\n[ (L_1 + L_2) (L_1) + (L_2) ]\n\nCes propriétés garantissent que la mesure du risque se comporte de manière intuitive et économiquement rationnelle."
  },
  {
    "objectID": "risque-regulation.html#var-et-expected-shortfall",
    "href": "risque-regulation.html#var-et-expected-shortfall",
    "title": "Risque & Régulation",
    "section": "VaR et Expected Shortfall",
    "text": "VaR et Expected Shortfall\n\nValue-at-Risk (VaR)\nLa Value-at-Risk représente la perte maximale qui ne sera dépassée qu’avec une probabilité donnée :\n[ _(L) = { x : (L x) }. ]\nBien que simple et intuitive, elle souffre de deux limites majeures : - elle n’est pas cohérente,\n- elle ne renseigne pas sur la gravité des pertes extrêmes.\n\n\nExpected Shortfall (ES)\nL’Expected Shortfall correspond à la perte moyenne dans les pires scénarios :\n[ _(L) = [, L L _(L),]. ]\nSa cohérence et sa meilleure capture des risques extrêmes en font désormais la mesure privilégiée dans les réglementations récentes."
  },
  {
    "objectID": "risque-regulation.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "href": "risque-regulation.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "title": "Risque & Régulation",
    "section": "Le Cadre Réglementaire : de Bâle I à Bâle IV",
    "text": "Le Cadre Réglementaire : de Bâle I à Bâle IV\nBâle I (1988) : premier standard international avec le ratio Cooke (capital/RWA ≥ 8%).\nBâle II (2004) : approche en trois piliers, reconnaissance des modèles internes.\nBâle III (2010/2011) : renforcement de la qualité du capital, coussins, ratios LCR et NSFR.\nBâle IV (2017) : remplacement de la VaR par l’ES, nouvelle approche opérationnelle, restrictions sur modèles internes.\nCette évolution réglementaire témoigne de la recherche permanente d’un équilibre entre innovation financière et stabilité du système."
  },
  {
    "objectID": "risques/chapter_one.html",
    "href": "risques/chapter_one.html",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "",
    "text": "⬅ Retour à Risque & Régulation"
  },
  {
    "objectID": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique",
    "href": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "La Gestion des Risques : une Fonction Stratégique",
    "text": "La Gestion des Risques : une Fonction Stratégique\nAu cœur du système économique, les banques jouent un rôle fondamental d’intermédiation en orientant les capitaux entre agents en excédent et agents en besoin de financement. Cette position centrale les expose naturellement à une variété de risques financiers qu’il est essentiel de comprendre et de maîtriser.\nAvec le temps, la fonction de gestion des risques s’est profondément transformée. Jadis perçue comme un simple dispositif de contrôle, elle constitue aujourd’hui un levier stratégique, permettant :\n\nune allocation optimale du capital,\nune tarification plus fine des produits,\nune meilleure anticipation des chocs,\nun pilotage éclairé des décisions stratégiques."
  },
  {
    "objectID": "risques/chapter_one.html#panorama-des-risques-bancaires",
    "href": "risques/chapter_one.html#panorama-des-risques-bancaires",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Panorama des Risques Bancaires",
    "text": "Panorama des Risques Bancaires\nPour appréhender correctement la gestion des risques, il est fondamental d’en connaître les différentes facettes. Quatre catégories principales structurent le paysage du risk management.\n\nLe risque de marché\nLe risque de marché apparaît lorsque la valeur des positions détenues par une banque évolue sous l’effet des mouvements des marchés financiers. Ces mouvements concernent notamment :\n\nles taux d’intérêt,\n\nles taux de change,\n\nles cours des actions,\n\nles matières premières,\n\net, plus globalement, l’ensemble des conditions économiques et financières.\n\nCe risque ne se résume pas à une liste de menaces distinctes : il traduit la manière dont un portefeuille réagit aux évolutions de son environnement. Certains instruments sont sensibles à de petites variations, tandis que d’autres réagissent davantage à des mouvements plus marqués ou à des périodes de forte instabilité, où les prix peuvent changer rapidement et où les relations entre actifs deviennent moins prévisibles.\n\n\nLe risque de crédit\nLe risque de crédit, le plus significatif pour une banque, correspond à la probabilité qu’une contrepartie ne honore pas ses engagements financiers.\n\n\nLe risque de liquidité\nIl se manifeste sous deux angles : - liquidité de marché : capacité à céder des actifs sans perte de valeur significative ; - liquidité de financement : aptitude à se refinancer à un coût raisonnable.\n\n\nLe risque opérationnel\nIl englobe les pertes potentielles liées à des défaillances internes (processus, personnes, systèmes) ou à des événements externes, incluant le risque juridique.\n\n\nAutres risques\nParmi les autres risques notables : - le risque de réputation,\n- le risque de modèle,\n- le risque systémique.\n\nCes risques sont profondément interconnectés. Une approche en silos pourrait conduire à sous-estimer des menaces transversales aux conséquences potentiellement graves."
  },
  {
    "objectID": "risques/chapter_one.html#la-théorie-des-mesures-de-risque",
    "href": "risques/chapter_one.html#la-théorie-des-mesures-de-risque",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "La Théorie des Mesures de Risque",
    "text": "La Théorie des Mesures de Risque\nLa quantification rigoureuse du risque s’appuie sur des outils mathématiques sophistiqués. Une mesure de risque, notée ( (L) ) où ( L ) représente une perte aléatoire, détermine le capital économique nécessaire à sa couverture.\n\nMesures de risque cohérentes\nArtzner et ses co-auteurs ont défini quatre axiomes qui caractérisent une mesure de risque robuste :\n\nInvariance par translation\n\\[\n\\mathcal{R}(L + c) = \\mathcal{R}(L) + c\n\\]\nHomogénéité positive\n\\[\n\\mathcal{R}(\\lambda L) = \\lambda \\mathcal{R}(L)\n\\]\nMonotonie\n\\[\nL_1 \\le L_2 \\;\\Rightarrow\\; \\mathcal{R}(L_1) \\le \\mathcal{R}(L_2)\n\\]\nSous-additivité\n\\[\n\\mathcal{R}(L_1 + L_2) \\le \\mathcal{R}(L_1) + \\mathcal{R}(L_2)\n\\]\n\nCes propriétés garantissent que la mesure du risque se comporte de manière intuitive et économiquement rationnelle.\nDans ce contexte, la Value-at-Risk (VaR) s’est imposée comme un outil largement utilisé en pratique et dans les réglementations prudentielles."
  },
  {
    "objectID": "risques/chapter_one.html#var-et-expected-shortfall",
    "href": "risques/chapter_one.html#var-et-expected-shortfall",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "VaR et Expected Shortfall",
    "text": "VaR et Expected Shortfall\n\nValue-at-Risk (VaR)\nLa VaR à un niveau de confiance \\(\\alpha\\) et sur un horizon donné représente la perte maximale qui ne sera dépassée qu’avec une probabilité \\(1 - \\alpha\\) :\n\\[\n\\text{VaR}_{\\alpha}(L)\n= \\inf \\{\\, x : \\mathbb{P}(L \\le x) \\ge \\alpha \\,\\}.\n\\]\nBien qu’intuitive et largement adoptée, elle présente deux limites importantes :\n\nelle peut violer la sous-additivité (donc n’est pas cohérente),\nelle ne dit rien de l’ampleur des pertes extrêmes situées au-delà du quantile.\n\nCes limitations ont conduit à privilégier une mesure plus robuste : l’Expected Shortfall.\n\n\n\nExpected Shortfall (ES)\nL’Expected Shortfall correspond à la perte moyenne observée dans les pires scénarios, c’est-à-dire au-delà de la VaR :\n\\[\n\\text{ES}_{\\alpha}(L)\n= \\mathbb{E}\\big[\\, L \\mid L \\ge \\text{VaR}_{\\alpha}(L) \\,\\big].\n\\]\nContrairement à la VaR, l’ES :\n\ntient compte de la sévérité des pertes extrêmes,\n\nrespecte les quatre axiomes de cohérence,\n\noffre une meilleure mesure du risque en conditions de stress.\n\nC’est pour ces raisons qu’elle a été adoptée dans les cadres réglementaires récents."
  },
  {
    "objectID": "risques/chapter_one.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "href": "risques/chapter_one.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Le Cadre Réglementaire : de Bâle I à Bâle IV",
    "text": "Le Cadre Réglementaire : de Bâle I à Bâle IV\nL’évolution de la régulation prudentielle suit l’objectif de renforcer la résilience du système bancaire face aux chocs.\n\nBâle I (1988) : introduction du ratio Cooke (capital/RWA ≥ 8%).\n\nBâle II (2004) : architecture en trois piliers, reconnaissance des modèles internes.\n\nBâle III (2010–2011) : amélioration de la qualité du capital, coussins de conservation, ratios LCR et NSFR.\n\nBâle IV (2017) : remplacement de la VaR par l’ES pour le risque de marché, refonte de l’approche opérationnelle, encadrement accru des modèles internes."
  },
  {
    "objectID": "risques/index.html",
    "href": "risques/index.html",
    "title": "Risque & Régulation",
    "section": "",
    "text": "Dans cette première partie, nous présentons les bases de la gestion des risques bancaires. Elle rassemble les concepts, définitions et éléments méthodologiques de base.\nCette synthèse n’a pas vocation à être exhaustive : elle propose un cadre de référence minimal destiné à faciliter la compréhension des analyses, modèles et travaux présentés dans les sections suivantes."
  },
  {
    "objectID": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique-1",
    "href": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique-1",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "La Gestion des Risques : Une Fonction Stratégique",
    "text": "La Gestion des Risques : Une Fonction Stratégique\nAu cœur du système économique, les banques assurent une mission essentielle de circulation des capitaux entre épargnants et investisseurs. Cette position centrale les expose naturellement à divers risques financiers qu’elles doivent apprendre à maîtriser.\nLa gestion des risques a considérablement évolué : d’une fonction traditionnelle de contrôle, elle est devenue un véritable levier de création de valeur. Elle permet aujourd’hui une allocation optimale du capital et une tarification plus fine des produits financiers, tout en guidant les décisions stratégiques.\nLe risk manager moderne endosse un rôle crucial à l’interface des métiers. Véritable vigie, il anticipe les scénarios défavorables, conseille les équipes opérationnelles et éclaire la direction générale sur l’exposition aux risques."
  },
  {
    "objectID": "risques/chapter_two.html",
    "href": "risques/chapter_two.html",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "",
    "text": "⬅ Retour à Risque & Régulation"
  },
  {
    "objectID": "risques/chapter_two.html#comprendre-le-risque-de-marché",
    "href": "risques/chapter_two.html#comprendre-le-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Comprendre le Risque de Marché",
    "text": "Comprendre le Risque de Marché\nLe risque de marché correspond à la possibilité de subir une perte liée à l’évolution défavorable des prix sur les marchés financiers. Contrairement au risque de crédit, qui concerne la solidité financière d’une contrepartie, le risque de marché affecte directement la valeur des actifs détenus dans le portefeuille de négociation d’une banque.\nIl découle de mouvements de facteurs tels que les taux d’intérêt, les taux de change, les cours des actions, les spreads de crédit ou les prix des matières premières. Sa gestion exige une compréhension fine de ces facteurs et de leurs interactions, en particulier dans des conditions de volatilité élevée ou de stress de marché."
  },
  {
    "objectID": "risques/chapter_two.html#les-différentes-formes-du-risque-de-marché",
    "href": "risques/chapter_two.html#les-différentes-formes-du-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Les Différentes Formes du Risque de Marché",
    "text": "Les Différentes Formes du Risque de Marché\n\nRisque de Taux d’Intérêt\nLe risque de taux traduit la sensibilité des instruments financiers aux variations des taux d’intérêt. Il se manifeste principalement à travers :\n\nle risque de niveau, lié à un déplacement parallèle de la courbe des taux ;\nle risque de pente, lorsque les taux courts et les taux longs évoluent différemment ;\nle risque de courbure, qui apparaît lorsque la convexité ou la forme de la courbe se modifie.\n\n\n\nRisque de Change\nLorsque des positions sont libellées dans différentes devises, les fluctuations des taux de change peuvent générer des gains ou des pertes. Le risque de change est particulièrement significatif pour les institutions opérant à l’international ou pour les portefeuilles multi-devises.\n\n\nRisque Actions\nCe risque correspond à l’exposition aux variations des cours boursiers. Il combine :\n\nun risque systématique, lié à l’évolution générale du marché ;\nun risque spécifique, propre à chaque entreprise ou secteur.\n\n\n\nRisque Matières Premières\nLes prix des matières premières (pétrole, gaz, métaux, produits agricoles) sont influencés par des facteurs géopolitiques, climatiques ou économiques, rendant les expositions potentiellement très volatiles."
  },
  {
    "objectID": "risques/chapter_two.html#les-mesures-de-sensibilité",
    "href": "risques/chapter_two.html#les-mesures-de-sensibilité",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Les Mesures de Sensibilité",
    "text": "Les Mesures de Sensibilité\nLa gestion du risque de marché repose largement sur l’analyse des sensibilités, qui mesurent la réaction du prix d’un instrument à la variation d’un facteur de risque.\n\nLa Duration\nPour les actifs de taux, la durée (duration) quantifie la sensibilité du prix d’un instrument à une variation du taux d’intérêt :\n\\[\nD = -\\frac{1}{P}\\,\\frac{\\partial P}{\\partial y},\n\\]\noù \\(P\\) désigne le prix de l’instrument et \\(y\\) son rendement.\n\n\nLes Grecques\nPour les produits dérivés (instruments financiers dont la valeur dépend d’un actif sous-jacent), les sensibilités sont décrites à l’aide des Grecques, qui capturent différentes dimensions du risque :\n\nDelta :\n\\[\n\\Delta = \\frac{\\partial P}{\\partial S}\n\\] mesure la sensibilité au prix du sous-jacent \\(S\\) ;\nGamma :\n\\[\n\\Gamma = \\frac{\\partial^2 P}{\\partial S^2}\n\\] capture la convexité de la relation entre \\(P\\) et \\(S\\) ;\nVega : sensibilité à la volatilité \\(\\sigma\\) ;\nTheta : sensibilité au temps (décroissance de valeur liée à l’écoulement du temps) ;\nRho : sensibilité aux variations du taux d’intérêt \\(r\\)."
  },
  {
    "objectID": "risques/chapter_two.html#value-at-risk-et-expected-shortfall-appliqués-au-risque-de-marché",
    "href": "risques/chapter_two.html#value-at-risk-et-expected-shortfall-appliqués-au-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Value-at-Risk et Expected Shortfall appliqués au Risque de Marché",
    "text": "Value-at-Risk et Expected Shortfall appliqués au Risque de Marché\nLa Value-at-Risk (VaR) et l’Expected Shortfall (ES) sont les mesures principales utilisées pour quantifier le risque de marché à l’échelle du portefeuille.\n\nApproches de Calcul de la VaR\n\nMéthode Historique\nLa VaR historique repose sur les réalisations passées des facteurs de marché. On calcule les pertes journalières successives et on retient le quantile empirique :\n\\[\n\\text{VaR}_{\\alpha} = L_{(n\\alpha)},\n\\]\noù \\(L_{(n\\alpha)}\\) désigne la perte correspondant au quantile \\(\\alpha\\).\n\n\nMéthode Paramétrique (Variance–Covariance)\nSous des hypothèses de normalité des rendements et de linéarité du portefeuille :\n\\[\n\\text{VaR}_{\\alpha} = \\mu + \\Phi^{-1}(\\alpha)\\,\\sigma,\n\\]\noù \\(\\mu\\) et \\(\\sigma\\) représentent respectivement la moyenne et l’écart-type des pertes.\n\n\nMéthode Monte Carlo\nCette approche consiste à simuler un grand nombre de scénarios, puis à en extraire le quantile :\n\\[\n\\text{VaR}_{\\alpha} = F_L^{-1}(\\alpha),\n\\]\noù \\(F_L\\) est la distribution simulée des pertes.\n\n\n\nExpected Shortfall\nL’Expected Shortfall mesure la perte moyenne au-delà du quantile défini par la VaR :\n\\[\n\\text{ES}_{\\alpha}\n= \\mathbb{E}[\\, L \\mid L &gt; \\text{VaR}_{\\alpha}(L) \\,].\n\\]\nIl fournit une information complémentaire sur la sévérité des pertes extrêmes, particulièrement importante en période de stress."
  },
  {
    "objectID": "risques/chapter_two.html#la-gestion-avancée-du-risque-de-marché",
    "href": "risques/chapter_two.html#la-gestion-avancée-du-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "La Gestion Avancée du Risque de Marché",
    "text": "La Gestion Avancée du Risque de Marché\n\nBacktesting\nLe backtesting consiste à confronter les prévisions de risque aux pertes réellement observées.\nDeux dimensions sont généralement testées :\n\nla couverture inconditionnelle, qui vérifie que le nombre de violations est cohérent avec le niveau de confiance ;\nl’indépendance, qui s’assure que les exceptions ne sont pas regroupées en périodes de tension.\n\n\n\nStress Testing\nLe stress testing évalue l’impact sur le portefeuille de scénarios extrêmes mais plausibles, qu’ils soient inspirés de périodes historiques (crise de 2008, choc COVID-19) ou construits de manière hypothétique.\n\n\nAnalyse de Scénarios\nCette méthode complète le stress testing en étudiant des évolutions coordonnées de plusieurs facteurs de risque, permettant une évaluation plus fine des vulnérabilités."
  },
  {
    "objectID": "risques/chapter_two.html#le-cadre-réglementaire-bâle-iv-et-la-frtb",
    "href": "risques/chapter_two.html#le-cadre-réglementaire-bâle-iv-et-la-frtb",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Le Cadre Réglementaire : Bâle IV et la FRTB",
    "text": "Le Cadre Réglementaire : Bâle IV et la FRTB\nLa réforme FRTB (Fundamental Review of the Trading Book) issue de Bâle IV modernise profondément la mesure du risque de marché.\n\nApproche Standardisée\nElle repose sur :\n\ndes sensibilités (delta, vega, curvature),\ndes classes de risque homogènes,\ndes horizons de liquidité différenciés selon les instruments.\n\n\n\nApproche par Modèles Internes\nPlus exigeante, elle impose notamment :\n\nl’utilisation de l’ES au lieu de la VaR ;\nla prise en compte des horizons de liquidité ;\ndes critères de backtesting renforcés.\n\n\n\nExigences Quantitatives\nLes principaux paramètres sont :\n\nun niveau de confiance de 97,5% pour l’ES ;\nune période de calcul intégrant des données de stress ;\nune diversification reconnue mais strictement encadrée."
  },
  {
    "objectID": "risques/chapter_two.html#perspectives-dévolution",
    "href": "risques/chapter_two.html#perspectives-dévolution",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Perspectives d’Évolution",
    "text": "Perspectives d’Évolution\nLe risque de marché évolue avec les transformations des marchés financiers et l’apparition de nouveaux facteurs de risque. Parmi eux :\n\nl’innovation financière et les nouveaux instruments complexes ;\nl’augmentation de la fréquence des chocs de marché ;\nla prise en compte des risques climatiques ;\nl’usage croissant de l’intelligence artificielle dans les modèles.\n\nCes évolutions imposent une adaptation continue des outils et des cadres prudentiels afin de garantir la résilience du système financier."
  },
  {
    "objectID": "risques/chapter_one.html#les-principaux-risques-bancaires",
    "href": "risques/chapter_one.html#les-principaux-risques-bancaires",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Les Principaux Risques Bancaires",
    "text": "Les Principaux Risques Bancaires\nLa première étape du risk management consiste à identifier les sources d’incertitude susceptibles d’affecter la situation financière de la banque. On distingue traditionnellement plusieurs grandes catégories de risques.\n\nRisque de Marché\nLe risque de marché se manifeste lorsque la valeur des positions détenues par une banque évolue défavorablement sous l’effet des variations des facteurs de marché :\n\ntaux d’intérêt,\ntaux de change,\ncours des actions,\nprix des matières premières,\nspreads de crédit,\nconditions macroéconomiques générales.\n\nCe risque reflète la sensibilité globale du portefeuille à l’environnement financier. Certains actifs réagissent fortement à de petites variations de marché, tandis que d’autres sont davantage sensibles aux mouvements amples ou aux périodes d’instabilité durant lesquelles les dépendances entre actifs deviennent moins prévisibles.\n\n\nRisque de Crédit\nLe risque de crédit correspond à la possibilité qu’une contrepartie ne respecte pas ses obligations financières. Il constitue généralement la composante la plus importante du risque bancaire.\n\n\nRisque de Liquidité\nCe risque présente deux dimensions :\n\nliquidité de marché : capacité à vendre un actif sans perte de valeur excessive,\nliquidité de financement : capacité à se refinancer à un coût raisonnable.\n\n\n\nRisque Opérationnel\nIl résulte de défaillances internes (processus, systèmes, erreurs humaines) ou d’événements externes, et englobe notamment le risque juridique.\n\n\nAutres Risques\nParmi les risques complémentaires :\n\nrisque de réputation,\n\nrisque de modèle,\n\nrisque systémique.\n\n\nLes risques bancaires sont fortement interconnectés : une approche fragmentée peut conduire à sous-estimer les vulnérabilités globales."
  },
  {
    "objectID": "risques/chapter_one.html#les-mesures-de-risque",
    "href": "risques/chapter_one.html#les-mesures-de-risque",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Les Mesures de Risque",
    "text": "Les Mesures de Risque\nLa mesure du risque repose sur des outils statistiques permettant de quantifier les pertes potentielles associées à une position ou un portefeuille.\nUne mesure de risque, notée \\(\\mathcal{R}(L)\\) pour une perte aléatoire \\(L\\), représente généralement le capital économique nécessaire pour couvrir cette perte dans un scénario défavorable.\n\nMesures de Risque Cohérentes\nArtzner et al. ont défini quatre propriétés fondamentales caractérisant une mesure de risque cohérente :\n\nInvariance par translation \\[\n\\mathcal{R}(L + c) = \\mathcal{R}(L) + c\n\\]\nHomogénéité positive \\[\n\\mathcal{R}(\\lambda L) = \\lambda \\mathcal{R}(L)\n\\]\nMonotonie \\[\nL_1 \\le L_2 \\;\\Rightarrow\\; \\mathcal{R}(L_1) \\le \\mathcal{R}(L_2)\n\\]\nSous-additivité \\[\n\\mathcal{R}(L_1 + L_2) \\le \\mathcal{R}(L_1) + \\mathcal{R}(L_2)\n\\]\n\nCes propriétés assurent un comportement économique raisonnable de la mesure du risque.\nDans ce cadre, la Value-at-Risk (VaR) est devenue l’outil le plus couramment utilisé en pratique, notamment dans les réglementations prudentielles."
  },
  {
    "objectID": "risques/chapter_three.html",
    "href": "risques/chapter_three.html",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "",
    "text": "⬅ Retour à Risque & Régulation"
  },
  {
    "objectID": "risques/chapter_three.html#comprendre-le-risque-de-crédit",
    "href": "risques/chapter_three.html#comprendre-le-risque-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Comprendre le Risque de Crédit",
    "text": "Comprendre le Risque de Crédit\nLe risque de crédit correspond à la possibilité qu’un emprunteur ou une contrepartie ne respecte pas ses obligations financières. Pour la plupart des institutions bancaires, il constitue la composante la plus importante du risque global, car il concerne directement la capacité des clients à rembourser leurs dettes.\nContrairement au risque de marché, qui résulte de la fluctuation des prix financiers, le risque de crédit se focalise sur la solvabilité. Sa gestion repose sur l’évaluation de la qualité des contreparties, la modélisation des comportements de défaut et l’estimation des pertes potentielles."
  },
  {
    "objectID": "risques/chapter_three.html#les-concepts-clés-du-risque-de-crédit",
    "href": "risques/chapter_three.html#les-concepts-clés-du-risque-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Les Concepts Clés du Risque de Crédit",
    "text": "Les Concepts Clés du Risque de Crédit\nLes modèles réglementaires et internes reposent sur quatre paramètres fondamentaux.\n\nProbabilité de Défaut (PD)\nLa probabilité de défaut (PD) mesure la probabilité qu’une contrepartie fasse défaut sur un horizon donné, le plus souvent un an :\n\\[\nPD = \\mathbb{P}(\\text{Défaut})\n\\]\n\n\nPerte en Cas de Défaut (LGD)\nLa LGD (Loss Given Default) représente la proportion de l’exposition qui serait perdue en cas de défaut après prise en compte des recouvrements :\n\\[\nLGD = 1 - \\text{Taux de Recouvrement}\n\\]\n\n\nExposition au Moment du Défaut (EAD)\nL’exposition au défaut (EAD) correspond au montant effectivement engagé au moment du défaut.\nPour les engagements conditionnels, elle inclut un facteur de conversion :\n\\[\nEAD = \\text{Encours Courant} + \\text{Engagements Non Utilisés} \\times CCF\n\\]\n\n\nPerte Attendue (EL)\nLes trois paramètres précédents permettent de définir la perte attendue, composante centrale de la gestion du risque de crédit :\n\\[\nEL = PD \\times LGD \\times EAD\n\\]"
  },
  {
    "objectID": "risques/chapter_three.html#les-marchés-du-crédit",
    "href": "risques/chapter_three.html#les-marchés-du-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Les Marchés du Crédit",
    "text": "Les Marchés du Crédit\n\nMarché des Prêts Bancaires\nLe marché du crédit bancaire se divise principalement en :\n\nBanque de détail : crédits aux particuliers et aux petites entreprises\n\nBanque corporate : financements aux grandes entreprises\n\nProcessus de scoring : modèles automatisés d’évaluation du risque\n\n\n\nMarché Obligataire\nLes entreprises et États peuvent se financer via l’émission d’obligations :\n\nObligations souveraines : exposition au risque de crédit de l’État\n\nObligations corporate : risque propre aux entreprises\n\nSpread de crédit : prime de risque exigée pour compenser un risque de défaut\n\n\n\nTitrisation et Dérivés de Crédit\n\nTitrisation\nLa titrisation permet de transformer des actifs illiquides en titres négociables :\n\nABS : Asset-Backed Securities\n\nMBS : Mortgage-Backed Securities\n\nCDO : Collateralized Debt Obligations\n\n\n\nDérivés de Crédit\nLe CDS (Credit Default Swap) est l’instrument de référence pour transférer le risque de crédit :\n\nl’acheteur de protection paie une prime périodique,\n\nle vendeur de protection indemnise en cas de défaut."
  },
  {
    "objectID": "risques/chapter_three.html#le-cadre-réglementaire-bâle-ii-iii",
    "href": "risques/chapter_three.html#le-cadre-réglementaire-bâle-ii-iii",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Le Cadre Réglementaire Bâle II / III",
    "text": "Le Cadre Réglementaire Bâle II / III\n\nApproche Standardisée\nLes pondérations de risque sont déterminées à partir des notations externes. Exemple simplifié :\n\n\n\nNotation\nPondération\n\n\n\n\nAAA–AA\n20%\n\n\nA+\n50%\n\n\nBBB\n100%\n\n\nBB\n150%\n\n\n\n\n\nApproche Fondation (FIRB)\nDans l’approche FIRB :\n\nLa banque estime la PD\n\nLa LGD et l’EAD sont standardisées\n\nLe capital est calculé via la formule réglementaire\n\n\n\nApproche Avancée (AIRB)\nL’approche AIRB permet à la banque d’estimer :\n\nPD\n\nLGD\n\nEAD\n\nmaturité effective (M)\n\nCes approches offrent plus de précision mais exigent une validation stricte des modèles."
  },
  {
    "objectID": "risques/chapter_three.html#modélisation-des-paramètres-de-crédit",
    "href": "risques/chapter_three.html#modélisation-des-paramètres-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Modélisation des Paramètres de Crédit",
    "text": "Modélisation des Paramètres de Crédit\n\nModélisation de la PD\n\nApproche par Cohortes\nLa méthode des cohortes repose sur l’historique de défauts :\n\\[\nPD_c = \\frac{\\sum D^c_T}{\\sum N^c_T}\n\\]\n\n\nApproche Statistique\nLes modèles statistiques, comme la régression logistique, relient la PD aux caractéristiques des emprunteurs :\n\\[\nPD = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n)}}\n\\]\n\n\n\nModélisation de la LGD\n\nLGD de Marché\nEstimée à partir des prix observés après défaut :\n\\[\nLGD_{\\text{Marché}} = 1 - \\frac{\\text{Prix Après Défaut}}{\\text{Nominal}}\n\\]\n\n\nLGD de Recouvrement\nBasée sur les flux de recouvrement actualisés :\n\\[\nLGD_{\\text{Recouvrement}} = \\frac{EAD - \\text{Recouvrements Actualisés}}{EAD}\n\\]\n\n\n\nModélisation de l’EAD\nPour les engagements conditionnels :\n\\[\nEAD = \\text{Encours} + \\text{Non Utilisé} \\times CCF\n\\]"
  },
  {
    "objectID": "risques/chapter_three.html#le-capital-économique-pour-le-risque-de-crédit",
    "href": "risques/chapter_three.html#le-capital-économique-pour-le-risque-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Le Capital Économique pour le Risque de Crédit",
    "text": "Le Capital Économique pour le Risque de Crédit\n\nValue-at-Risk Crédit\nLe capital économique est défini comme la différence entre la VaR et la perte attendue :\n\\[\nEC_{\\alpha} = VaR_{\\alpha} - EL\n\\]\n\n\nModèle de Vasicek\nDans le cadre du modèle à un facteur systémique :\n\\[\nVaR_{\\alpha}\n= \\Phi\\!\\left(\n\\frac{\\Phi^{-1}(PD) + \\sqrt{\\rho}\\,\\Phi^{-1}(\\alpha)}\n     {\\sqrt{1-\\rho}}\n\\right)\n\\]\nCe modèle constitue la base de la formule réglementaire IRB."
  },
  {
    "objectID": "risques/chapter_three.html#évolutions-règlementaires-récentes",
    "href": "risques/chapter_three.html#évolutions-règlementaires-récentes",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Évolutions Règlementaires Récentes",
    "text": "Évolutions Règlementaires Récentes\n\nBâle III\nParmi les renforcements :\n\nexigences plus strictes pour les expositions financières,\ncoussin de conservation du capital,\nexigences spécifiques pour les portefeuilles de trading.\n\n\n\nBâle IV\nLes réformes introduisent :\n\ndes planchers limitant la réduction des RWA via modèles internes,\nune révision des pondérations standardisées,\nun traitement plus prudent des expositions actions et immobilières."
  },
  {
    "objectID": "risques/chapter_three.html#perspectives-et-défis",
    "href": "risques/chapter_three.html#perspectives-et-défis",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Perspectives et Défis",
    "text": "Perspectives et Défis\nLa gestion du risque de crédit doit s’adapter à plusieurs tendances majeures :\n\nintégration des risques climatiques,\n\nmontée en puissance des techniques de machine learning,\n\ncycles économiques plus incertains,\n\nexploitation de données massives.\n\nCes évolutions nécessitent une amélioration continue des méthodes d’évaluation et des cadres prudentiels."
  },
  {
    "objectID": "risques/chapter_four.html",
    "href": "risques/chapter_four.html",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "",
    "text": "⬅ Retour à Risque & Régulation"
  },
  {
    "objectID": "risques/chapter_four.html#comprendre-le-risque-de-contrepartie",
    "href": "risques/chapter_four.html#comprendre-le-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Comprendre le Risque de Contrepartie",
    "text": "Comprendre le Risque de Contrepartie\nLe risque de contrepartie correspond à la possibilité qu’une partie engagée dans une transaction financière ne respecte pas ses obligations contractuelles. Il est particulièrement important pour les produits dérivés (options, swaps, forwards) et certaines opérations de financement (prêts de titres, pensions livrées).\nContrairement au risque de crédit traditionnel, où le montant dû est connu à l’avance, le risque de contrepartie porte sur des expositions futures incertaines, dépendant de l’évolution des marchés. La valeur d’un contrat peut devenir positive ou négative selon les mouvements du sous-jacent, ce qui rend la mesure du risque plus complexe."
  },
  {
    "objectID": "risques/chapter_four.html#les-spécificités-du-risque-de-contrepartie",
    "href": "risques/chapter_four.html#les-spécificités-du-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Les Spécificités du Risque de Contrepartie",
    "text": "Les Spécificités du Risque de Contrepartie\n\nRisque Bilatéral\nDans la majorité des dérivés, chaque partie peut tour à tour être créancière ou débitrice selon l’évolution du marché. L’exposition est donc bilatérale et dépend du sens des variations du sous-jacent.\n\n\nValeur Incertaine des Expositions\nLa perte potentielle en cas de défaut n’est pas fixe : elle dépend du mark-to-market du contrat au moment où le défaut survient. Ainsi, le risque de contrepartie est intimement lié à la dynamique du marché et peut varier considérablement dans le temps."
  },
  {
    "objectID": "risques/chapter_four.html#mesure-de-lexposition",
    "href": "risques/chapter_four.html#mesure-de-lexposition",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Mesure de l’Exposition",
    "text": "Mesure de l’Exposition\nLa gestion du risque de contrepartie repose sur la définition de plusieurs mesures d’exposition potentielle ou attendue.\n\nExposition Potentielle Future (PFE)\nLa PFE représente une borne supérieure de l’exposition future pour un niveau de confiance donné. Elle joue un rôle analogue à une Value-at-Risk appliquée aux expositions positives :\n\\[\nPFE_{\\alpha}(t) = \\text{Quantile}_{\\alpha}\\big( \\max(V_t, 0) \\big)\n\\]\noù ( V_t ) est la valeur future du contrat.\n\n\nExposition Attendue (EE)\nL’exposition attendue à une date future correspond à la moyenne des expositions positives possibles :\n\\[\nEE(t) = \\mathbb{E}\\big[ \\max(V_t, 0) \\big]\n\\]\n\n\nExposition Attendue Positive (EPE)\nL’EPE est la moyenne pondérée de l’EE sur l’horizon considéré. C’est une mesure synthétique largement utilisée dans les approches réglementaires :\n\\[\nEPE = \\frac{1}{T} \\int_0^T EE(t)\\,dt\n\\]"
  },
  {
    "objectID": "risques/chapter_four.html#techniques-de-réduction-du-risque",
    "href": "risques/chapter_four.html#techniques-de-réduction-du-risque",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Techniques de Réduction du Risque",
    "text": "Techniques de Réduction du Risque\n\nCompensation (Netting)\nLes contrats conclus avec une même contrepartie peuvent être regroupés dans une convention de compensation, permettant de compenser les valeurs positives et négatives :\n\nSans netting : les expositions s’additionnent\n\nAvec netting : seule l’exposition nette est considérée\n\nCela réduit mécaniquement la perte potentielle en cas de défaut.\n\n\nCollatéralisation\nLes accords de collatéral (CSA) prévoient l’échange de garanties pour couvrir l’exposition courante. Les montants sont ajustés régulièrement pour refléter l’évolution de la valeur des positions.\nLe processus comprend généralement :\n\nCalcul quotidien du mark-to-market\n\nDétermination de l’exposition nette\n\nDépôt ou restitution de collatéral selon les seuils contractuels\n\n\n\nAppels de Marge\nLorsque la valeur du contrat varie défavorablement pour une partie, celle-ci peut être tenue de fournir des garanties additionnelles. Les appels de marge constituent un mécanisme essentiel pour limiter l’accumulation d’expositions."
  },
  {
    "objectID": "risques/chapter_four.html#le-cva-valorisation-du-risque-de-contrepartie",
    "href": "risques/chapter_four.html#le-cva-valorisation-du-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Le CVA : Valorisation du Risque de Contrepartie",
    "text": "Le CVA : Valorisation du Risque de Contrepartie\n\nDéfinition\nLe Credit Valuation Adjustment (CVA) correspond à la diminution de la valeur théorique d’un contrat dérivé liée au risque de défaut de la contrepartie. Il s’agit de l’écart entre la valeur du contrat dans un monde sans défaut et sa valeur ajustée du risque de crédit.\nFormellement :\n\\[\nCVA = V_{\\text{sans risque}} - V_{\\text{avec risque}}\n\\]\n\n\nImportance du CVA\nLors de la crise de 2008, une grande partie des pertes liées aux dérivés provenait de la variation du CVA plutôt que de défauts avérés. Le CVA est devenu un déterminant central de la valorisation et du capital réglementaire.\n\n\nFormule Simplifiée\nUne approximation classique du CVA est :\n\\[\nCVA = (1 - R)\\, \\sum_{t} EE(t)\\, PD(t)\n\\]\noù :\n\n( R ) : taux de recouvrement\n\n( EE(t) ) : exposition attendue\n\n( PD(t) ) : probabilité de défaut marginale\n\nCette formule met en lumière les trois composantes essentielles du CVA : exposition, risque de défaut, sévérité de la perte."
  },
  {
    "objectID": "risques/chapter_four.html#le-cadre-réglementaire",
    "href": "risques/chapter_four.html#le-cadre-réglementaire",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Le Cadre Réglementaire",
    "text": "Le Cadre Réglementaire\n\nBâle III — Capital CVA\nBâle III introduit un capital CVA destiné à couvrir non seulement le défaut mais aussi la dégradation de la qualité de crédit des contreparties. Deux approches coexistent :\n\n\nApproche Standardisée\nBasée sur des coefficients réglementaires et les notations externes des contreparties.\n\n\nApproche Avancée\nLes banques peuvent utiliser leurs propres modèles internes pour le calcul du CVA et du capital associé, sous conditions de validation prudentielle stricte."
  },
  {
    "objectID": "risques/chapter_four.html#défis-de-la-modélisation",
    "href": "risques/chapter_four.html#défis-de-la-modélisation",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Défis de la Modélisation",
    "text": "Défis de la Modélisation\n\nRisque de Wrong-Way\nLe risque de wrong-way survient lorsque l’exposition augmente précisément lorsque la contrepartie devient plus susceptible de faire défaut.\nExemple typique : contrepartie très exposée au prix d’une matière première qui s’effondre.\n\n\nQualité des Données\nLa mesure du CVA repose sur :\n\ndes données de marché (courbes de taux, volatilités, CDS),\ndes données contractuelles (CSA, netting),\nune modélisation du défaut.\n\nL’agrégation et la cohérence de ces données constituent un défi opérationnel majeur.\n\n\nComplexité des Modèles\nLes modèles doivent intégrer :\n\ndépendance entre exposition et défaut,\neffets de la compensation,\ndynamique du collatéral,\nmigrations de crédit.\n\nLa sophistication croissante des instruments accroît les exigences de modélisation."
  },
  {
    "objectID": "risques/chapter_four.html#bonnes-pratiques-pour-la-gestion-du-risque-de-contrepartie",
    "href": "risques/chapter_four.html#bonnes-pratiques-pour-la-gestion-du-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Bonnes Pratiques pour la Gestion du Risque de Contrepartie",
    "text": "Bonnes Pratiques pour la Gestion du Risque de Contrepartie\n\nSuivi continu des expositions et de la qualité de crédit\n\nLimites d’exposition par contrepartie, secteur, notation\n\nDiversification des partenaires contractuels\n\nCommunication claire entre équipes (front office, risque, opérations)\n\nTests de stress centrés sur les expositions futures"
  },
  {
    "objectID": "risques/chapter_four.html#perspectives-et-défis",
    "href": "risques/chapter_four.html#perspectives-et-défis",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Perspectives et Défis",
    "text": "Perspectives et Défis\nLe risque de contrepartie évolue face à plusieurs enjeux stratégiques :\n\nintégration des risques climatiques dans les modèles,\nrôle croissant de la compensation centrale,\ndéveloppement de la blockchain et des smart contracts,\névolution des standards comptables et prudentiels.\n\nLa gestion rigoureuse du risque de contrepartie demeure essentielle pour la résilience du système financier."
  },
  {
    "objectID": "risques/index.html#mes-publications",
    "href": "risques/index.html#mes-publications",
    "title": "Risque & Régulation",
    "section": "Mes publications",
    "text": "Mes publications\n\n\n\n\n\nGestion de la liquidité des actifs\n\n\nProfondeur de marché, calcul du profil d’écoulement, liquidation avec ou sans impact de marché.\n\nLire"
  },
  {
    "objectID": "risques/index.html#section",
    "href": "risques/index.html#section",
    "title": "Risque & Régulation",
    "section": " ",
    "text": "Gestion de la liquidité des actifs\n\n\nProfondeur de marché, calcul du profil d’écoulement, liquidation avec ou sans impact de marché.\n\nLire"
  },
  {
    "objectID": "risques/projets/demo.html",
    "href": "risques/projets/demo.html",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "",
    "text": "Ce document présente une introduction à la gestion des risques en Asset Management.\nL’objectif principal est de comprendre et d’appliquer les méthodologies de base utilisées pour analyser, mesurer et suivre les risques financiers au sein d’un portefeuille.\nL’étude couvre differentes notions de risk management notamment:\n- les méthodes de mesure du risque de marché (sensibilité, volatilité, Value-at-Risk, Tracking Error, stress tests et profil de liquidité),\n- les composantes clés du risque de crédit (intensité de défaut, sensibilité des obligations aux variations de spreads, triangle du crédit),\n- le risque de contrepartie et ses mécanismes de mitigation via les appels de marge,\n- ainsi que les risques émergents tels que le risque climatique et le risque modèle.\n\n\nCode\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import norm\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize"
  },
  {
    "objectID": "risques/projets/demo.html#i.2.-suivi-du-porte-feuille",
    "href": "risques/projets/demo.html#i.2.-suivi-du-porte-feuille",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "I.2. Suivi du porte feuille",
    "text": "I.2. Suivi du porte feuille\n\nDans cette section on va determiner l’AUM du portefeuille, suivre ses performances sa volatilité et ses performances relatives par rapport au CAC40.\nNous effectuerons également un stress test sur le portefeuille pour ebaluer sa sensibilité relative à la période COVID.\n\n\nCode\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), weights*100, color='skyblue')\n\nplt.title(f\"Poids optimaux des actifs dans le portefeuille (Volatilité: {round(vol, 4)})\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Poids (en %)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, weights):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "risques/projets/demo.html#stress-test",
    "href": "risques/projets/demo.html#stress-test",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Stress test",
    "text": "Stress test\nLe stress test ou test de résistance est un exercice qui consiste à reproduire des scénarios extrêmes sur un portefeuille afin d’évaluer sa résilience en conjoncture défavorable. On distingue notamment deux types de stress test: - Le stress test historique qui vise à reproduire des scénarios de crises historisques, - Le stress test hypothétique qui consiste à considérer des scénarios théoriques extrêmes.\nNous allons dans la suite implémenter un stress test historique en reproduisant le choc COVID entre 19/02/2020 et 19/03/2020. Il s’agit s’agira de calculer la performance du fonds entre ces deux date et de la comparer à celle de son indice de référence.\n\nDonnées des actions en période covid\n\n\nCode\n# Données des actions en période covid\nstress_date_1 = \"2020-02-19\"\nstress_date_2 = \"2020-03-19\"\n\ntickers = ['SAN.PA', 'BN.PA',  'AIR.PA', 'AI.PA', 'ORA.PA', 'CAP.PA', 'VIV.PA', 'CA.PA', 'ENGI.PA', 'DG.PA']\ndf_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\ncac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\3326232529.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**********            20%                       ]  2 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\3326232529.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\naum_stress = df_stress@weights\nportfolio_stress = pd.DataFrame({'AUM': aum_stress})\n\n\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\nportfolio_stress.iloc[-1,] /portfolio_stress.iloc[0,]  - 1\n\n\nAUM   -0.335339\ndtype: float64\n\n\n\n\nRendement du benchmark enn période covid\n\n\nCode\n# Rendemnt du benchmark enn période covid\ncac40_stress.iloc[-1,] / cac40_stress.iloc[0,] -1\n\n\nTicker\n^FCHI   -0.385585\ndtype: float64\n\n\nLe fonds stressé a une performance absolue de -33.69% contre -38.55% pour son benchmark. Ces performances en période de crise COVID sont du même ordre de grandeur. Toutefois, la perte enregistrée sur le fonds est moins importante. Une analyse des performances des actions constituant le fonds montre que la sous-performance enegistrée est essentiellement portée par: Air Liquide, Capgenie et Engie.\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\ndf_stress.iloc[-1,] /df_stress.iloc[0,]  - 1\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1, color='skyblue')\n\nplt.title(f\"Performance de la composition du fond stressé\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Performance (%)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "risques/projets/demo.html#cadre-de-suivi-de-la-liquidité",
    "href": "risques/projets/demo.html#cadre-de-suivi-de-la-liquidité",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Cadre de suivi de la liquidité",
    "text": "Cadre de suivi de la liquidité\n\nLe suivi de la liquidité est d’une importance centrale en gestion de fonds et est encadré par des exigences règlementaires. Il s’agit entre autres pour le gestionnaire de pouvoir honorer ses engagements vis-à-vis de des investisseurs en cas de rachat de parts du fonds. Il faut donc s’assurer de la capacité à revendre les actifs composant le fonds dans des délais courts sans toutefois les brader.\nC’est ce qu’on appelle le risque de liquidité qui se défini plus formellement comme étant la facilité avec laquelle une entreprise peut échanger ses actifs contre du cash, même en situation soudaine de besoin de liquidité, sans subir de coûts anormaux par rapport aux autres acteurs du marché.\nLe suivi de la liquidité d’un fonds s’effectue par le calcul de son profil d’écoulement en condition normale et en condition stressée.\n\nLa méthodologie de calcul d’un profil d’écoulement\nLe fonds est constitué de n actifs en quantité \\(Q_i\\) chacun à liquider. Dans la pratique il existe une quantité maximale au delà de laquelle les échanges ne peuvent se faire sans subir de coûts anormaux: c’est la profondeur de marché. Le coût de liquidation est induit par les volume d’ordre émis.\nLa profondeur de marché est généralement estimée en examinant: - Les carnets d’ordres ou - Les volumes quotidiens échangés: On considère notamment le volume quotidien moyen échangé sur 3 mois appelé ADV ou ATV (Average Daily/Traded Volume). La profondeur de marché est alors estimée par \\(Q^* = 20\\% \\times ADV_{3 mois}\\)\n\nOn distingue quatre profils d’écoulement: - En codition normale: avec et sans déformation - En condition stressée: avec déformation et sans déformation\nDans une liquidation avec déformation, les actifs les plus liquides sont liquidés en premiers. La déformation du portefeuille s’illustre par les changements de poids des différents émetteurs dans le fonds. La liquidation sans déformation quant à elle consistera à liquider le fonds au rythme de l’actif le moins liquide afin de préserver les poids initiaux.\nN.B: Le profil de liquidation est représenté en proportion du portefeuille initial.\nOn utilise le profil de liquidité pour déterminer la taille optimal du fonds permettant de respecter les exigence de minimum de liquidité. Il s’agira de s’assurer que plus de 90% du fonds est liquidé en deux jours, laissant ainsi un maximum de 10 % dans la catégorie dite “poubelle” (c’est-à-dire les actifs moins liquides).\nEn condition stressée avec déformation: On se place dans une situation dans laquelle la profondeur de marché est réduite à la suite d’un choc conduisant à un assèchement du marché.\n\n\nCode\ndef liqudity(df_volume, ADV_rate=0.2, weight_adjust=1, label=\"Conditions normales avec déformation\", seed = 42):\n    \"\"\"\n    Fonction de calcul de la liquidité d'un portefeuille sur plusieurs jours.\n    \n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        DataFrame contenant les volumes quotidiens pour chaque actif (tickers).\n    ADV_rate : float, optionnel\n        Taux de liquidité journalière exprimé en pourcentage de l'ADV (par défaut 0.2, soit 20%).\n    weight_adjust : float, optionnel\n        Facteur de pondération pour ajuster la quantité générée aléatoirement (par défaut 1).\n    label : str, optionnel\n        Label pour décrire le scénario de liquidation (par défaut \"Conditions normales avec déformation\").\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        DataFrame contenant les informations suivantes pour chaque actif :\n            - 'Ticker' : Le symbole de l'actif.\n            - 'ADV' : Le volume moyen quotidien calculé sur une fenêtre de 60 jours.\n            - 'Prix' : Le prix actuel de l'actif (extrait de df_close).\n            - 'QUANTITE' : La quantité totale simulée de l'actif détenu par le portefeuille.\n            - 'QUANTITE LIQUIDABLE' : La quantité qui peut être liquidée quotidiennement (ADV_rate * ADV).\n            - 'NB_JOURS DE LIQUIDATION' : Nombre de jours estimés pour liquider entièrement l'actif.\n            - 'JOUR X' (où X est un numéro de jour) : Quantité restante après chaque jour de liquidation.\n            - 'POIDS X' : Poids relatif de chaque actif dans le portefeuille après chaque jour de liquidation.\n    \n    Description :\n    -------------\n    1. Calcul de l'Average Daily Volume (ADV) sur une fenêtre de 60 jours.\n    2. Création d'un DataFrame contenant les informations de liquidité de chaque actif.\n    3. Génération de quantités d'actifs aléatoires pour simuler un portefeuille.\n    4. Calcul de la quantité liquidable par jour (20 % de l'ADV par défaut).\n    5. Simulation de la liquidation progressive de chaque actif jour par jour jusqu'à épuisement.\n    6. Calcul des poids du portefeuille après chaque jour de liquidation.\n    \n    \"\"\"\n    # Calcul de la moyenne mobile sur 60 jours du volume de transactions (ADV : Average Daily Volume)\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de l'ADV le plus récent pour chaque ticker\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame contenant l'ADV, le prix actuel et d'autres informations pour chaque actif\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # On définit la colonne 'Ticker' comme index pour faciliter l'accès aux données\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Génération de quantités d'actifs simulées basées sur l'ADV\n    np.random.seed(seed)  # Pour des résultats reproductibles\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable : 20 % de l'ADV (peut être ajusté via 'ADV_rate')\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours nécessaires pour liquider chaque actif\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(\n        portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE']\n    )\n\n    # Initialisation pour le jour 0\n    i = 1\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    # Calcul de la quantité restante après le premier jour de liquidation\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    # Calcul itératif jusqu'à liquidation complète\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        # Calcul de la quantité restante après chaque jour supplémentaire\n        portfolio_liquidity[f'JOUR {i}'] = (\n            portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n        ).clip(lower=0)\n\n        # Mise à jour des poids du portefeuille pour ce jour\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    return portfolio_liquidity\n\n\n\n\nCode\ndef liquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales avec déformation\"):\n    \"\"\"\n    Fonction qui génère un profil de liquidité d'un portefeuille en fonction du nombre de jours nécessaires pour liquider tous les actifs.\n\n    Paramètres :\n    -----------\n    portfolio_liquidity : DataFrame\n        Le DataFrame généré par la fonction liqudity() contenant les quantités restantes par jour pour chaque actif.\n    label : str, optionnel\n        Titre du graphique affiché (par défaut \"Profil de liquidité en conditions normales avec déformation\").\n\n    Description :\n    -------------\n    1. Calcule la quantité restante d'actifs pour chaque jour jusqu'à liquidation totale.\n    2. Calcule le profil de liquidité en pourcentage du portefeuille initial liquidé au fil du temps.\n    3. Génère deux graphiques :\n       - Profil de liquidité en pourcentage du portefeuille initial.\n       - Évolution des poids des actifs au fil des jours.\n\n    Retourne :\n    ---------\n    Affiche deux graphiques côte à côte montrant la liquidation progressive et l'évolution des poids.\n    \"\"\"\n    # Calculer le profil de liquidité\n    liquidity_profile = [portfolio_liquidity['QUANTITE']]\n    for i in range(1, int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        liquidity_profile.append(portfolio_liquidity[f'JOUR {i}'])\n\n    liquidity_profile = pd.DataFrame(liquidity_profile)\n\n    # Calculer le profil en pourcentage de liquidité restante\n    profile = liquidity_profile @ portfolio_liquidity['Prix']\n    portefeuille_valeur = profile['QUANTITE']  # Récupérer la valeur du portefeuille\n    profile = 100 - profile[1:,] * 100 / portefeuille_valeur\n\n    # Créer une figure avec deux sous-graphiques côte à côte\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))  \n\n    # Premier graphique : Profil de liquidité\n    ax1 = axes[0]\n    profile.plot(ax=ax1, marker='o', legend=False, color='blue')\n    barplot = profile.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black', alpha=0.5)\n\n    # Affichage des valeurs sur les barres\n    for bar in barplot.patches:\n        value = round(bar.get_height(), 2)\n        ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{value}\",\n                 ha='center', va='bottom', fontsize=10, color='black')\n\n    ax1.set_title(f\"{label}\", fontsize=14)\n    ax1.set_xlabel(\"Jours\", fontsize=12)\n    ax1.set_ylabel(\"Liquidité du portefeuille (%)\", fontsize=12)\n    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Étiquette de valeur du portefeuille\n    ax1.text(0.95, 0.05, f\"Valeur portefeuille : {round(portefeuille_valeur)} euros\",\n             transform=ax1.transAxes, fontsize=12, color='red', ha='right', va='bottom',\n             bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"red\"))\n\n\n    # Deuxième graphique : Évolution des poids\n    ax2 = axes[1]\n    for i in range(int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        if f'POIDS {i}' in portfolio_liquidity.columns:\n            ax2.plot(portfolio_liquidity[f'POIDS {i}'], label=f'Jour {i}')\n        else:\n            print(f\"Colonne 'POIDS {i}' non trouvée.\")\n\n    ax2.set_title(\"Évolution des Poids au Fil des Jours\", fontsize=14)\n    ax2.set_xlabel(\"Index\", fontsize=12)\n    ax2.set_ylabel(\"Poids\", fontsize=12)\n    ax2.legend()\n    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Ajustement de l'affichage\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nProfil d’écoulement avec déformation\n\nProfil d’écoulement en conditions normales avec déformation\n\n\nCode\nportfolio_liquidity = liqudity(df_volume)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLe portefeuille étudié est liquidable en 8 jours au minimum. Au premier jour seul 23.17% du portefeuille est liquidable et atteint les 73.54% au 4eme jour. On observe que les poids des actifs composant le portefeuille sont déformés au fil des jours. Il y a alors une concentration du portefeuille sur les actifs les moins liquides. Cette situation est domageable pour les investisseurs restant dans le fonds, car il faudra plus de temps pour honorer les engagements du fonds vis-à-vis d’eux en cas de rachat ce qui augmente leur exposition au risque de liquidité..\nAfin de protéger ces investisseurs, le régulateur impose aux gérants de fonds de fixer des GATES qui doivent obligatoirement être présentés dans leurs prospectus. Cela consiste à donner une option au gérant pour restreindre les rachats quotidien, à 5% du fonds par exemple.\n\n\n\nProfil d’écoulement en conditions stressée avec déformation\nEn conjoncture défavorable, la profondeur de marché est réduite ce qui entraîne un contraction des volumes liquidables. Nous simulons un tel choc en réduisant la profondeur de marché à 10% l’ADV.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLa contraction de la profondeur de marché rallonge la durée minimale de liquidation du fonds à 15 jours contre 8 jours initialement. Ceci illustre bien l’exposition du fonds au risque de liquidité. Il devient donc indispensable pour le gérant du fonds de déterminer une taille optimale de celui-ci afin de réduire son exposition au risque de liquidité.\n\n\n\nTaille optimale du portefeuille\n\nOn peut ajuster la taille du portefeuille en faisant varier le paramètre weight_adjust de la fonction liqudity afin de déterminer la taille optimale du fonds liquidable en 1 jour.\nPour le cas étudié, le fonds optimal a une valeur de 123 184 238 euros dont 99.82% est liquidable en 1 jour.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\nEn repétant le scénario défavorable d’un choc qui réduit la profondeur de marché à 10% l’ADV, le fonds optimal ainsi constitué est liquidable à 99.82% en deux jours. Il est donc plus résilient.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\n\n\n\nProfil d’écoulement sans déformation\nRappelons qu’une politique de gestion sans déformation consiste à appliquer des rachats proformat. Ceci garantisse une stabilité de la composition du fonds.\n\n\nCode\ndef liqudity_proformat(df_volume, ADV_rate = 0.2, weight_adjust = 1):\n    \"\"\"\n    Fonction qui génère un DataFrame décrivant la liquidité d'un portefeuille sur plusieurs jours, en fonction de la quantité disponible par jour et des poids correspondants.\n\n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        Un DataFrame contenant les volumes quotidiens de chaque ticker.\n    ADV_rate : float, optionnel\n        Le pourcentage d'Average Daily Volume (ADV) disponible chaque jour pour être liquidé (par défaut : 0.2, soit 20 %).\n    weight_adjust : float, optionnel\n        Facteur d'ajustement appliqué pour moduler la quantité calculée (par défaut : 1).\n\n    Description :\n    -------------\n    1. Calcule l'ADV (volume moyen sur 60 jours) pour chaque actif.\n    2. Génère un DataFrame contenant l'ADV, les prix actuels, les quantités à liquider, et les quantités liquidables par jour.\n    3. Calcule le nombre de jours nécessaire pour liquider complètement chaque actif.\n    4. Produit une série de DataFrames pour chaque jour, montrant la quantité restante, les poids associés, et les vitesses de liquidation.\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        Un DataFrame contenant les informations de liquidité du portefeuille sur plusieurs jours.\n    \"\"\"\n    # Calcul de la moyenne mobile du volume (ADV) sur une fenêtre de 60 jours\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de la dernière ligne (les valeurs ADV les plus récentes)\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame pour la liquidité du portefeuille\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # Définir 'Ticker' comme index pour un accès plus facile\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Calcul de la quantité (QUANTITE) basée sur le volume le plus récent\n    np.random.seed(42)\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable par jour\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours de liquidation nécessaire\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE'])\n\n    i = 1\n    # Initialisation du premier jour\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity['QUANTITE']\n\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        portfolio_liquidity[f'JOUR {i}'] = (\n            (portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'] * portfolio_liquidity[f'SPEED {i-1}'].min())\n        ).clip(lower=0)\n\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n        portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity[f'JOUR {i}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity[f'JOUR {i}']\n\n    return portfolio_liquidity\n\n\n\nProfil d’écoulement en conditions normales sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales sans déformation\")\n\n\n\n\n\n\n\n\n\nLes actions sont liquidées à la vitesse de l’actif le moins liquide du portefeuille. Par conséquent, contrairement à un écoulement avec déformation, la quantité liquidable progresse plus lentement dans le temps. Par exemple, on observe qu’en quatre jours seuls 59,01 % du fonds peuvent être liquidés, contre 73,54 % dans un scénario avec déformation.\nPar ailleurs, les poids des différents actifs restent globalement stables au fil des jours, à l’exception du dernier jour où l’intégralité du fonds est liquidée. Cette stabilité implique qu’il n’y a pas de concentration progressive du risque de liquidité sur les actifs les moins liquides, ce qui constitue une caractéristique importante du mécanisme d’écoulement sans déformation.\n\n\nProfil d’écoulement en conditions stressée sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions stressée sans déformation\")\n\n\n\n\n\n\n\n\n\nUn choc sur la profondeur de marché a pour effet un rallongement de la durée minimale de liquidation du fonds. Toutefois, les poids restent bien stables."
  },
  {
    "objectID": "risques/projets/demo.html#risque-de-crédit-et-risque-de-taux",
    "href": "risques/projets/demo.html#risque-de-crédit-et-risque-de-taux",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque de crédit et risque de taux",
    "text": "Risque de crédit et risque de taux\nLe risque de crédit est le risque de perte engendrée par la defaillance d’une partie prenante à remplir ses engagements contractuels préalablement établis. C’est principal risque observé sur périmètre du retail.\nOn définit le taux d’intérêt comme le loyer de l’argent (annualisé dans la pratique).\nCette définition est celle historique car très intuitive. Il peut cependant s’averer que les taux d’interêt soient négatifs. Là cette definition devient donc limitée.\nDepéndamment de ce qu’on fait de son argent, le thésauriser, le prêter à la banque , ou encore à l’etat, il existe toujours un risque de perte qui subsiste. Il peut donc advenir, que dans le souci de sécuriser son argent, dans un contexte particulier (notamment incertain), le posseusseur soit disposé à payer pour securiser son argent : on parle de taux d’intérêt négatif.\nCela illustre la métaphore du loyer du coffre fort.\n\nGénéralités sur le pricing d’obligations\nCONTEXTE\nEn raison des obligations réglementaires auxquelles les banques sont soumises, elles ne peuvent pas prêter à toutes les entreprises ayant besoin de financement. C’est dans ce contexte que la notion d’obligation prend son sens. La banque agit alors en tant qu’intermédiaire entre l’entreprise et le marché, et perçoit des frais de commission. le marché prête un montant M à l’entreprise et reçoit des annuités et le nominal à maturité.\nUne obligation est, économiquement, un prêt-emprunt.\nDe manière générale, la valorisation d’un actif est l’espérance des flux actualisés au taux sans risque sous la probabilité risque neutre :\n\\[\n\\begin{aligned}\nX_0 &= \\mathbb{E}[e^{-rT} X_T] \\\\\n    &= e^{-rT} \\, c \\times PS(T)\n\\end{aligned}\n\\]\nN.B : Le taux de recouvrement historique est de 40 %.\nLe recouvrement s’applique uniquement au nominal.\nLa probabilité de survie \\(PS(T)\\) est généralement déterminée à partir du modèle à intensité de Poisson via :\n\\[\nPS(T) = e^{-\\lambda T}\n\\]\nConsidérons une obligation d’échéances \\(T_i\\), \\(i = 1, \\dots, n\\), de coupon \\(c\\) et de nominal \\(N\\).\nLes coupons et le nominal sont payés en cas de survie, et le recouvrement en cas de défaut.\nLa valeur de cette obligation à la date \\(t\\) vaut :\n\\[\nC_t = \\sum_{i=1}^{n} c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}}\n\\]\nLa probabilité de survenue du défaut à une date \\(t\\) vaut :\n\\[\n\\begin{aligned}\nPD(t)\n    &= PS(t) - PS(t + dt) \\\\\n    &= -\\frac{PS(t+dt) - PS(t)}{dt} \\, dt \\\\\n    &= -\\frac{dPS(t)}{dt} \\, dt \\\\\n    &= \\lambda e^{-\\lambda t} \\, dt\n\\end{aligned}\n\\]\nLa valeur actualisée du recouvrement vaut :\n\\[\n\\mathcal{R}_0\n= \\int_0^T R \\lambda e^{-\\lambda t} e^{-rt} \\, dt\n= \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\]\nDe manière générale, pour une date \\(t\\) :\n\\[\n\\mathcal{R}_t\n= \\int_t^T R \\lambda e^{-\\lambda u} e^{-ru} \\, du\n= \\lambda R \\, e^{(r+\\lambda)t}\n  \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n  \\mathbf{1}_{\\{T \\ge t\\}}\n\\]\nLa valeur totale de l’obligation est alors :\n\\[\n\\begin{aligned}\nB_t\n&= \\sum_{i=1}^{n}\n    c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}} \\\\\n&\\quad +\n\\left(\ne^{-(r+\\lambda)(T_n - t)}\n+\n\\lambda R \\, e^{(r+\\lambda)t}\n\\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\right)\n\\mathbf{1}_{\\{T_n \\ge t\\}}\n\\end{aligned}\n\\]\n\n\nImpementation de la valorisation d’un bond\n\n\nCode\n# Impementation de la fonctiuon de valorisation d'un bond\nimport numpy as np\n\ndef Bond(t,c,T,r,lamda, R = 0.4):\n    B = 0\n    for T_i in range(1,T+1):\n        B += np.exp(-(lamda + r)*(T_i - t))*(T_i&gt;=t)\n    B *= c\n    B += (np.exp(-(r + lamda)*(T_i-t)) + lamda * R  * (1-(np.exp(-(r + lamda)*(T_i -t)))) / (r + lamda))*(T_i&gt;=t)\n    return B\n\n\n\nExemple: Obligation au pair\n\nlamda = 0\n\n\nr = 0.02\n\n\nc = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nBond(t,c,T,r,lamda)\n\n\nCode\n# Exemple1: Obligation au pair\nlamda = 0\nr = 0.02\nc = 0.02\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.9981933497987289)\n\n\nAvec les paramètres ci-dessus considérés, on remarque que le prix de l’obligation est proche du nominal. En effet, le taux coupon est égal au taux de marché, ce qui indique que l’obligation est remunérée au taux du marché. Il s’agit donc d’une obligation au pair.\nAu cas où on aurait proposé une remunération supérieure à celle du marché, elle serait beaucoup plus attractive et sa valeur se serait appréciée. l’illustration est donnée ci dessous pour c= 0.03\n\n\nBond(0,0.03,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.03,10,0.02,0)\nprice\n\n\nnp.float64(1.0879246481591023)\n\n\nAvec une remunération inférieure à ce qu’aurait proposé le marché, on obtient une valorisation de l’obligation inférieure au nomimal\n\n\nBond(0,0.015,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.015,10,0.02,0)\nprice\n\n\nnp.float64(0.9533277006185421)\n\n\nEtant donné une intensité de défaut supérieure à zéro, on peut également calculer le coupon pour lequel l’obligation est au pair. On obtient, après calculs prsentés ci-dessous, un taux coupon de 2.6%.\n\n\nCode\n# Recherche du coupon  pour émettre une obligation au pair.\ndef dichot(t,T,r,lamda, P_MKT):\n    c_inf = 1e-8\n    c_sup = 1\n    epsi = 1e-8\n    c_moy = (c_inf + c_sup)/2\n    error = c_sup - c_inf\n\n    while error&gt;epsi:\n        p_hw = Bond(t,c_moy,T,r,lamda)\n        if p_hw &gt; P_MKT:\n            c_sup = c_moy\n        elif p_hw &lt; P_MKT:\n            c_inf = c_moy\n        c_moy = (c_inf + c_sup)/2\n        error = np.abs(c_sup - c_inf)\n\n    return c_moy\n\n\n\n\n\nTaux coupon pour émettre une obligation au pair\n\nlamda = 0.01\n\n\nr = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nCode\n# Taux coupon pour émettre une obligation au pair\nlamda = 0.01\nr = 0.02\nt = 0\nT = 10\ndichot(t,T,r,lamda, P_MKT = 1)\n\n\n0.026393926193952293\n\n\nEn maintenant une remunération égale à celle du marché , avec une intensité de défaut très grande (de l’ordre de 1000%), l’obligation tombe presque instantannément en défaut. Dans cette situation, la valeur du coupon vaut alors 39.92% qui est sensiblement proche du taux de recouvrement (40%).\n\n\n\nValeur du bond poiur une intensité de défaut à 1000%\n\n\nCode\n# Valeur du bond poiur une intensité de défaut à 1000%\nlamda = 10\nr = 0.02\nc = 0.03\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.39920293189432754)\n\n\n\n\n\nÉvolution du prix de l’obligation en fonction du temps\n\n\nCode\nimport matplotlib.pyplot as plt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB = np.array([Bond(t,c,T,r,lamda) for t in times])\nplt.plot(times,B)\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.title(\"Evolution du prix plein coupon du Bond\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nPlus on se rapproche de la date de détachement du coupon plus l’obligation devient attractive, elle prend donc de la valeur.\nChaque saut correspond à un détachement de coupons. une fois le coupon détaché de l’obligation, les flux à venir diminuent et la valeur de l’obligation se deprecie de la valeur du coupon qui a été détaché.\nCes sauts ne reflètent donc pas une dépréciation des bonds par le marché. Ce sont des sauts techniques. Raison pour laquelle on dit que le prix plein coupon est pollué par le coupon (en anglais Dirty price).\nOn va donc s’interesser par la suite au clean price ou pied de coupon qui correspond à au prix du bond moins le coupon couru.\n\\[\n\\tilde{B}_t = B_t - cc\n\\]\nOù (cc) est le coupon couru :\n\\[\ncc = c \\times (t - T^*)\n\\]\nEn retirant cette valeur de coupon couru, on supprime cet effet de saut après les detachements de coupons\n\nImplémentation du clean price\n\n\nCode\n# Implémentation du clean price\n\ndef CleanPrice(t,c,T,r,lamda, R = 0.4):\n    cc = c*(t - np.floor(t))*(t&lt;=T)\n    return Bond(t,c,T,r,lamda, R)-cc\n\n\n\n\nPieds coupon\n\n\nCode\n# Pieds coupon\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for t in times])\nB_pieds_coupon = np.array([CleanPrice(t,c,T,r,lamda) for t in times])\nplt.plot(times,B_pieds_coupon, label =\"Pieds coupon\")\nplt.plot(times,B_plein_coupon, label =\"Plein coupon\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nEn retirant cette valeur de coupon couru, on supprime l’effet de saut après les detachements de coupons, et on observe une courbe sans discontinuités\n\n\n\nEvolution du prix plein coupon et du clean price en fonction du temps pour differents coupons\n\n\nCode\nlamda = 0.01\nr = 0.02\nc = 0.01\nT = 10\n\ntimes = np.linspace(0,10, num = 100)\nB_plein_coupon1 = np.array([Bond(t,0.01,T,r,lamda) for t in times])\nB_pieds_coupon1 = np.array([CleanPrice(t,0.01,T,r,lamda) for t in times])\n\nB_plein_coupon5 = np.array([Bond(t,0.05,T,r,lamda) for t in times])\nB_pieds_coupon5 = np.array([CleanPrice(t,0.05,T,r,lamda) for t in times])\n\nplt.plot(times,B_pieds_coupon1, label =\"Pieds coupon 1%\")\nplt.plot(times,B_plein_coupon1, label =\"Plein coupon 1%\")\n\nplt.plot(times,B_pieds_coupon5, label =\"Pieds coupon 5%\")\nplt.plot(times,B_plein_coupon5, label =\"Plein coupon 5%\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nLes constats que l’on fait : - Lorsque c = 1% : La rémunération de l’obligation est inférieure à celle offerte par le marché. Ainsi, lors de son émission, sa valeur sera nécessairement inférieure à celle du pair, ce qui explique une valeur initiale proche de 87 %. À maturité, on reçoit 100 % du nominal, plus 1 % de ce dernier, correspondant au coupon. Le résultat final est donc sensiblement égal à 101 %.\n\nLorsque c = 5% : L’obligation rémunère plus que ce que le marché offre, ce qui la rend particulièrement attractive dès son émission. Ainsi, sa valeur de départ sera d’environ 120 %. Au fur et à mesure que les obligations sont détachées, les flux futurs diminuent, ce qui entraîne une dépréciation de sa valeur. À maturité, on reçoit 100 % du nominal, plus 5 % du montant nominal, correspondant au coupon. Le résultat final est donc de 105 %.\n\n\n\nEvolution du prix de l’obligation en fonction du taux d’intérêt\nDe l’expression analytique du prix du bond, on observe que le prix est décroissant du taux d’intérêt. La figure ci-dessous en donne une illustration, toute chose égale par ailleurs.\nOn vérifie graphiquement que le prix du bond est de 100% lorsque le taux d’intérêt est égal au taux sans risque \\(r^* = c - \\lambda (1 - R)\\).\n\nEvolution du prix du bond en fonction du taux d’intérêt\n\n\nCode\n# Evolution du prix du bond en fonction du taux d'intérêt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR = 0.4\ninterest = np.linspace(0,0.10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest,B_plein_coupon, label =\"Plein coupon\")\nplt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\nplt.xlabel(\"Taux d'intérêt (%)\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_30568\\2938139823.py:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\n\n\nText(0.5, 1.0, \"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotion de sensibilité\nLa sensibilité mesure la variation du prix du bond face à une variation d’un facteur de risque. On distingue entre autres la sensibilité de taux et la sensibilité de crédit.\nDans cette sectioin nous nous intéressons particulièrement à la sensibilité de taux. Elle est définie par la formule:\n\\[\n    Sensibilité = - \\frac{dB_t}{dr} \\frac{1}{B_t}\n\\]\nInterprétation: Lorsque le taux d’intérêt bouge de 1%, alors le prix du bond bouge de -sensibilité %.\nCette sensibilité peut également être vue comme le barycentre des différentes échéances pondérées par les flux actualisés. C’est la duration.\n\\[\n    \\frac{\\sum T_i\\times F_i}{\\sum F_i}\n\\]\nExemple: En considérant une obligation de maturité 3 ans payant des coupons annuels, avec intensité de défaut nul. Alors son prix et la sensibilité de taux sont données par:\n\\[\n\\begin{aligned}\nP &= c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial P}{\\partial r}\n&= -1c \\, e^{-1r} - 2c \\, e^{-2r} - 3c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Sensibilité}\n&= \\frac{1c \\, e^{-1r} + 2c \\, e^{-2r} + 3c \\, e^{-3r}}\n        {c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}}\n\\end{aligned}\n\\]\nD’où l’expression barycentrique des échéances pondérées par les flux.\n\n\nCode\n# Sensibilité de taux du prix du bond\ndelta_r = 1e-4 #1bp\n\n\ndef Sensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    return -(Bond(t,c,T,r+delta_r,lamda, R)-B_t)/(delta_r*B_t)\n\n\nOn considère un bond dont les caractérisiques sont les suivqantes:\n\\[\n    \\begin{cases}\n        \\lambda = 0.01\\\\\n        r = 0.02\\\\\n        c = 0.03\\\\\n        T = 10\\\\\n        R = 0.4\\\\\n    \\end{cases}\n\\]\nLa sensiblité de ce bond est de 8.64. Ainsi, lorsque le taux d’intérêt augmente de 1 point de pourcentage, le prix du bond diminue de 8.64%.\n\n\nCode\n# Sensibilité de taux d'intérêt\nSensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4)\n\n\nnp.float64(8.643982489105056)\n\n\n\nEvolution de la sensibilité de taux fonction de la maturité\n\n\nCode\n# Evolution de la sensibilité au taux d'intérêt en fonction de la maturité\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,c,T,r,lamda) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité')\n\n\n\n\n\n\n\n\n\n\nOn constate que la sensibilité de taux croît avec la maturité et tend à être linéaire.\nPour des paramètres extrêmes \\(c = \\lambda = r = 0\\), la sensibilité (duration) est identique à la maturité comme illustré sur la figure ci-dessous. Cette remarque met en évidence la relation mlathématique entre la sensibilité et la maturité présentée plus haut.\nConnaissant la maturité, pour une variation du taux d’intérêt on peut donc donner une estimation “grossière” de la sensibilité (en approximant la duration par 0.8*maturité, par exemple).\n\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,1e-6,T,1e-6,1e-6) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité (cas extrême)\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité (cas extrême)')\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR d’une obligation.\nL’estimation de la VaR sur un bond peut se faire à partir de la senssibilité de taux ou par la méthode de repricing.\n\n\nEstimation de la VaR par la sensibilité du taux d’intérêt\nL’approche par la sensibilité se présente de la manière suivante:\nOn suppose que la dynamique du taux d’intérêt est donnée par \\[\\Delta r \\sim \\mathcal{N}(0, \\sigma \\sqrt{\\Delta t})\\].\nSachant que \\[\\frac{\\Delta P}{P} = - Duration \\times \\Delta r\\]\nil suit que \\[\\frac{\\Delta P}{P} \\sim \\mathcal{N}(0, Duration \\times \\sigma \\sqrt{\\Delta t})\\]\nUne approche par la sensibilité de la VaR à 99% donne \\[VaR = Duration \\times \\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\\] Où \\(z_{99\\%}\\) est le quantile d’ordre 99% de la loi normale standard.\nPour les besoins de notre exercice, on pose \\(\\sigma = 1\\%\\).\n\n\nCode\nfrom scipy.stats import norm\n\n# Calcul de la VaR par la sensibilité du taux d'intérêt\ndef Sensi_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    duration = Sensi(t,c,T,r,lamda, R)\n    var = duration*sigma*np.sqrt(H)*norm.ppf(alpha)\n    return var\n\n\n\n\nCode\nsigma= 0.01\nH = 1/12\nt = 0\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR =0.40\n\nSensi_VaR(sigma , H, t,c,T,r,lamda)\n\n\nnp.float64(0.05804942383590022)\n\n\nLa VaR à 99% d’horizon 1 an sur le bond est de 5.8%. - On note que la VaR est une fonction linéaire croissante de la volatilité. C’est une conséquence directe de l’hypothèse de normalité des variations du taux d’intérêt. Ainsi, une augmentation de la volatilité du taux d’intérêt s’accompagne d’une augmentation de la volatilité du prix du bond. - La VaR est une fonction décroissante du taux d’intérêt. En effet, une augmentation du taux d’intérêt entraîne une diminution de la valeur des bonds, et par conséquent des valeurs extrêmes atteintes par celles-ci; d’où la VaR décroît. Toutefois l’évolution de la VaR en fonction du taux d’intérêt n’est pas linéaire.\n\n\nEvolution de la Z-VaR en fonction du taux d’intérêt\n\n\nCode\n# Evolution de la VaR en fonction de la volatilité\nvol = np.linspace(0,1e-2, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for sigma in vol])\n\nplt.plot(vol, var, label =\"VaR_99%\")\nplt.xlabel(\"Volatilité\")\nplt.ylabel(\"VaR \")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de la volatilité\")\n\n\nText(0.5, 1.0, 'VaR à 99% à horizon 1 an en fonction de la volatilité')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Evolution de la VaR en fonction du taux d'intérêt\ninterest = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nEvolution de la Z-VaR en fonction de l’intensité de défaut\nPour des intensités de défaut croissantes, la VaR du bond décroît de façon exponentielle, ceteris paribus. Tout comme avec le taux d’intérêt, une augmentation de l’intensité de défaut s’accompagne d’une imminence du défaut et par conséquent de la diminution de la valeur du bond. D’où une diminution de la VaR.\n\n\nCode\n# Evolution de la VaR en fonction de l'intensité de défaut\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de défaut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR par repricing de l’obligation\nLa méthodologie consiste à revaloriser le bond pour une variation du taux d’intérêt, puis d’en déduire la variation du prix du bond qui en résulte.\n\\[\n    \\Delta r^* =\\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\n\\]\n\\[\n    VaR_{99\\%} = \\frac{B(r+ \\Delta r^*) - B(r)}{B(r)}\n\\]\n\n\nApproche repricing\n\n\nCode\n# Approche repricing\n\ndef repricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    delta_r = sigma*np.sqrt(H)*norm.ppf(alpha)\n    B_rep = Bond(t,c,T,r+delta_r,lamda, R)\n    B_r = Bond(t,c,T,r,lamda, R)\n    \n    return  (B_r - B_rep)/B_r\n\n\n\n\nCode\nrepricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99)\n\n\nnp.float64(0.05627224378244837)\n\n\n\nLa VaR obtenue par l’approche repricing, sous les mêmes conditions que précédemment, est de 5.6%. Cette VaR est inférieure à celle obtenue sous l’hypothèse de normalité des variations du taux d’intérêt.\nCeci s’explique par le fait que la duration est une fonction convexe décroissante du taux d’intérêt et représente une approximation affine de la valeur du bond en le taux d’intérêt. De manière générale, l’approche par les sensibilités surestime la VaR.\nL’évolution de la VaR, obtenue par reprincing, en fonction du taux d’intérêt ou de l’intensité de défaut (voir les graphiques ci-dessus) à la même allure que la z-VaR.\nEn règle générale la VaR doit être inférieure au seuil de 20% de par la limite règlementaire. L’utilisation de la z-VaR peut donc constituer un manque à gagner pour les institutions financieres. En effet, elles peuvent être amenées à dérisquer leur portefeuille en limitant leur investissements pour des raisons purement techniques.\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\ninterest = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda)for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de saut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")"
  },
  {
    "objectID": "risques/projets/demo.html#risque-de-contrepartie",
    "href": "risques/projets/demo.html#risque-de-contrepartie",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque de contrepartie",
    "text": "Risque de contrepartie\nDans le cadre d’un engagement contratuel incluant deux parties prenantes, le risque de contrepartie fait reférence au rique de perte suite au défaut d’une des parties à remplir les engagements contractuels pré-établis: c’est un risque bilatéral.\nAfin de se couvrir du defaut de l’émetteur d’un bond, l’acheteur du bond peut entrer dans un credit default swap (CDS). Dans un tel contrat, lorsque la contrepartie est correlée à l’émetteur de l’obligation on parle de wrong way risk ce qui a pour conséquence d’exposer davantage le souscripteur. La contrepartie doit donc être décorrelée de l’émetteur.\nLe risque de contrepartie est en général mitigé par les appels de marge (chambre de compensation). - Quand la qualité de l’émetteur se dégrade, son spread de crédit spread croît et le CDS s’apprécie. - Si la contrepartie fait défaut, le détenteur perd \\(CDS = 1-R\\). - Un mécanisme d’appel de marge permet de mitiger le risque.\n\nLien entre CDS et obligation: la formule du triangle de crédit\nUn estimation du taux sans risque d’une obligation: \\[\n\\begin{aligned}\nP &= \\sum_{i=1}^{n} c \\, e^{-(r+\\lambda)T_i}\n    + e^{-(r+\\lambda)T_n}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\int_0^T e^{-(r+\\lambda)t} \\, dt\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\nL’obligation est au pair si et seulement si P = 1 Ainsi, \\[\n    (c + \\lambda R) \\frac{ 1- e^{-(r + \\lambda )T}}{r + \\lambda} + e^{-(r + \\lambda)T} = 1\n\\]\nQuand T tend vers l’infini le taux sans risque vaut alors, \\[\n    (c + \\lambda R)  = r + \\lambda\n\\]\n\\[\n    (c - r )  = \\lambda (1 - R)\n\\] - \\(c-r\\): spread ou prime de risque - \\(\\lambda\\): intensité de défaut - \\(1-R\\): LGD \\[\n    Spread = PD \\times LGD\n\\]\n\n\nCDS: principe\n\nPremium Leg: est payé tant que l’émetteur est “en vie”. \\[ PL = \\int_0^T s \\times e^{-rt} \\times e^{-\\lambda t}dt = s \\frac{1-e^{-(r+\\lambda)T}}{r + \\lambda}\\]\nDefault Leg: compensation à la date t si le défaut y survient. \\[DL = \\int_0^T(1-R) e^{-rt}\\times e^{-\\lambda t} dt = \\lambda (1-R) \\frac{1-e^{-(r+\\lambda)T}}{r+\\lambda}\\].\n\nLa valeur initiale d’un spread étant nulle, les deux jambes sont égales et il on obtient:\n\\[\n    s = \\lambda (1-R)\n\\]\nUn estimation de ce spread peut donc être déduite de la formule du triangle de crédit.\n\n\nSensibilité de crédit\nLa sensibilité de crédit d’une obligation est la variation du prix de l’obligation lorsque varie le spread de crédit de la contrepartie. De la formule du triangle, la sensibilité de crédit peut être déduite de la sensibilité par rapport à l’intensité de défaut \\(\\lambda\\).\n\\[\n    \\frac{\\partial P}{\\partial s} = \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda}\n\\]\n\\[\n    \\frac{\\partial P}{\\partial s} \\times \\frac{1}{P}= \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda} \\times  \\frac{1}{P}\n\\]\nOn peut calculer une VaR taux et une VaR crédit. Toutefois, la VaR qui sera regardée de près est celle issue de la variation conjointe des taux et des spreads de crédit.\n\nSensibilité de crédit du bond\n\n\nCode\n# Sensibilité de crédit du bond\ndelta_lambda = 1e-4 #1bp\n\ndef Sensi_credit(t,c,T,r,lamda, R = 0.4, delta_lambda = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    variation = -(Bond(t,c,T,r,lamda+delta_lambda, R)-B_t)/(delta_r*B_t)\n    return variation/(1-R)\n\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda)\n\n\nnp.float64(8.82119086802735)\n\n\n\nUne variation d’un point de pourcentage du spread de crédit entraîne une variation de 8.82% de la valeur du bond. Cette sensibilité de crédit est assez proche de la sensibilité taux. En effet, on observe de l’écriture mathématique de la valeur du bond que le taux d’intérêt et l’intensité apparaissent conjointement de façon additive (sauf pour le terme de recouvrement).\nCes deux sensibilitées sont identiques pour un taux de recouvrement nul.\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda, R = 0.0, delta_lambda = 1e-4)\n\n\nnp.float64(8.77911216566412)\n\n\n\n\n\nEstimation de la VaR par variation conjointe du taux d’intérêt et du spread de crédit\nOn suppose que la dynamique du taux d’intérêt est donnée par l’EDS\n\\[\n    dr_t = \\sigma dW_t\n\\]\nLe spread ne prenant pas de valeur négative, on supposera qu’il est log-normal. Sa dynamique est donnée par l’EDS\n\\[\n    \\frac{ds_t}{s_t} = \\alpha dZ_t\n\\]\nEn supposant \\(dW_t dZ_t = \\rho\\) et \\(Z_t = \\rho W_t + \\sqrt{1-\\rho^2}V_t\\) tel que \\(dV_t dW_t = 0\\),\nLa dynamique du spread s’écrit alors \\[\n\\frac{ds_t}{s_t} = \\alpha (\\rho W_t + \\sqrt{1-\\rho^2}V_t)\n\\]\nEn appliquant un schéma de discrétisation d’Euler, on peut alors simuler les taux \\(r_t\\) et les spreads \\(s_t\\) par:\n\\[\n    r_H = r + \\sigma \\sqrt{H} W_H\n\\]\n\\[\n    s_H = s(1 + \\alpha \\rho \\sqrt{H} W_H + \\alpha \\sqrt{1-\\rho^2} \\sqrt{H} V_H)\n\\]\nOù H est le pas de discrétisation.\nLa méthodologie de détermination de la sensibilité du prix d’une obligation conjointement au taux d’intérêt et au spread de crédit est la suivante: - Calculer la valeur du bond pour les paramètres initiaux - Simuler les taux \\(r_H\\) et les spreads \\(s_H\\) - Pour chaque simulation, calculer la valeur du bond (cetertis paribus) - pour déterminer la variation des nouveaux prix par rapport au prix initial - calculer le quantile d’ordre 99% des variations obtenues\n\n\nCode\n# Sensibilité conjointe de taux et de crédit\n\ndef SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12, M=10_000):\n    \n    u1 = np.random.uniform(0,1, size = M)\n    u2 = np.random.uniform(0,1, size = M)\n    w = norm.ppf(u1)\n    v = norm.ppf(u2)\n    s = lamda*(1-R)\n    \n    r_H = r + sigma*np.sqrt(H)*w\n    s_H = s*(1 + alpha*rho*np.sqrt(H)*w + alpha*np.sqrt(1-rho**2)*np.sqrt(H)*v)\n    \n    P_0 = Bond(0,c,T,r,lamda, R)\n    P_1 = Bond(H,c,T,r_H,s_H/(1-R), R)\n    mu = (P_1 - P_0)/P_0\n    return  - np.quantile(mu,0.01)\n\n\n\n\nCode\nsigma = 0.01 # Volatilité du taux d'intérêt\nalpha =0.4   # Volatilité du spread\nrho = 0.4    # Corrélation entre le spread et le taux d'intérêt\n\nnp.random.seed(90) # Pour la reproductibilité\nSimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12)\n\n\nnp.float64(0.0602691861203251)\n\n\n\n\nCode\nnp.random.seed(90)\n\nrhos = np.linspace(0,1, num= 1000)\nvar = np.array([SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12) for rho in rhos])\n\nplt.plot(rhos, var)\n\n\n\n\n\n\n\n\n\nLa corrélation \\(\\rho\\) doit être positive, car une augmentation des taux d’intérêt entraîne une meilleure rémunération du marché par rapport aux obligations. Pour rester attractifs, les coupons doivent alors augmenter. Cette hausse des coupons accroît le coût de financement de l’émetteur, ce qui conduit à une augmentation du spread de crédit. Il en résulte une corrélation positive entre les taux d’intérêt et les spreads de crédit.\nUne augmentation conjointe des taux et des spreads accroît le risque de défaut, en raison de la corrélation positive entre ces deux facteurs de risque. Ainsi, une corrélation élevée reflète un manque de diversification, ce qui expose davantage le portefeuille au risque systémique"
  },
  {
    "objectID": "risques/projets/demo.html#risque-modèle",
    "href": "risques/projets/demo.html#risque-modèle",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque modèle",
    "text": "Risque modèle\nLe risque modèle désigne le risque associé à l’utilisation d’un modèle mathématique ou statistique qui ne représente pas correctement la réalité ou qui mène à des décisions erronées en finance et en gestion des risques.\nEléments du suivi du risque modèle: - Sanity check: Ecart de performance - Backtesting: Application aux données antérieures - Comparaison avec desmodèles plus riches - Provisionner au titre du risque de modèle"
  },
  {
    "objectID": "risques/projets/demo.html#risque-climatique",
    "href": "risques/projets/demo.html#risque-climatique",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque climatique",
    "text": "Risque climatique\nLe risque climatique désigne les risques financiers liés au changement climatique et aux politiques de transition vers une économie bas carbone. Sa modélisation reste complexe, notamment en raison du manque de données historiques et de l’absence d’un cadre réglementaire clairement défini.\nOn distingue deux principaux types de risques climatiques :\n\nRisque physique: l correspond aux pertes financières causées par des événements climatiques extrêmes. Il est défini par deux éléments clés : la fréquence et la sévérité des événements.\n\nRisque aigu : Événements rares mais très intenses ( ouragans, inondations).\nRisque chronique : Changements progressifs et durables (élévation du niveau de la mer).\n\nRisque de transition: il découle de l’adaptation des entreprises et des acteurs économiques aux exigences de transition écologique.\n\nRisque politique et réglementaire : Durcissement progressif des régulations environnementales (taxe carbone).\nOpportunités technologiques : Innovations favorisant la transition énergétique et redéfinissant les modèles économiques.\n\n\nLa resilience des institutions financieres notamment aux changement climatique est évaluée via des stress tests climatiques dont les scénarios sont issus des données du NGFS (Network for Greening financial System)."
  },
  {
    "objectID": "risques/projets/demo.html#introduction",
    "href": "risques/projets/demo.html#introduction",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "",
    "text": "Ce document présente une introduction à la gestion des risques en Asset Management.\nL’objectif principal est de comprendre et d’appliquer les méthodologies de base utilisées pour analyser, mesurer et suivre les risques financiers au sein d’un portefeuille.\nL’étude couvre differentes notions de risk management notamment:\n- les méthodes de mesure du risque de marché (sensibilité, volatilité, Value-at-Risk, Tracking Error, stress tests et profil de liquidité),\n- les composantes clés du risque de crédit (intensité de défaut, sensibilité des obligations aux variations de spreads, triangle du crédit),\n- le risque de contrepartie et ses mécanismes de mitigation via les appels de marge,\n- ainsi que les risques émergents tels que le risque climatique et le risque modèle.\n\n\nCode\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import norm\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize"
  },
  {
    "objectID": "risques/projets/demo.html#i.-risques-de-marché",
    "href": "risques/projets/demo.html#i.-risques-de-marché",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "I. Risques de marché",
    "text": "I. Risques de marché\n\nI.1. Construction d’un portefeuille diversifié\nOn construit un portefeuille constitué de 10 entreprises du CAC40. Nous avons choisi des entreprises de secteurs variés afin de s’assurer que le portefeuille soit diversifié.\nAinsi, notre portefeuille est constitué des entreprises des secteurs suivants :\n\nBanque et assurance : BNP Paribas (BN.PA), Crédit Agricole (CA.PA)\n\nAéronautique et défense : Airbus (AIR.PA)\n\nTechnologie: Capgemini (CAP.PA)\nConcession et Construction: Vinci (DG.PA)\nTélécommunications : Orange (ORA.PA)\n\nÉnergie : Engie (ENGI.PA)\n\nLuxe & Consommation : LVMH (MC.PA)\n\nMédias & Divertissement : Vivendi (VIV.PA)\n\nIndustrie Chimique : Air Liquide (AI.PA)\n\nNous allons procéder à une optimisation de Markowitz visant à construire un portefeuille qui minimise la volatilité (risque) sous la contrainte d’un niveau de rendement modulable en fonction du profil de risque de l’investisseur.\n\n\nCode\n# On recupère les données sur un historique de deux ans\n\nend_date = '2025-03-16'  \nstart_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=2*365)).strftime('%Y-%m-%d')\ntickers = ['AI.PA', 'AIR.PA', 'BN.PA', 'CA.PA', 'CAP.PA', 'DG.PA', 'ENGI.PA', 'ORA.PA', 'SAN.PA', 'VIV.PA']\n\ndf = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\ncac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\388541553.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\n[                       0%                       ][                       0%                       ][**************        30%                       ]  3 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************80%*************          ]  8 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\388541553.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\ndf_close = df['Close']\ndf_volume = df['Volume']\n# Calcul des  rendements journaliers\nreturns = df_close.pct_change()\nreturns.dropna(inplace=True)\n\n\n\nAperçu des données\n\n\nCode\n#Aperçu des données\ndf.head(3)\n\n\n\n\n\n\n\n\nPrice\nClose\nVolume\n\n\nTicker\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-17\n126.875404\n110.644333\n49.379742\n14.572609\n157.424210\n91.923409\n10.256954\n8.935349\n79.612030\n8.390329\n1971502\n2171272\n3041249\n4474936\n719990\n3575676\n12989586\n20039590\n5122220\n7405822\n\n\n2023-03-20\n129.684891\n112.407204\n49.742428\n14.783135\n157.659653\n93.078445\n10.359279\n9.040172\n79.310608\n8.565088\n1139355\n1290939\n1697298\n2379408\n418826\n1207887\n7087933\n9795603\n2119074\n2624806\n\n\n2023-03-21\n129.288467\n115.250557\n50.186718\n14.955767\n160.579285\n95.104210\n10.579202\n9.144996\n79.948921\n8.741748\n745105\n1002062\n2018104\n2130100\n293112\n934999\n7313789\n9077226\n1449009\n2962587\n\n\n\n\n\n\n\nComme indicateur quantifiable de cette diversification nous presentons ci après la matrice de correlation entre les rendements journaliers des actifs du portefeuille pour notre période d’étude.\n\n\nCode\n# Calcul de la matrice de corrélation\ncorr_actifs = returns.corr().to_numpy()\n\n# Affichage du corrélogramme\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_actifs, annot=True, fmt='.2f', cmap='coolwarm', xticklabels=returns.columns, yticklabels=returns.columns)\nplt.title(\"Corrélogramme des actifs\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nI.1.1 Présentation du principe d’optimisation de Markovitz\nL’optimisation de Markowitz, également connue sous le nom de théorie moderne du portefeuille (MPT), permet de construire un portefeuille optimal en combinant des actifs de manière à minimiser la volatilité (ou le risque) tout en respectant une contrainte sur un niveau de rendement fixé. L’un des éléments clés de cette théorie est la détermination des poids de chaque actif dans le portefeuille, afin d’atteindre un compromis optimal entre le risque et le rendement.\nHypothèses principales :\n\nLes décisions sont prises en fonction du couple rendement-risque.\n\nLes investisseurs cherchent à diversifier leurs actifs pour réduire le risque.\n\nL’accès au taux sans risque est illimité.\n\nAucun coût de transaction n’est pris en compte.\n\nProgramme d’optimisation : L’objectif est de minimiser la volatilité sous la contrainte d’un rendement cible $ _T $ :\n\\[\n\\text{Minimiser } \\omega^t\\Sigma \\omega\n\\] Sous les contraintes :\n$$\n\\[\\begin{cases}\n    \\omega^t \\mu = \\mu_T \\\\\n    \\sum_{i=1}^{n} \\omega_i = 1\n\n\\end{cases}\\]\n$$\n\n\nI.1.2 Calcul des rendements annualisées et des volatilités annualisés des log_rendements pour chaque actif\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nI.1.3. Optimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nI.1.3. Optimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\n \n# Nombre d'actifs\nn_assets = len(yearly_return)\nmu = yearly_return\n\ndef portfolio_variance(weights):\n    return weights.T @ sigma @ weights\n\ndef weight_sum_constraint(weights):\n    return np.sum(weights) - 1\n\ndef target_return_constraint(weights, mu_target):\n    return weights.T @ mu - mu_target\n\ndef efficient_portfolio(mu_target, range_=(-0.1, None), positive=True):\n    # On initialise les poids de maniere équitable\n    init_weights = np.ones(n_assets) / n_assets\n\n    # Contraintes\n    constraints = [\n        {'type': 'eq', 'fun': weight_sum_constraint},\n        {'type': 'eq', 'fun': lambda w: target_return_constraint(w, mu_target)}\n    ]\n\n    # On verifie si on a définit une contrainte de positivité\n    if positive:\n        bounds = [(0, None) for _ in range(n_assets)]  # Chaque poids doit être &gt;= 0\n    else:\n        bounds = [range_ for _ in range(n_assets)] \n\n    # Optimisation\n    result = minimize(portfolio_variance, init_weights, bounds=bounds, method='SLSQP', constraints=constraints)\n\n    if result.success:\n        optimal_weights = result.x\n        portfolio_volatility = np.sqrt(portfolio_variance(optimal_weights))\n        return portfolio_volatility, optimal_weights\n    else:\n        return None, None\n\n\n\n\nI.1.4 Illustration pour un rendement cible de 10%\nPour un rendement annuel cible de 10%, correspondant à l’ordre de grandeur des rendements du CAC40 qui représente le benchmark du fond, les poids optimaux donnés par le l’optimisation de Markovitz et qui ont permis de minimiser le risque auquel est exposé le porte feuille ( 9.67% de volatilité) sont donnés ci après:\n\n\nCode\n# Rendement cible\nmu_target = 0.1 \n\n# les poids optimaux et la volatilité\nvol, weights = efficient_portfolio(mu_target)\nprint(\"Le portefeuille ainsi construit a un rendement de\",round(weights.T @ mu,4), \"et est soumis à une volatilité de l'ordre de :\", round(vol,4), \"\\n\")\nprint(\"---\"*50)\nprint(\"Les poids associés aux actifs du porte feuille sont listés ci après:\\n\")\ni = 0\nfor column in df_close.columns.tolist():\n    print(f\"{column}, {weights[i]:.4f}\")\n    i += 1\n\n\nLe portefeuille ainsi construit a un rendement de 0.1 et est soumis à une volatilité de l'ordre de : 0.0989 \n\n------------------------------------------------------------------------------------------------------------------------------------------------------\nLes poids associés aux actifs du porte feuille sont listés ci après:\n\nAI.PA, 0.0243\nAIR.PA, 0.0382\nBN.PA, 0.2031\nCA.PA, 0.1929\nCAP.PA, 0.1255\nDG.PA, 0.0000\nENGI.PA, 0.0732\nORA.PA, 0.2652\nSAN.PA, 0.0542\nVIV.PA, 0.0234\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), weights*100, color='skyblue')\n\nplt.title(f\"Poids optimaux des actifs dans le portefeuille (Volatilité: {round(vol, 4)})\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Poids (en %)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, weights):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nOn constate que le portefeuille optimal est assez diversifié.Les trois principales entreipes representées sont :\n\nTélécommunications : Orange (26.53%)\nBanque et assurance: BNP Paribas (20.24%), Credit Agricole (19.27%)\nTechnologie : Capgemini (12.52%)\nÉnergie : Engie (7.30%)\n\n\n\n\nI.2. Suivi du porte feuille\nDans cette section on va determiner l’AUM du portefeuille, suivre ses performances sa volatilité et ses performances relatives par rapport au CAC40.\nNous effectuerons également un stress test sur le portefeuille pour ebaluer sa sensibilité relative à la période COVID.\n\n\nCode\n# Poids des actions dans le portefeuille\nweights = pd.DataFrame({'Ticker': tickers ,\n     'Weight': weights})\nweights = weights.set_index('Ticker')['Weight']\n\n#print(weights)\n\n\n\nI.2.1. AUM (Asset under management)\nElle représente la valeur du porte feuille et est déterminée par la formule suivante\nAUM = (NAV)(Nombre_de_ parts)\nAvec\nLa Net Value Asset (NAV) représente la valeur nette d’une part du fonds (portefeuille) à un moment donné.\n\nDétermination de l’AUM\n\n\nCode\n# Détermination de l'AUM\naum = df_close@weights\nportfolio = pd.DataFrame({'AUM': aum})\nportfolio.head()\n\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-03-17\n47.527177\n\n\n2023-03-20\n47.829467\n\n\n2023-03-21\n48.500885\n\n\n2023-03-22\n48.360237\n\n\n2023-03-23\n48.621806\n\n\n\n\n\n\n\n\n\nCode\nportfolio.describe()\n\n\n\n\n\n\n\n\n\nAUM\n\n\n\n\ncount\n509.000000\n\n\nmean\n53.563795\n\n\nstd\n3.084850\n\n\nmin\n47.527177\n\n\n25%\n50.547954\n\n\n50%\n53.835647\n\n\n75%\n56.161594\n\n\nmax\n59.243527\n\n\n\n\n\n\n\nSur notre période d’etude, l’AUM en moyenne est de 59.10, et oscille entre 50.68 et 66.84.\n\n\n\nI.2.2 Performance du portefeuille\nDans le cadre du suivi de notre portfeuille, il est important d’identifier et de mesurer les risques auxquels il est exposé afin de mieux les encadrer. Dans cette section nous intéressons tout particulièrement au risque de marché. Ce risque peut être évalué à l’aide de:\n\nLa tracking error\nLa volatilité\nLa VaR\n\nqui sont des mesures ex-ante.\n\nOn s’intéresse aux performances du portefeuille construit ainsi que celles du benchmark\n\n\nCode\n# Performance du portefeuille\nportfolio['perf'] = portfolio['AUM'].pct_change()\nportfolio.dropna(inplace=True)\n\n# Performance du benchmark\ncac40_df = cac40['Close']\ncac40_df ['perf'] = cac40_df.pct_change()\ncac40_df.dropna(inplace=True)\n\nprint(\"Rendement annualisé du fond: \\t\", (1+portfolio.perf.mean())**252 - 1)\nprint(\"Rendement annualisé du CAC40: \\t\", (1+cac40_df.perf.mean())**252 - 1)\nprint(\"Performance relative du fond: \\t\", (1+portfolio.perf.mean())**252 - (1+cac40_df.perf.mean())**252 )\n\n\nRendement annualisé du fond:     0.08254785047977653\nRendement annualisé du CAC40:    0.08502178188176579\nPerformance relative du fond:    -0.00247393140198926\n\n\nLa performance absolue du fonds est de 8.24%, tandis que celle du CAC40 est de 8.50%. Pour évaluer la performance relative du fonds par rapport à son indice de référence, on calcule l’écart de performance : -0.256%. Cela signifie que le portefeuille sous-performe par rapport à son benchmark.\n\n\n\nI.2.2.1 Tracking error\nLa Tracking Error (TE) mesure l’écart de performance entre un fonds et son indice de référence (benchmark). Elle est définie comme l’écart-type des différences de rendement entre le fonds et le benchmark sur une période donnée. Plus la Tracking Error est faible, plus le fonds suit fidèlement son indice de référence.\n\\[\nTE = \\sqrt{\\frac{\\sum_1^n (r_{fond} - r_{benchmark})^2}{n-1}}\n\\]\n\n\nCode\n# Tracking error\nportfolio['benchmark'] = cac40_df['perf']\nportfolio['tracking_error'] = portfolio['perf'] - portfolio['benchmark']\nTError = portfolio['tracking_error'].std()*np.sqrt(252) # Annualisation\n\nTError\n\n\nnp.float64(0.10911787788473695)\n\n\nIl ressort que la dispersion des rendements du fond par rapport ceux de son indice de reférence est de 10.8% en moyenne.\n\n\nI.2.2.2 Volatilité\nAfin d’evaluer le risque de marché du fond, la volatilité complète l’information apportée par la tracking error et permet de se faire une idée sur la regularité et la dispersion des rendements du fond sur la periode d’étude. Il s’agit de l’écart-type des performances absolues du fond.\n\nVolatilité annualisé du portefeuille\n\n\nCode\n# Volatilité annualisé du portefeuille\nstd_dev = portfolio.std()*np.sqrt(252)\nstd_dev\n\n\nAUM               48.833280\nperf               0.132069\nbenchmark          0.128884\ntracking_error     0.109118\ndtype: float64\n\n\n\n\nVolatilité du benchmark\n\n\nCode\n# Volatilité du benchmark\ncac40_df.std()*np.sqrt(252)\n\n\nTicker\n^FCHI    5304.018890\nperf        0.128884\ndtype: float64\n\n\n\n\n\nI.2.2.3 Value at risk\nLa value-at-risk (VaR) est une mesure très répendue en gestion de risques. Elle permet d’evaluer les pertes extrêmes encourrues sur un portefeuille sur un horizon donné sous un niveau de confiance. Elle est généralement calculée sur un horizon 1 jour. Pour obtenir une VaR sur un horizon (H) plus grand, on utilise la méthode de scaling qui consiste à multpiplier la VaR 1 jour par la racine carrée de H.\nElle est donnée par \\(\\mathbf{P}(P \\& L &lt; VaR_H) = \\alpha\\)\nLa VaR est déterminée à l’aide diverses approches: - Approche historique: Elle consiste à calculer le quantile empirique d’ordre \\(\\alpha\\) des pertes historiques. C’est l’approche la plus utilisée en pratique. Cette méthode présente toutefois des limites en cas de changement soudain et inhabituel de conjoncture (crise etc.)\n\nApproche paramétrique: Ici on fait l’hypothèse que les P&L sont distribués suivant un loi dont les paramètres sont calibrés sur les données historiques. Comme toute méthode paramétrique, elle peut conduire à des estimations très biaisées si la loi supposée n’est pas adaptée aux données\n\n\nVisualisation de la distribution des log-rendements journaliers\n\n\nCode\nimport seaborn as sns\n\nsns.histplot(portfolio.perf)\nsns.kdeplot(portfolio.perf, color='red')\n\n\n\n\n\n\n\n\n\n\n\n\nOn s’interesse aux VaR sur un horizon de 20 jours\n\n\n(a) VaR historique\n\n\nCode\nVaR = portfolio.quantile(1-0.99)*np.sqrt(20) # 1-day VaR scaled to 20-day VaR\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.09788047199168412\nla VaR  Relative à 99% d'horizon 20 jours  est : -0.08993815186398733\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\764873701.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\764873701.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 9.66%. Le porte feuille a 99% de chance de sous performer son indice de reference de 8.96% sur un horizon de 20 jours.\n\n\n(b) VaR paramétrique\n\n\nCode\nVaR = (portfolio.mean() + norm.ppf(1-0.99)*portfolio.std())*np.sqrt(20)\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.08514683127950859\nla VaR  Relative  à 99% d'horizon 20 jours  est : -0.07155357198997868\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\937860044.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\937860044.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 8.47%. Le porte feuille a 99% de chance de sous performer son indice de reference de 7.10% sur un horizon de 20 jours.\nOn observe que les VaR paramétriques sont strictement inférieures aux VaR historiques. Ceci pourrait s’expliquer par le fait que les données ont des queues plus lourdes que la loi normale comme l’illustre le graphique présenté plus haut. De plus l’excess de kurtosis confirme cette observation. Par conséquent, la VaR obtenue sous une paramétrisation gaussienne sous-estime la perte maximale encourrue sur le portefeuille."
  },
  {
    "objectID": "risques/projets/demo.html#ii.-risque-de-crédit-et-risque-de-taux",
    "href": "risques/projets/demo.html#ii.-risque-de-crédit-et-risque-de-taux",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "II. Risque de crédit et risque de taux",
    "text": "II. Risque de crédit et risque de taux\nLe risque de crédit est le risque de perte engendrée par la defaillance d’une partie prenante à remplir ses engagements contractuels préalablement établis. C’est principal risque observé sur périmètre du retail.\nOn définit le taux d’intérêt comme le loyer de l’argent (annualisé dans la pratique).\nCette définition est celle historique car très intuitive. Il peut cependant s’averer que les taux d’interêt soient négatifs. Là cette definition devient donc limitée.\nDepéndamment de ce qu’on fait de son argent, le thésauriser, le prêter à la banque , ou encore à l’etat, il existe toujours un risque de perte qui subsiste. Il peut donc advenir, que dans le souci de sécuriser son argent, dans un contexte particulier (notamment incertain), le posseusseur soit disposé à payer pour securiser son argent : on parle de taux d’intérêt négatif.\nCela illustre la métaphore du loyer du coffre fort.\n\nGénéralités sur le pricing d’obligations\nCONTEXTE\nEn raison des obligations réglementaires auxquelles les banques sont soumises, elles ne peuvent pas prêter à toutes les entreprises ayant besoin de financement. C’est dans ce contexte que la notion d’obligation prend son sens. La banque agit alors en tant qu’intermédiaire entre l’entreprise et le marché, et perçoit des frais de commission. le marché prête un montant M à l’entreprise et reçoit des annuités et le nominal à maturité.\nUne obligation est, économiquement, un prêt-emprunt.\nDe manière générale, la valorisation d’un actif est l’espérance des flux actualisés au taux sans risque sous la probabilité risque neutre :\n\\[\n\\begin{aligned}\nX_0 &= \\mathbb{E}[e^{-rT} X_T] \\\\\n    &= e^{-rT} \\, c \\times PS(T)\n\\end{aligned}\n\\]\nN.B : Le taux de recouvrement historique est de 40 %.\nLe recouvrement s’applique uniquement au nominal.\nLa probabilité de survie \\(PS(T)\\) est généralement déterminée à partir du modèle à intensité de Poisson via :\n\\[\nPS(T) = e^{-\\lambda T}\n\\]\nConsidérons une obligation d’échéances \\(T_i\\), \\(i = 1, \\dots, n\\), de coupon \\(c\\) et de nominal \\(N\\).\nLes coupons et le nominal sont payés en cas de survie, et le recouvrement en cas de défaut.\nLa valeur de cette obligation à la date \\(t\\) vaut :\n\\[\nC_t = \\sum_{i=1}^{n} c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}}\n\\]\nLa probabilité de survenue du défaut à une date \\(t\\) vaut :\n\\[\n\\begin{aligned}\nPD(t)\n    &= PS(t) - PS(t + dt) \\\\\n    &= -\\frac{PS(t+dt) - PS(t)}{dt} \\, dt \\\\\n    &= -\\frac{dPS(t)}{dt} \\, dt \\\\\n    &= \\lambda e^{-\\lambda t} \\, dt\n\\end{aligned}\n\\]\nLa valeur actualisée du recouvrement vaut :\n\\[\n\\mathcal{R}_0\n= \\int_0^T R \\lambda e^{-\\lambda t} e^{-rt} \\, dt\n= \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\]\nDe manière générale, pour une date \\(t\\) :\n\\[\n\\mathcal{R}_t\n= \\int_t^T R \\lambda e^{-\\lambda u} e^{-ru} \\, du\n= \\lambda R \\, e^{(r+\\lambda)t}\n  \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n  \\mathbf{1}_{\\{T \\ge t\\}}\n\\]\nLa valeur totale de l’obligation est alors :\n\\[\n\\begin{aligned}\nB_t\n&= \\sum_{i=1}^{n}\n    c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}} \\\\\n&\\quad +\n\\left(\ne^{-(r+\\lambda)(T_n - t)}\n+\n\\lambda R \\, e^{(r+\\lambda)t}\n\\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\right)\n\\mathbf{1}_{\\{T_n \\ge t\\}}\n\\end{aligned}\n\\]\n\n\nImpementation de la valorisation d’un bond\n\n\nCode\n# Impementation de la fonctiuon de valorisation d'un bond\nimport numpy as np\n\ndef Bond(t,c,T,r,lamda, R = 0.4):\n    B = 0\n    for T_i in range(1,T+1):\n        B += np.exp(-(lamda + r)*(T_i - t))*(T_i&gt;=t)\n    B *= c\n    B += (np.exp(-(r + lamda)*(T_i-t)) + lamda * R  * (1-(np.exp(-(r + lamda)*(T_i -t)))) / (r + lamda))*(T_i&gt;=t)\n    return B\n\n\n\nExemple: Obligation au pair\n\nlamda = 0\n\n\nr = 0.02\n\n\nc = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nBond(t,c,T,r,lamda)\n\n\nCode\n# Exemple1: Obligation au pair\nlamda = 0\nr = 0.02\nc = 0.02\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.9981933497987289)\n\n\nAvec les paramètres ci-dessus considérés, on remarque que le prix de l’obligation est proche du nominal. En effet, le taux coupon est égal au taux de marché, ce qui indique que l’obligation est remunérée au taux du marché. Il s’agit donc d’une obligation au pair.\nAu cas où on aurait proposé une remunération supérieure à celle du marché, elle serait beaucoup plus attractive et sa valeur se serait appréciée. l’illustration est donnée ci dessous pour c= 0.03\n\n\nBond(0,0.03,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.03,10,0.02,0)\nprice\n\n\nnp.float64(1.0879246481591023)\n\n\nAvec une remunération inférieure à ce qu’aurait proposé le marché, on obtient une valorisation de l’obligation inférieure au nomimal\n\n\nBond(0,0.015,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.015,10,0.02,0)\nprice\n\n\nnp.float64(0.9533277006185421)\n\n\nEtant donné une intensité de défaut supérieure à zéro, on peut également calculer le coupon pour lequel l’obligation est au pair. On obtient, après calculs prsentés ci-dessous, un taux coupon de 2.6%.\n\n\nCode\n# Recherche du coupon  pour émettre une obligation au pair.\ndef dichot(t,T,r,lamda, P_MKT):\n    c_inf = 1e-8\n    c_sup = 1\n    epsi = 1e-8\n    c_moy = (c_inf + c_sup)/2\n    error = c_sup - c_inf\n\n    while error&gt;epsi:\n        p_hw = Bond(t,c_moy,T,r,lamda)\n        if p_hw &gt; P_MKT:\n            c_sup = c_moy\n        elif p_hw &lt; P_MKT:\n            c_inf = c_moy\n        c_moy = (c_inf + c_sup)/2\n        error = np.abs(c_sup - c_inf)\n\n    return c_moy\n\n\n\n\n\nTaux coupon pour émettre une obligation au pair\n\nlamda = 0.01\n\n\nr = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nCode\n# Taux coupon pour émettre une obligation au pair\nlamda = 0.01\nr = 0.02\nt = 0\nT = 10\ndichot(t,T,r,lamda, P_MKT = 1)\n\n\n0.026393926193952293\n\n\nEn maintenant une remunération égale à celle du marché , avec une intensité de défaut très grande (de l’ordre de 1000%), l’obligation tombe presque instantannément en défaut. Dans cette situation, la valeur du coupon vaut alors 39.92% qui est sensiblement proche du taux de recouvrement (40%).\n\n\n\nValeur du bond poiur une intensité de défaut à 1000%\n\n\nCode\n# Valeur du bond poiur une intensité de défaut à 1000%\nlamda = 10\nr = 0.02\nc = 0.03\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.39920293189432754)\n\n\n\n\n\nÉvolution du prix de l’obligation en fonction du temps\n\n\nCode\nimport matplotlib.pyplot as plt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB = np.array([Bond(t,c,T,r,lamda) for t in times])\nplt.plot(times,B)\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.title(\"Evolution du prix plein coupon du Bond\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nPlus on se rapproche de la date de détachement du coupon plus l’obligation devient attractive, elle prend donc de la valeur.\nChaque saut correspond à un détachement de coupons. une fois le coupon détaché de l’obligation, les flux à venir diminuent et la valeur de l’obligation se deprecie de la valeur du coupon qui a été détaché.\nCes sauts ne reflètent donc pas une dépréciation des bonds par le marché. Ce sont des sauts techniques. Raison pour laquelle on dit que le prix plein coupon est pollué par le coupon (en anglais Dirty price).\nOn va donc s’interesser par la suite au clean price ou pied de coupon qui correspond à au prix du bond moins le coupon couru.\n\\[\n\\tilde{B}_t = B_t - cc\n\\]\nOù (cc) est le coupon couru :\n\\[\ncc = c \\times (t - T^*)\n\\]\nEn retirant cette valeur de coupon couru, on supprime cet effet de saut après les detachements de coupons\n\nImplémentation du clean price\n\n\nCode\n# Implémentation du clean price\n\ndef CleanPrice(t,c,T,r,lamda, R = 0.4):\n    cc = c*(t - np.floor(t))*(t&lt;=T)\n    return Bond(t,c,T,r,lamda, R)-cc\n\n\n\n\nPieds coupon\n\n\nCode\n# Pieds coupon\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for t in times])\nB_pieds_coupon = np.array([CleanPrice(t,c,T,r,lamda) for t in times])\nplt.plot(times,B_pieds_coupon, label =\"Pieds coupon\")\nplt.plot(times,B_plein_coupon, label =\"Plein coupon\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nEn retirant cette valeur de coupon couru, on supprime l’effet de saut après les detachements de coupons, et on observe une courbe sans discontinuités\n\n\n\nEvolution du prix plein coupon et du clean price en fonction du temps pour differents coupons\n\n\nCode\nlamda = 0.01\nr = 0.02\nc = 0.01\nT = 10\n\ntimes = np.linspace(0,10, num = 100)\nB_plein_coupon1 = np.array([Bond(t,0.01,T,r,lamda) for t in times])\nB_pieds_coupon1 = np.array([CleanPrice(t,0.01,T,r,lamda) for t in times])\n\nB_plein_coupon5 = np.array([Bond(t,0.05,T,r,lamda) for t in times])\nB_pieds_coupon5 = np.array([CleanPrice(t,0.05,T,r,lamda) for t in times])\n\nplt.plot(times,B_pieds_coupon1, label =\"Pieds coupon 1%\")\nplt.plot(times,B_plein_coupon1, label =\"Plein coupon 1%\")\n\nplt.plot(times,B_pieds_coupon5, label =\"Pieds coupon 5%\")\nplt.plot(times,B_plein_coupon5, label =\"Plein coupon 5%\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nLes constats que l’on fait : - Lorsque c = 1% : La rémunération de l’obligation est inférieure à celle offerte par le marché. Ainsi, lors de son émission, sa valeur sera nécessairement inférieure à celle du pair, ce qui explique une valeur initiale proche de 87 %. À maturité, on reçoit 100 % du nominal, plus 1 % de ce dernier, correspondant au coupon. Le résultat final est donc sensiblement égal à 101 %.\n\nLorsque c = 5% : L’obligation rémunère plus que ce que le marché offre, ce qui la rend particulièrement attractive dès son émission. Ainsi, sa valeur de départ sera d’environ 120 %. Au fur et à mesure que les obligations sont détachées, les flux futurs diminuent, ce qui entraîne une dépréciation de sa valeur. À maturité, on reçoit 100 % du nominal, plus 5 % du montant nominal, correspondant au coupon. Le résultat final est donc de 105 %.\n\n\n\nEvolution du prix de l’obligation en fonction du taux d’intérêt\nDe l’expression analytique du prix du bond, on observe que le prix est décroissant du taux d’intérêt. La figure ci-dessous en donne une illustration, toute chose égale par ailleurs.\nOn vérifie graphiquement que le prix du bond est de 100% lorsque le taux d’intérêt est égal au taux sans risque \\(r^* = c - \\lambda (1 - R)\\).\n\nEvolution du prix du bond en fonction du taux d’intérêt\n\n\nCode\n# Evolution du prix du bond en fonction du taux d'intérêt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR = 0.4\ninterest = np.linspace(0,0.10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest,B_plein_coupon, label =\"Plein coupon\")\nplt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\nplt.xlabel(\"Taux d'intérêt (%)\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\2938139823.py:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\n\n\nText(0.5, 1.0, \"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotion de sensibilité\nLa sensibilité mesure la variation du prix du bond face à une variation d’un facteur de risque. On distingue entre autres la sensibilité de taux et la sensibilité de crédit.\nDans cette sectioin nous nous intéressons particulièrement à la sensibilité de taux. Elle est définie par la formule:\n\\[\n    Sensibilité = - \\frac{dB_t}{dr} \\frac{1}{B_t}\n\\]\nInterprétation: Lorsque le taux d’intérêt bouge de 1%, alors le prix du bond bouge de -sensibilité %.\nCette sensibilité peut également être vue comme le barycentre des différentes échéances pondérées par les flux actualisés. C’est la duration.\n\\[\n    \\frac{\\sum T_i\\times F_i}{\\sum F_i}\n\\]\nExemple: En considérant une obligation de maturité 3 ans payant des coupons annuels, avec intensité de défaut nul. Alors son prix et la sensibilité de taux sont données par:\n\\[\n\\begin{aligned}\nP &= c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial P}{\\partial r}\n&= -1c \\, e^{-1r} - 2c \\, e^{-2r} - 3c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Sensibilité}\n&= \\frac{1c \\, e^{-1r} + 2c \\, e^{-2r} + 3c \\, e^{-3r}}\n        {c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}}\n\\end{aligned}\n\\]\nD’où l’expression barycentrique des échéances pondérées par les flux.\n\n\nCode\n# Sensibilité de taux du prix du bond\ndelta_r = 1e-4 #1bp\n\n\ndef Sensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    return -(Bond(t,c,T,r+delta_r,lamda, R)-B_t)/(delta_r*B_t)\n\n\nOn considère un bond dont les caractérisiques sont les suivqantes:\n\\[\n    \\begin{cases}\n        \\lambda = 0.01\\\\\n        r = 0.02\\\\\n        c = 0.03\\\\\n        T = 10\\\\\n        R = 0.4\\\\\n    \\end{cases}\n\\]\nLa sensiblité de ce bond est de 8.64. Ainsi, lorsque le taux d’intérêt augmente de 1 point de pourcentage, le prix du bond diminue de 8.64%.\n\n\nCode\n# Sensibilité de taux d'intérêt\nSensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4)\n\n\nnp.float64(8.643982489105056)\n\n\n\nEvolution de la sensibilité de taux fonction de la maturité\n\n\nCode\n# Evolution de la sensibilité au taux d'intérêt en fonction de la maturité\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,c,T,r,lamda) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité')\n\n\n\n\n\n\n\n\n\n\nOn constate que la sensibilité de taux croît avec la maturité et tend à être linéaire.\nPour des paramètres extrêmes \\(c = \\lambda = r = 0\\), la sensibilité (duration) est identique à la maturité comme illustré sur la figure ci-dessous. Cette remarque met en évidence la relation mlathématique entre la sensibilité et la maturité présentée plus haut.\nConnaissant la maturité, pour une variation du taux d’intérêt on peut donc donner une estimation “grossière” de la sensibilité (en approximant la duration par 0.8*maturité, par exemple).\n\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,1e-6,T,1e-6,1e-6) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité (cas extrême)\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité (cas extrême)')\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR d’une obligation.\nL’estimation de la VaR sur un bond peut se faire à partir de la senssibilité de taux ou par la méthode de repricing.\n\n\nEstimation de la VaR par la sensibilité du taux d’intérêt\nL’approche par la sensibilité se présente de la manière suivante:\nOn suppose que la dynamique du taux d’intérêt est donnée par \\[\\Delta r \\sim \\mathcal{N}(0, \\sigma \\sqrt{\\Delta t})\\].\nSachant que \\[\\frac{\\Delta P}{P} = - Duration \\times \\Delta r\\]\nil suit que \\[\\frac{\\Delta P}{P} \\sim \\mathcal{N}(0, Duration \\times \\sigma \\sqrt{\\Delta t})\\]\nUne approche par la sensibilité de la VaR à 99% donne \\[VaR = Duration \\times \\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\\] Où \\(z_{99\\%}\\) est le quantile d’ordre 99% de la loi normale standard.\nPour les besoins de notre exercice, on pose \\(\\sigma = 1\\%\\).\n\n\nCode\nfrom scipy.stats import norm\n\n# Calcul de la VaR par la sensibilité du taux d'intérêt\ndef Sensi_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    duration = Sensi(t,c,T,r,lamda, R)\n    var = duration*sigma*np.sqrt(H)*norm.ppf(alpha)\n    return var\n\n\n\n\nCode\nsigma= 0.01\nH = 1/12\nt = 0\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR =0.40\n\nSensi_VaR(sigma , H, t,c,T,r,lamda)\n\n\nnp.float64(0.05804942383590022)\n\n\nLa VaR à 99% d’horizon 1 an sur le bond est de 5.8%. - On note que la VaR est une fonction linéaire croissante de la volatilité. C’est une conséquence directe de l’hypothèse de normalité des variations du taux d’intérêt. Ainsi, une augmentation de la volatilité du taux d’intérêt s’accompagne d’une augmentation de la volatilité du prix du bond. - La VaR est une fonction décroissante du taux d’intérêt. En effet, une augmentation du taux d’intérêt entraîne une diminution de la valeur des bonds, et par conséquent des valeurs extrêmes atteintes par celles-ci; d’où la VaR décroît. Toutefois l’évolution de la VaR en fonction du taux d’intérêt n’est pas linéaire.\n\n\nEvolution de la Z-VaR en fonction du taux d’intérêt\n\n\nCode\n# Evolution de la VaR en fonction de la volatilité\nvol = np.linspace(0,1e-2, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for sigma in vol])\n\nplt.plot(vol, var, label =\"VaR_99%\")\nplt.xlabel(\"Volatilité\")\nplt.ylabel(\"VaR \")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de la volatilité\")\n\n\nText(0.5, 1.0, 'VaR à 99% à horizon 1 an en fonction de la volatilité')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Evolution de la VaR en fonction du taux d'intérêt\ninterest = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nEvolution de la Z-VaR en fonction de l’intensité de défaut\nPour des intensités de défaut croissantes, la VaR du bond décroît de façon exponentielle, ceteris paribus. Tout comme avec le taux d’intérêt, une augmentation de l’intensité de défaut s’accompagne d’une imminence du défaut et par conséquent de la diminution de la valeur du bond. D’où une diminution de la VaR.\n\n\nCode\n# Evolution de la VaR en fonction de l'intensité de défaut\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de défaut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR par repricing de l’obligation\nLa méthodologie consiste à revaloriser le bond pour une variation du taux d’intérêt, puis d’en déduire la variation du prix du bond qui en résulte.\n\\[\n    \\Delta r^* =\\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\n\\]\n\\[\n    VaR_{99\\%} = \\frac{B(r+ \\Delta r^*) - B(r)}{B(r)}\n\\]\n\n\nApproche repricing\n\n\nCode\n# Approche repricing\n\ndef repricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    delta_r = sigma*np.sqrt(H)*norm.ppf(alpha)\n    B_rep = Bond(t,c,T,r+delta_r,lamda, R)\n    B_r = Bond(t,c,T,r,lamda, R)\n    \n    return  (B_r - B_rep)/B_r\n\n\n\n\nCode\nrepricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99)\n\n\nnp.float64(0.05627224378244837)\n\n\n\nLa VaR obtenue par l’approche repricing, sous les mêmes conditions que précédemment, est de 5.6%. Cette VaR est inférieure à celle obtenue sous l’hypothèse de normalité des variations du taux d’intérêt.\nCeci s’explique par le fait que la duration est une fonction convexe décroissante du taux d’intérêt et représente une approximation affine de la valeur du bond en le taux d’intérêt. De manière générale, l’approche par les sensibilités surestime la VaR.\nL’évolution de la VaR, obtenue par reprincing, en fonction du taux d’intérêt ou de l’intensité de défaut (voir les graphiques ci-dessus) à la même allure que la z-VaR.\nEn règle générale la VaR doit être inférieure au seuil de 20% de par la limite règlementaire. L’utilisation de la z-VaR peut donc constituer un manque à gagner pour les institutions financieres. En effet, elles peuvent être amenées à dérisquer leur portefeuille en limitant leur investissements pour des raisons purement techniques.\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\ninterest = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda)for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de saut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")"
  },
  {
    "objectID": "risques/projets/demo.html#iii.-risque-de-contrepartie",
    "href": "risques/projets/demo.html#iii.-risque-de-contrepartie",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "III. Risque de contrepartie",
    "text": "III. Risque de contrepartie\nDans le cadre d’un engagement contratuel incluant deux parties prenantes, le risque de contrepartie fait reférence au rique de perte suite au défaut d’une des parties à remplir les engagements contractuels pré-établis: c’est un risque bilatéral.\nAfin de se couvrir du defaut de l’émetteur d’un bond, l’acheteur du bond peut entrer dans un credit default swap (CDS). Dans un tel contrat, lorsque la contrepartie est correlée à l’émetteur de l’obligation on parle de wrong way risk ce qui a pour conséquence d’exposer davantage le souscripteur. La contrepartie doit donc être décorrelée de l’émetteur.\nLe risque de contrepartie est en général mitigé par les appels de marge (chambre de compensation). - Quand la qualité de l’émetteur se dégrade, son spread de crédit spread croît et le CDS s’apprécie. - Si la contrepartie fait défaut, le détenteur perd \\(CDS = 1-R\\). - Un mécanisme d’appel de marge permet de mitiger le risque.\n\nLien entre CDS et obligation: la formule du triangle de crédit\nUn estimation du taux sans risque d’une obligation: \\[\n\\begin{aligned}\nP &= \\sum_{i=1}^{n} c \\, e^{-(r+\\lambda)T_i}\n    + e^{-(r+\\lambda)T_n}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\int_0^T e^{-(r+\\lambda)t} \\, dt\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\nL’obligation est au pair si et seulement si P = 1 Ainsi, \\[\n    (c + \\lambda R) \\frac{ 1- e^{-(r + \\lambda )T}}{r + \\lambda} + e^{-(r + \\lambda)T} = 1\n\\]\nQuand T tend vers l’infini le taux sans risque vaut alors, \\[\n    (c + \\lambda R)  = r + \\lambda\n\\]\n\\[\n    (c - r )  = \\lambda (1 - R)\n\\] - \\(c-r\\): spread ou prime de risque - \\(\\lambda\\): intensité de défaut - \\(1-R\\): LGD \\[\n    Spread = PD \\times LGD\n\\]\n\n\nCDS: principe\n\nPremium Leg: est payé tant que l’émetteur est “en vie”. \\[ PL = \\int_0^T s \\times e^{-rt} \\times e^{-\\lambda t}dt = s \\frac{1-e^{-(r+\\lambda)T}}{r + \\lambda}\\]\nDefault Leg: compensation à la date t si le défaut y survient. \\[DL = \\int_0^T(1-R) e^{-rt}\\times e^{-\\lambda t} dt = \\lambda (1-R) \\frac{1-e^{-(r+\\lambda)T}}{r+\\lambda}\\].\n\nLa valeur initiale d’un spread étant nulle, les deux jambes sont égales et il on obtient:\n\\[\n    s = \\lambda (1-R)\n\\]\nUn estimation de ce spread peut donc être déduite de la formule du triangle de crédit.\n\n\nSensibilité de crédit\nLa sensibilité de crédit d’une obligation est la variation du prix de l’obligation lorsque varie le spread de crédit de la contrepartie. De la formule du triangle, la sensibilité de crédit peut être déduite de la sensibilité par rapport à l’intensité de défaut \\(\\lambda\\).\n\\[\n    \\frac{\\partial P}{\\partial s} = \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda}\n\\]\n\\[\n    \\frac{\\partial P}{\\partial s} \\times \\frac{1}{P}= \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda} \\times  \\frac{1}{P}\n\\]\nOn peut calculer une VaR taux et une VaR crédit. Toutefois, la VaR qui sera regardée de près est celle issue de la variation conjointe des taux et des spreads de crédit.\n\nSensibilité de crédit du bond\n\n\nCode\n# Sensibilité de crédit du bond\ndelta_lambda = 1e-4 #1bp\n\ndef Sensi_credit(t,c,T,r,lamda, R = 0.4, delta_lambda = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    variation = -(Bond(t,c,T,r,lamda+delta_lambda, R)-B_t)/(delta_r*B_t)\n    return variation/(1-R)\n\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda)\n\n\nnp.float64(8.82119086802735)\n\n\n\nUne variation d’un point de pourcentage du spread de crédit entraîne une variation de 8.82% de la valeur du bond. Cette sensibilité de crédit est assez proche de la sensibilité taux. En effet, on observe de l’écriture mathématique de la valeur du bond que le taux d’intérêt et l’intensité apparaissent conjointement de façon additive (sauf pour le terme de recouvrement).\nCes deux sensibilitées sont identiques pour un taux de recouvrement nul.\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda, R = 0.0, delta_lambda = 1e-4)\n\n\nnp.float64(8.77911216566412)\n\n\n\n\n\nEstimation de la VaR par variation conjointe du taux d’intérêt et du spread de crédit\nOn suppose que la dynamique du taux d’intérêt est donnée par l’EDS\n\\[\n    dr_t = \\sigma dW_t\n\\]\nLe spread ne prenant pas de valeur négative, on supposera qu’il est log-normal. Sa dynamique est donnée par l’EDS\n\\[\n    \\frac{ds_t}{s_t} = \\alpha dZ_t\n\\]\nEn supposant \\(dW_t dZ_t = \\rho\\) et \\(Z_t = \\rho W_t + \\sqrt{1-\\rho^2}V_t\\) tel que \\(dV_t dW_t = 0\\),\nLa dynamique du spread s’écrit alors \\[\n\\frac{ds_t}{s_t} = \\alpha (\\rho W_t + \\sqrt{1-\\rho^2}V_t)\n\\]\nEn appliquant un schéma de discrétisation d’Euler, on peut alors simuler les taux \\(r_t\\) et les spreads \\(s_t\\) par:\n\\[\n    r_H = r + \\sigma \\sqrt{H} W_H\n\\]\n\\[\n    s_H = s(1 + \\alpha \\rho \\sqrt{H} W_H + \\alpha \\sqrt{1-\\rho^2} \\sqrt{H} V_H)\n\\]\nOù H est le pas de discrétisation.\nLa méthodologie de détermination de la sensibilité du prix d’une obligation conjointement au taux d’intérêt et au spread de crédit est la suivante: - Calculer la valeur du bond pour les paramètres initiaux - Simuler les taux \\(r_H\\) et les spreads \\(s_H\\) - Pour chaque simulation, calculer la valeur du bond (cetertis paribus) - pour déterminer la variation des nouveaux prix par rapport au prix initial - calculer le quantile d’ordre 99% des variations obtenues\n\n\nCode\n# Sensibilité conjointe de taux et de crédit\n\ndef SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12, M=10_000):\n    \n    u1 = np.random.uniform(0,1, size = M)\n    u2 = np.random.uniform(0,1, size = M)\n    w = norm.ppf(u1)\n    v = norm.ppf(u2)\n    s = lamda*(1-R)\n    \n    r_H = r + sigma*np.sqrt(H)*w\n    s_H = s*(1 + alpha*rho*np.sqrt(H)*w + alpha*np.sqrt(1-rho**2)*np.sqrt(H)*v)\n    \n    P_0 = Bond(0,c,T,r,lamda, R)\n    P_1 = Bond(H,c,T,r_H,s_H/(1-R), R)\n    mu = (P_1 - P_0)/P_0\n    return  - np.quantile(mu,0.01)\n\n\n\n\nCode\nsigma = 0.01 # Volatilité du taux d'intérêt\nalpha =0.4   # Volatilité du spread\nrho = 0.4    # Corrélation entre le spread et le taux d'intérêt\n\nnp.random.seed(90) # Pour la reproductibilité\nSimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12)\n\n\nnp.float64(0.0602691861203251)\n\n\n\n\nCode\nnp.random.seed(90)\n\nrhos = np.linspace(0,1, num= 1000)\nvar = np.array([SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12) for rho in rhos])\n\nplt.plot(rhos, var)\n\n\n\n\n\n\n\n\n\nLa corrélation \\(\\rho\\) doit être positive, car une augmentation des taux d’intérêt entraîne une meilleure rémunération du marché par rapport aux obligations. Pour rester attractifs, les coupons doivent alors augmenter. Cette hausse des coupons accroît le coût de financement de l’émetteur, ce qui conduit à une augmentation du spread de crédit. Il en résulte une corrélation positive entre les taux d’intérêt et les spreads de crédit.\nUne augmentation conjointe des taux et des spreads accroît le risque de défaut, en raison de la corrélation positive entre ces deux facteurs de risque. Ainsi, une corrélation élevée reflète un manque de diversification, ce qui expose davantage le portefeuille au risque systémique"
  },
  {
    "objectID": "risques/projets/demo.html#iv.-risque-modèle",
    "href": "risques/projets/demo.html#iv.-risque-modèle",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "IV. Risque modèle",
    "text": "IV. Risque modèle\nLe risque modèle désigne le risque associé à l’utilisation d’un modèle mathématique ou statistique qui ne représente pas correctement la réalité ou qui mène à des décisions erronées en finance et en gestion des risques.\nEléments du suivi du risque modèle: - Sanity check: Ecart de performance - Backtesting: Application aux données antérieures - Comparaison avec desmodèles plus riches - Provisionner au titre du risque de modèle"
  },
  {
    "objectID": "risques/projets/demo.html#v.risque-climatique",
    "href": "risques/projets/demo.html#v.risque-climatique",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "V.Risque climatique",
    "text": "V.Risque climatique\nLe risque climatique désigne les risques financiers liés au changement climatique et aux politiques de transition vers une économie bas carbone. Sa modélisation reste complexe, notamment en raison du manque de données historiques et de l’absence d’un cadre réglementaire clairement défini.\nOn distingue deux principaux types de risques climatiques :\n\nRisque physique: l correspond aux pertes financières causées par des événements climatiques extrêmes. Il est défini par deux éléments clés : la fréquence et la sévérité des événements.\n\nRisque aigu : Événements rares mais très intenses ( ouragans, inondations).\nRisque chronique : Changements progressifs et durables (élévation du niveau de la mer).\n\nRisque de transition: il découle de l’adaptation des entreprises et des acteurs économiques aux exigences de transition écologique.\n\nRisque politique et réglementaire : Durcissement progressif des régulations environnementales (taxe carbone).\nOpportunités technologiques : Innovations favorisant la transition énergétique et redéfinissant les modèles économiques.\n\n\nLa resilience des institutions financieres notamment aux changement climatique est évaluée via des stress tests climatiques dont les scénarios sont issus des données du NGFS (Network for Greening financial System)."
  },
  {
    "objectID": "risques/projets/liquidity.html",
    "href": "risques/projets/liquidity.html",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "",
    "text": "Ce document présente une introduction à la gestion des risques en Asset Management.\nL’objectif principal est de comprendre et d’appliquer les méthodologies de base utilisées pour analyser, mesurer et suivre les risques financiers au sein d’un portefeuille.\nL’étude couvre differentes notions de risk management notamment:\n- les méthodes de mesure du risque de marché (sensibilité, volatilité, Value-at-Risk, Tracking Error, stress tests et profil de liquidité),\n- les composantes clés du risque de crédit (intensité de défaut, sensibilité des obligations aux variations de spreads, triangle du crédit),\n- le risque de contrepartie et ses mécanismes de mitigation via les appels de marge,\n- ainsi que les risques émergents tels que le risque climatique et le risque modèle.\n\n\nCode\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import norm\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize"
  },
  {
    "objectID": "risques/projets/liquidity.html#introduction",
    "href": "risques/projets/liquidity.html#introduction",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "",
    "text": "Ce document présente une introduction à la gestion des risques en Asset Management.\nL’objectif principal est de comprendre et d’appliquer les méthodologies de base utilisées pour analyser, mesurer et suivre les risques financiers au sein d’un portefeuille.\nL’étude couvre differentes notions de risk management notamment:\n- les méthodes de mesure du risque de marché (sensibilité, volatilité, Value-at-Risk, Tracking Error, stress tests et profil de liquidité),\n- les composantes clés du risque de crédit (intensité de défaut, sensibilité des obligations aux variations de spreads, triangle du crédit),\n- le risque de contrepartie et ses mécanismes de mitigation via les appels de marge,\n- ainsi que les risques émergents tels que le risque climatique et le risque modèle.\n\n\nCode\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import norm\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize"
  },
  {
    "objectID": "risques/projets/liquidity.html#i.-risques-de-marché",
    "href": "risques/projets/liquidity.html#i.-risques-de-marché",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "I. Risques de marché",
    "text": "I. Risques de marché\n\nI.1. Construction d’un portefeuille diversifié\nOn construit un portefeuille constitué de 10 entreprises du CAC40. Nous avons choisi des entreprises de secteurs variés afin de s’assurer que le portefeuille soit diversifié.\nAinsi, notre portefeuille est constitué des entreprises des secteurs suivants :\n\nBanque et assurance : BNP Paribas (BN.PA), Crédit Agricole (CA.PA)\n\nAéronautique et défense : Airbus (AIR.PA)\n\nTechnologie: Capgemini (CAP.PA)\nConcession et Construction: Vinci (DG.PA)\nTélécommunications : Orange (ORA.PA)\n\nÉnergie : Engie (ENGI.PA)\n\nLuxe & Consommation : LVMH (MC.PA)\n\nMédias & Divertissement : Vivendi (VIV.PA)\n\nIndustrie Chimique : Air Liquide (AI.PA)\n\nNous allons procéder à une optimisation de Markowitz visant à construire un portefeuille qui minimise la volatilité (risque) sous la contrainte d’un niveau de rendement modulable en fonction du profil de risque de l’investisseur.\n\n\nCode\n# On recupère les données sur un historique de deux ans\n\nend_date = '2025-03-16'  \nstart_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=2*365)).strftime('%Y-%m-%d')\ntickers = ['AI.PA', 'AIR.PA', 'BN.PA', 'CA.PA', 'CAP.PA', 'DG.PA', 'ENGI.PA', 'ORA.PA', 'SAN.PA', 'VIV.PA']\n\ndf = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\ncac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\388541553.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**************        30%                       ]  3 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\388541553.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\ndf_close = df['Close']\ndf_volume = df['Volume']\n# Calcul des  rendements journaliers\nreturns = df_close.pct_change()\nreturns.dropna(inplace=True)\n\n\n\nAperçu des données\n\n\nCode\n#Aperçu des données\ndf.head(3)\n\n\n\n\n\n\n\n\nPrice\nClose\nVolume\n\n\nTicker\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-17\n126.875412\n110.644325\n49.379742\n14.572609\n157.424194\n91.923409\n10.256953\n8.935349\n79.612030\n8.390329\n1971502\n2171272\n3041249\n4474936\n719990\n3575676\n12989586\n20039590\n5122220\n7405822\n\n\n2023-03-20\n129.684906\n112.407211\n49.742428\n14.783135\n157.659653\n93.078445\n10.359279\n9.040173\n79.310608\n8.565088\n1139355\n1290939\n1697298\n2379408\n418826\n1207887\n7087933\n9795603\n2119074\n2624806\n\n\n2023-03-21\n129.288452\n115.250565\n50.186718\n14.955767\n160.579285\n95.104202\n10.579202\n9.144996\n79.948914\n8.741748\n745105\n1002062\n2018104\n2130100\n293112\n934999\n7313789\n9077226\n1449009\n2962587\n\n\n\n\n\n\n\nComme indicateur quantifiable de cette diversification nous presentons ci après la matrice de correlation entre les rendements journaliers des actifs du portefeuille pour notre période d’étude.\n\n\nCode\n# Calcul de la matrice de corrélation\ncorr_actifs = returns.corr().to_numpy()\n\n# Affichage du corrélogramme\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_actifs, annot=True, fmt='.2f', cmap='coolwarm', xticklabels=returns.columns, yticklabels=returns.columns)\nplt.title(\"Corrélogramme des actifs\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nI.1.1 Présentation du principe d’optimisation de Markovitz\nL’optimisation de Markowitz, également connue sous le nom de théorie moderne du portefeuille (MPT), permet de construire un portefeuille optimal en combinant des actifs de manière à minimiser la volatilité (ou le risque) tout en respectant une contrainte sur un niveau de rendement fixé. L’un des éléments clés de cette théorie est la détermination des poids de chaque actif dans le portefeuille, afin d’atteindre un compromis optimal entre le risque et le rendement.\nHypothèses principales :\n\nLes décisions sont prises en fonction du couple rendement-risque.\n\nLes investisseurs cherchent à diversifier leurs actifs pour réduire le risque.\n\nL’accès au taux sans risque est illimité.\n\nAucun coût de transaction n’est pris en compte.\n\nProgramme d’optimisation : L’objectif est de minimiser la volatilité sous la contrainte d’un rendement cible $ _T $ :\n\\[\n\\text{Minimiser } \\omega^t\\Sigma \\omega\n\\] Sous les contraintes :\n$$\n\\[\\begin{cases}\n    \\omega^t \\mu = \\mu_T \\\\\n    \\sum_{i=1}^{n} \\omega_i = 1\n\n\\end{cases}\\]\n$$\n\n\nI.1.2 Calcul des rendements annualisées et des volatilités annualisés des log_rendements pour chaque actif\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nI.1.3. Optimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nI.1.3. Optimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\n \n# Nombre d'actifs\nn_assets = len(yearly_return)\nmu = yearly_return\n\ndef portfolio_variance(weights):\n    return weights.T @ sigma @ weights\n\ndef weight_sum_constraint(weights):\n    return np.sum(weights) - 1\n\ndef target_return_constraint(weights, mu_target):\n    return weights.T @ mu - mu_target\n\ndef efficient_portfolio(mu_target, range_=(-0.1, None), positive=True):\n    # On initialise les poids de maniere équitable\n    init_weights = np.ones(n_assets) / n_assets\n\n    # Contraintes\n    constraints = [\n        {'type': 'eq', 'fun': weight_sum_constraint},\n        {'type': 'eq', 'fun': lambda w: target_return_constraint(w, mu_target)}\n    ]\n\n    # On verifie si on a définit une contrainte de positivité\n    if positive:\n        bounds = [(0, None) for _ in range(n_assets)]  # Chaque poids doit être &gt;= 0\n    else:\n        bounds = [range_ for _ in range(n_assets)] \n\n    # Optimisation\n    result = minimize(portfolio_variance, init_weights, bounds=bounds, method='SLSQP', constraints=constraints)\n\n    if result.success:\n        optimal_weights = result.x\n        portfolio_volatility = np.sqrt(portfolio_variance(optimal_weights))\n        return portfolio_volatility, optimal_weights\n    else:\n        return None, None\n\n\n\n\nI.1.4 Illustration pour un rendement cible de 10%\nPour un rendement annuel cible de 10%, correspondant à l’ordre de grandeur des rendements du CAC40 qui représente le benchmark du fond, les poids optimaux donnés par le l’optimisation de Markovitz et qui ont permis de minimiser le risque auquel est exposé le porte feuille ( 9.67% de volatilité) sont donnés ci après:\n\n\nCode\n# Rendement cible\nmu_target = 0.1 \n\n# les poids optimaux et la volatilité\nvol, weights = efficient_portfolio(mu_target)\nprint(\"Le portefeuille ainsi construit a un rendement de\",round(weights.T @ mu,4), \"et est soumis à une volatilité de l'ordre de :\", round(vol,4), \"\\n\")\nprint(\"---\"*50)\nprint(\"Les poids associés aux actifs du porte feuille sont listés ci après:\\n\")\ni = 0\nfor column in df_close.columns.tolist():\n    print(f\"{column}, {weights[i]:.4f}\")\n    i += 1\n\n\nLe portefeuille ainsi construit a un rendement de 0.1 et est soumis à une volatilité de l'ordre de : 0.0989 \n\n------------------------------------------------------------------------------------------------------------------------------------------------------\nLes poids associés aux actifs du porte feuille sont listés ci après:\n\nAI.PA, 0.0243\nAIR.PA, 0.0382\nBN.PA, 0.2031\nCA.PA, 0.1929\nCAP.PA, 0.1255\nDG.PA, 0.0000\nENGI.PA, 0.0732\nORA.PA, 0.2652\nSAN.PA, 0.0542\nVIV.PA, 0.0234\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), weights*100, color='skyblue')\n\nplt.title(f\"Poids optimaux des actifs dans le portefeuille (Volatilité: {round(vol, 4)})\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Poids (en %)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, weights):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nOn constate que le portefeuille optimal est assez diversifié.Les trois principales entreipes representées sont :\n\nTélécommunications : Orange (26.53%)\nBanque et assurance: BNP Paribas (20.24%), Credit Agricole (19.27%)\nTechnologie : Capgemini (12.52%)\nÉnergie : Engie (7.30%)\n\n\n\n\nI.2. Suivi du porte feuille\nDans cette section on va determiner l’AUM du portefeuille, suivre ses performances sa volatilité et ses performances relatives par rapport au CAC40.\nNous effectuerons également un stress test sur le portefeuille pour ebaluer sa sensibilité relative à la période COVID.\n\n\nCode\n# Poids des actions dans le portefeuille\nweights = pd.DataFrame({'Ticker': tickers ,\n     'Weight': weights})\nweights = weights.set_index('Ticker')['Weight']\n\n#print(weights)\n\n\n\nI.2.1. AUM (Asset under management)\nElle représente la valeur du porte feuille et est déterminée par la formule suivante\nAUM = (NAV)(Nombre_de_ parts)\nAvec\nLa Net Value Asset (NAV) représente la valeur nette d’une part du fonds (portefeuille) à un moment donné.\n\nDétermination de l’AUM\n\n\nCode\n# Détermination de l'AUM\naum = df_close@weights\nportfolio = pd.DataFrame({'AUM': aum})\nportfolio.head()\n\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-03-17\n47.527169\n\n\n2023-03-20\n47.829461\n\n\n2023-03-21\n48.500879\n\n\n2023-03-22\n48.360231\n\n\n2023-03-23\n48.621802\n\n\n\n\n\n\n\n\n\nCode\nportfolio.describe()\n\n\n\n\n\n\n\n\n\nAUM\n\n\n\n\ncount\n509.000000\n\n\nmean\n53.563785\n\n\nstd\n3.084849\n\n\nmin\n47.527169\n\n\n25%\n50.547944\n\n\n50%\n53.835640\n\n\n75%\n56.161583\n\n\nmax\n59.243516\n\n\n\n\n\n\n\nSur notre période d’etude, l’AUM en moyenne est de 59.10, et oscille entre 50.68 et 66.84.\n\n\n\nI.2.2 Performance du portefeuille\nDans le cadre du suivi de notre portfeuille, il est important d’identifier et de mesurer les risques auxquels il est exposé afin de mieux les encadrer. Dans cette section nous intéressons tout particulièrement au risque de marché. Ce risque peut être évalué à l’aide de:\n\nLa tracking error\nLa volatilité\nLa VaR\n\nqui sont des mesures ex-ante.\n\nOn s’intéresse aux performances du portefeuille construit ainsi que celles du benchmark\n\n\nCode\n# Performance du portefeuille\nportfolio['perf'] = portfolio['AUM'].pct_change()\nportfolio.dropna(inplace=True)\n\n# Performance du benchmark\ncac40_df = cac40['Close']\ncac40_df ['perf'] = cac40_df.pct_change()\ncac40_df.dropna(inplace=True)\n\nprint(\"Rendement annualisé du fond: \\t\", (1+portfolio.perf.mean())**252 - 1)\nprint(\"Rendement annualisé du CAC40: \\t\", (1+cac40_df.perf.mean())**252 - 1)\nprint(\"Performance relative du fond: \\t\", (1+portfolio.perf.mean())**252 - (1+cac40_df.perf.mean())**252 )\n\n\nRendement annualisé du fond:     0.0825477892761306\nRendement annualisé du CAC40:    0.08502178188176579\nPerformance relative du fond:    -0.002473992605635189\n\n\nLa performance absolue du fonds est de 8.24%, tandis que celle du CAC40 est de 8.50%. Pour évaluer la performance relative du fonds par rapport à son indice de référence, on calcule l’écart de performance : -0.256%. Cela signifie que le portefeuille sous-performe par rapport à son benchmark.\n\n\n\nI.2.2.1 Tracking error\nLa Tracking Error (TE) mesure l’écart de performance entre un fonds et son indice de référence (benchmark). Elle est définie comme l’écart-type des différences de rendement entre le fonds et le benchmark sur une période donnée. Plus la Tracking Error est faible, plus le fonds suit fidèlement son indice de référence.\n\\[\nTE = \\sqrt{\\frac{\\sum_1^n (r_{fond} - r_{benchmark})^2}{n-1}}\n\\]\n\n\nCode\n# Tracking error\nportfolio['benchmark'] = cac40_df['perf']\nportfolio['tracking_error'] = portfolio['perf'] - portfolio['benchmark']\nTError = portfolio['tracking_error'].std()*np.sqrt(252) # Annualisation\n\nTError\n\n\nnp.float64(0.10911793626160475)\n\n\nIl ressort que la dispersion des rendements du fond par rapport ceux de son indice de reférence est de 10.8% en moyenne.\n\n\nI.2.2.2 Volatilité\nAfin d’evaluer le risque de marché du fond, la volatilité complète l’information apportée par la tracking error et permet de se faire une idée sur la regularité et la dispersion des rendements du fond sur la periode d’étude. Il s’agit de l’écart-type des performances absolues du fond.\n\nVolatilité annualisé du portefeuille\n\n\nCode\n# Volatilité annualisé du portefeuille\nstd_dev = portfolio.std()*np.sqrt(252)\nstd_dev\n\n\nAUM               48.833272\nperf               0.132069\nbenchmark          0.128884\ntracking_error     0.109118\ndtype: float64\n\n\n\n\nVolatilité du benchmark\n\n\nCode\n# Volatilité du benchmark\ncac40_df.std()*np.sqrt(252)\n\n\nTicker\n^FCHI    5304.018890\nperf        0.128884\ndtype: float64\n\n\n\n\n\nI.2.2.3 Value at risk\nLa value-at-risk (VaR) est une mesure très répendue en gestion de risques. Elle permet d’evaluer les pertes extrêmes encourrues sur un portefeuille sur un horizon donné sous un niveau de confiance. Elle est généralement calculée sur un horizon 1 jour. Pour obtenir une VaR sur un horizon (H) plus grand, on utilise la méthode de scaling qui consiste à multpiplier la VaR 1 jour par la racine carrée de H.\nElle est donnée par \\(\\mathbf{P}(P \\& L &lt; VaR_H) = \\alpha\\)\nLa VaR est déterminée à l’aide diverses approches: - Approche historique: Elle consiste à calculer le quantile empirique d’ordre \\(\\alpha\\) des pertes historiques. C’est l’approche la plus utilisée en pratique. Cette méthode présente toutefois des limites en cas de changement soudain et inhabituel de conjoncture (crise etc.)\n\nApproche paramétrique: Ici on fait l’hypothèse que les P&L sont distribués suivant un loi dont les paramètres sont calibrés sur les données historiques. Comme toute méthode paramétrique, elle peut conduire à des estimations très biaisées si la loi supposée n’est pas adaptée aux données\n\n\nVisualisation de la distribution des log-rendements journaliers\n\n\nCode\nimport seaborn as sns\n\nsns.histplot(portfolio.perf)\nsns.kdeplot(portfolio.perf, color='red')\n\n\n\n\n\n\n\n\n\n\n\n\nOn s’interesse aux VaR sur un horizon de 20 jours\n\n\n(a) VaR historique\n\n\nCode\nVaR = portfolio.quantile(1-0.99)*np.sqrt(20) # 1-day VaR scaled to 20-day VaR\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.09788041098109732\nla VaR  Relative à 99% d'horizon 20 jours  est : -0.08993823222591216\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\764873701.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\764873701.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 9.66%. Le porte feuille a 99% de chance de sous performer son indice de reference de 8.96% sur un horizon de 20 jours.\n\n\n(b) VaR paramétrique\n\n\nCode\nVaR = (portfolio.mean() + norm.ppf(1-0.99)*portfolio.std())*np.sqrt(20)\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.08514686514691171\nla VaR  Relative  à 99% d'horizon 20 jours  est : -0.07155361125232418\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\937860044.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\937860044.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 8.47%. Le porte feuille a 99% de chance de sous performer son indice de reference de 7.10% sur un horizon de 20 jours.\nOn observe que les VaR paramétriques sont strictement inférieures aux VaR historiques. Ceci pourrait s’expliquer par le fait que les données ont des queues plus lourdes que la loi normale comme l’illustre le graphique présenté plus haut. De plus l’excess de kurtosis confirme cette observation. Par conséquent, la VaR obtenue sous une paramétrisation gaussienne sous-estime la perte maximale encourrue sur le portefeuille."
  },
  {
    "objectID": "risques/projets/liquidity.html#stress-test",
    "href": "risques/projets/liquidity.html#stress-test",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Stress test",
    "text": "Stress test\nLe stress test ou test de résistance est un exercice qui consiste à reproduire des scénarios extrêmes sur un portefeuille afin d’évaluer sa résilience en conjoncture défavorable. On distingue notamment deux types de stress test: - Le stress test historique qui vise à reproduire des scénarios de crises historisques, - Le stress test hypothétique qui consiste à considérer des scénarios théoriques extrêmes.\nNous allons dans la suite implémenter un stress test historique en reproduisant le choc COVID entre 19/02/2020 et 19/03/2020. Il s’agit s’agira de calculer la performance du fonds entre ces deux date et de la comparer à celle de son indice de référence.\n\nDonnées des actions en période covid\n\n\nCode\n# Données des actions en période covid\nstress_date_1 = \"2020-02-19\"\nstress_date_2 = \"2020-03-19\"\n\ntickers = ['SAN.PA', 'BN.PA',  'AIR.PA', 'AI.PA', 'ORA.PA', 'CAP.PA', 'VIV.PA', 'CA.PA', 'ENGI.PA', 'DG.PA']\ndf_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\ncac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\3326232529.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**********            20%                       ]  2 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\3326232529.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\naum_stress = df_stress@weights\nportfolio_stress = pd.DataFrame({'AUM': aum_stress})\n\n\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\nportfolio_stress.iloc[-1,] /portfolio_stress.iloc[0,]  - 1\n\n\nAUM   -0.335339\ndtype: float64\n\n\n\n\nRendement du benchmark enn période covid\n\n\nCode\n# Rendemnt du benchmark enn période covid\ncac40_stress.iloc[-1,] / cac40_stress.iloc[0,] -1\n\n\nTicker\n^FCHI   -0.385585\ndtype: float64\n\n\nLe fonds stressé a une performance absolue de -33.69% contre -38.55% pour son benchmark. Ces performances en période de crise COVID sont du même ordre de grandeur. Toutefois, la perte enregistrée sur le fonds est moins importante. Une analyse des performances des actions constituant le fonds montre que la sous-performance enegistrée est essentiellement portée par: Air Liquide, Capgenie et Engie.\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\ndf_stress.iloc[-1,] /df_stress.iloc[0,]  - 1\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1, color='skyblue')\n\nplt.title(f\"Performance de la composition du fond stressé\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Performance (%)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "risques/projets/liquidity.html#cadre-de-suivi-de-la-liquidité",
    "href": "risques/projets/liquidity.html#cadre-de-suivi-de-la-liquidité",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Cadre de suivi de la liquidité",
    "text": "Cadre de suivi de la liquidité\n\nLe suivi de la liquidité est d’une importance centrale en gestion de fonds et est encadré par des exigences règlementaires. Il s’agit entre autres pour le gestionnaire de pouvoir honorer ses engagements vis-à-vis de des investisseurs en cas de rachat de parts du fonds. Il faut donc s’assurer de la capacité à revendre les actifs composant le fonds dans des délais courts sans toutefois les brader.\nC’est ce qu’on appelle le risque de liquidité qui se défini plus formellement comme étant la facilité avec laquelle une entreprise peut échanger ses actifs contre du cash, même en situation soudaine de besoin de liquidité, sans subir de coûts anormaux par rapport aux autres acteurs du marché.\nLe suivi de la liquidité d’un fonds s’effectue par le calcul de son profil d’écoulement en condition normale et en condition stressée.\n\nLa méthodologie de calcul d’un profil d’écoulement\nLe fonds est constitué de n actifs en quantité \\(Q_i\\) chacun à liquider. Dans la pratique il existe une quantité maximale au delà de laquelle les échanges ne peuvent se faire sans subir de coûts anormaux: c’est la profondeur de marché. Le coût de liquidation est induit par les volume d’ordre émis.\nLa profondeur de marché est généralement estimée en examinant: - Les carnets d’ordres ou - Les volumes quotidiens échangés: On considère notamment le volume quotidien moyen échangé sur 3 mois appelé ADV ou ATV (Average Daily/Traded Volume). La profondeur de marché est alors estimée par \\(Q^* = 20\\% \\times ADV_{3 mois}\\)\n\nOn distingue quatre profils d’écoulement: - En codition normale: avec et sans déformation - En condition stressée: avec déformation et sans déformation\nDans une liquidation avec déformation, les actifs les plus liquides sont liquidés en premiers. La déformation du portefeuille s’illustre par les changements de poids des différents émetteurs dans le fonds. La liquidation sans déformation quant à elle consistera à liquider le fonds au rythme de l’actif le moins liquide afin de préserver les poids initiaux.\nN.B: Le profil de liquidation est représenté en proportion du portefeuille initial.\nOn utilise le profil de liquidité pour déterminer la taille optimal du fonds permettant de respecter les exigence de minimum de liquidité. Il s’agira de s’assurer que plus de 90% du fonds est liquidé en deux jours, laissant ainsi un maximum de 10 % dans la catégorie dite “poubelle” (c’est-à-dire les actifs moins liquides).\nEn condition stressée avec déformation: On se place dans une situation dans laquelle la profondeur de marché est réduite à la suite d’un choc conduisant à un assèchement du marché.\n\n\nCode\ndef liqudity(df_volume, ADV_rate=0.2, weight_adjust=1, label=\"Conditions normales avec déformation\", seed = 42):\n    \"\"\"\n    Fonction de calcul de la liquidité d'un portefeuille sur plusieurs jours.\n    \n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        DataFrame contenant les volumes quotidiens pour chaque actif (tickers).\n    ADV_rate : float, optionnel\n        Taux de liquidité journalière exprimé en pourcentage de l'ADV (par défaut 0.2, soit 20%).\n    weight_adjust : float, optionnel\n        Facteur de pondération pour ajuster la quantité générée aléatoirement (par défaut 1).\n    label : str, optionnel\n        Label pour décrire le scénario de liquidation (par défaut \"Conditions normales avec déformation\").\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        DataFrame contenant les informations suivantes pour chaque actif :\n            - 'Ticker' : Le symbole de l'actif.\n            - 'ADV' : Le volume moyen quotidien calculé sur une fenêtre de 60 jours.\n            - 'Prix' : Le prix actuel de l'actif (extrait de df_close).\n            - 'QUANTITE' : La quantité totale simulée de l'actif détenu par le portefeuille.\n            - 'QUANTITE LIQUIDABLE' : La quantité qui peut être liquidée quotidiennement (ADV_rate * ADV).\n            - 'NB_JOURS DE LIQUIDATION' : Nombre de jours estimés pour liquider entièrement l'actif.\n            - 'JOUR X' (où X est un numéro de jour) : Quantité restante après chaque jour de liquidation.\n            - 'POIDS X' : Poids relatif de chaque actif dans le portefeuille après chaque jour de liquidation.\n    \n    Description :\n    -------------\n    1. Calcul de l'Average Daily Volume (ADV) sur une fenêtre de 60 jours.\n    2. Création d'un DataFrame contenant les informations de liquidité de chaque actif.\n    3. Génération de quantités d'actifs aléatoires pour simuler un portefeuille.\n    4. Calcul de la quantité liquidable par jour (20 % de l'ADV par défaut).\n    5. Simulation de la liquidation progressive de chaque actif jour par jour jusqu'à épuisement.\n    6. Calcul des poids du portefeuille après chaque jour de liquidation.\n    \n    \"\"\"\n    # Calcul de la moyenne mobile sur 60 jours du volume de transactions (ADV : Average Daily Volume)\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de l'ADV le plus récent pour chaque ticker\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame contenant l'ADV, le prix actuel et d'autres informations pour chaque actif\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # On définit la colonne 'Ticker' comme index pour faciliter l'accès aux données\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Génération de quantités d'actifs simulées basées sur l'ADV\n    np.random.seed(seed)  # Pour des résultats reproductibles\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable : 20 % de l'ADV (peut être ajusté via 'ADV_rate')\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours nécessaires pour liquider chaque actif\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(\n        portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE']\n    )\n\n    # Initialisation pour le jour 0\n    i = 1\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    # Calcul de la quantité restante après le premier jour de liquidation\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    # Calcul itératif jusqu'à liquidation complète\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        # Calcul de la quantité restante après chaque jour supplémentaire\n        portfolio_liquidity[f'JOUR {i}'] = (\n            portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n        ).clip(lower=0)\n\n        # Mise à jour des poids du portefeuille pour ce jour\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    return portfolio_liquidity\n\n\n\n\nCode\ndef liquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales avec déformation\"):\n    \"\"\"\n    Fonction qui génère un profil de liquidité d'un portefeuille en fonction du nombre de jours nécessaires pour liquider tous les actifs.\n\n    Paramètres :\n    -----------\n    portfolio_liquidity : DataFrame\n        Le DataFrame généré par la fonction liqudity() contenant les quantités restantes par jour pour chaque actif.\n    label : str, optionnel\n        Titre du graphique affiché (par défaut \"Profil de liquidité en conditions normales avec déformation\").\n\n    Description :\n    -------------\n    1. Calcule la quantité restante d'actifs pour chaque jour jusqu'à liquidation totale.\n    2. Calcule le profil de liquidité en pourcentage du portefeuille initial liquidé au fil du temps.\n    3. Génère deux graphiques :\n       - Profil de liquidité en pourcentage du portefeuille initial.\n       - Évolution des poids des actifs au fil des jours.\n\n    Retourne :\n    ---------\n    Affiche deux graphiques côte à côte montrant la liquidation progressive et l'évolution des poids.\n    \"\"\"\n    # Calculer le profil de liquidité\n    liquidity_profile = [portfolio_liquidity['QUANTITE']]\n    for i in range(1, int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        liquidity_profile.append(portfolio_liquidity[f'JOUR {i}'])\n\n    liquidity_profile = pd.DataFrame(liquidity_profile)\n\n    # Calculer le profil en pourcentage de liquidité restante\n    profile = liquidity_profile @ portfolio_liquidity['Prix']\n    portefeuille_valeur = profile['QUANTITE']  # Récupérer la valeur du portefeuille\n    profile = 100 - profile[1:,] * 100 / portefeuille_valeur\n\n    # Créer une figure avec deux sous-graphiques côte à côte\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))  \n\n    # Premier graphique : Profil de liquidité\n    ax1 = axes[0]\n    profile.plot(ax=ax1, marker='o', legend=False, color='blue')\n    barplot = profile.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black', alpha=0.5)\n\n    # Affichage des valeurs sur les barres\n    for bar in barplot.patches:\n        value = round(bar.get_height(), 2)\n        ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{value}\",\n                 ha='center', va='bottom', fontsize=10, color='black')\n\n    ax1.set_title(f\"{label}\", fontsize=14)\n    ax1.set_xlabel(\"Jours\", fontsize=12)\n    ax1.set_ylabel(\"Liquidité du portefeuille (%)\", fontsize=12)\n    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Étiquette de valeur du portefeuille\n    ax1.text(0.95, 0.05, f\"Valeur portefeuille : {round(portefeuille_valeur)} euros\",\n             transform=ax1.transAxes, fontsize=12, color='red', ha='right', va='bottom',\n             bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"red\"))\n\n\n    # Deuxième graphique : Évolution des poids\n    ax2 = axes[1]\n    for i in range(int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        if f'POIDS {i}' in portfolio_liquidity.columns:\n            ax2.plot(portfolio_liquidity[f'POIDS {i}'], label=f'Jour {i}')\n        else:\n            print(f\"Colonne 'POIDS {i}' non trouvée.\")\n\n    ax2.set_title(\"Évolution des Poids au Fil des Jours\", fontsize=14)\n    ax2.set_xlabel(\"Index\", fontsize=12)\n    ax2.set_ylabel(\"Poids\", fontsize=12)\n    ax2.legend()\n    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Ajustement de l'affichage\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nProfil d’écoulement avec déformation\n\nProfil d’écoulement en conditions normales avec déformation\n\n\nCode\nportfolio_liquidity = liqudity(df_volume)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLe portefeuille étudié est liquidable en 8 jours au minimum. Au premier jour seul 23.17% du portefeuille est liquidable et atteint les 73.54% au 4eme jour. On observe que les poids des actifs composant le portefeuille sont déformés au fil des jours. Il y a alors une concentration du portefeuille sur les actifs les moins liquides. Cette situation est domageable pour les investisseurs restant dans le fonds, car il faudra plus de temps pour honorer les engagements du fonds vis-à-vis d’eux en cas de rachat ce qui augmente leur exposition au risque de liquidité..\nAfin de protéger ces investisseurs, le régulateur impose aux gérants de fonds de fixer des GATES qui doivent obligatoirement être présentés dans leurs prospectus. Cela consiste à donner une option au gérant pour restreindre les rachats quotidien, à 5% du fonds par exemple.\n\n\n\nProfil d’écoulement en conditions stressée avec déformation\nEn conjoncture défavorable, la profondeur de marché est réduite ce qui entraîne un contraction des volumes liquidables. Nous simulons un tel choc en réduisant la profondeur de marché à 10% l’ADV.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLa contraction de la profondeur de marché rallonge la durée minimale de liquidation du fonds à 15 jours contre 8 jours initialement. Ceci illustre bien l’exposition du fonds au risque de liquidité. Il devient donc indispensable pour le gérant du fonds de déterminer une taille optimale de celui-ci afin de réduire son exposition au risque de liquidité.\n\n\n\nTaille optimale du portefeuille\n\nOn peut ajuster la taille du portefeuille en faisant varier le paramètre weight_adjust de la fonction liqudity afin de déterminer la taille optimale du fonds liquidable en 1 jour.\nPour le cas étudié, le fonds optimal a une valeur de 123 184 238 euros dont 99.82% est liquidable en 1 jour.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\nEn repétant le scénario défavorable d’un choc qui réduit la profondeur de marché à 10% l’ADV, le fonds optimal ainsi constitué est liquidable à 99.82% en deux jours. Il est donc plus résilient.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\n\n\n\nProfil d’écoulement sans déformation\nRappelons qu’une politique de gestion sans déformation consiste à appliquer des rachats proformat. Ceci garantisse une stabilité de la composition du fonds.\n\n\nCode\ndef liqudity_proformat(df_volume, ADV_rate = 0.2, weight_adjust = 1):\n    \"\"\"\n    Fonction qui génère un DataFrame décrivant la liquidité d'un portefeuille sur plusieurs jours, en fonction de la quantité disponible par jour et des poids correspondants.\n\n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        Un DataFrame contenant les volumes quotidiens de chaque ticker.\n    ADV_rate : float, optionnel\n        Le pourcentage d'Average Daily Volume (ADV) disponible chaque jour pour être liquidé (par défaut : 0.2, soit 20 %).\n    weight_adjust : float, optionnel\n        Facteur d'ajustement appliqué pour moduler la quantité calculée (par défaut : 1).\n\n    Description :\n    -------------\n    1. Calcule l'ADV (volume moyen sur 60 jours) pour chaque actif.\n    2. Génère un DataFrame contenant l'ADV, les prix actuels, les quantités à liquider, et les quantités liquidables par jour.\n    3. Calcule le nombre de jours nécessaire pour liquider complètement chaque actif.\n    4. Produit une série de DataFrames pour chaque jour, montrant la quantité restante, les poids associés, et les vitesses de liquidation.\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        Un DataFrame contenant les informations de liquidité du portefeuille sur plusieurs jours.\n    \"\"\"\n    # Calcul de la moyenne mobile du volume (ADV) sur une fenêtre de 60 jours\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de la dernière ligne (les valeurs ADV les plus récentes)\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame pour la liquidité du portefeuille\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # Définir 'Ticker' comme index pour un accès plus facile\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Calcul de la quantité (QUANTITE) basée sur le volume le plus récent\n    np.random.seed(42)\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable par jour\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours de liquidation nécessaire\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE'])\n\n    i = 1\n    # Initialisation du premier jour\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity['QUANTITE']\n\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        portfolio_liquidity[f'JOUR {i}'] = (\n            (portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'] * portfolio_liquidity[f'SPEED {i-1}'].min())\n        ).clip(lower=0)\n\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n        portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity[f'JOUR {i}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity[f'JOUR {i}']\n\n    return portfolio_liquidity\n\n\n\nProfil d’écoulement en conditions normales sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales sans déformation\")\n\n\n\n\n\n\n\n\n\nLes actions sont liquidées à la vitesse de l’actif le moins liquide du portefeuille. Par conséquent, contrairement à un écoulement avec déformation, la quantité liquidable progresse plus lentement dans le temps. Par exemple, on observe qu’en quatre jours seuls 59,01 % du fonds peuvent être liquidés, contre 73,54 % dans un scénario avec déformation.\nPar ailleurs, les poids des différents actifs restent globalement stables au fil des jours, à l’exception du dernier jour où l’intégralité du fonds est liquidée. Cette stabilité implique qu’il n’y a pas de concentration progressive du risque de liquidité sur les actifs les moins liquides, ce qui constitue une caractéristique importante du mécanisme d’écoulement sans déformation.\n\n\nProfil d’écoulement en conditions stressée sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions stressée sans déformation\")\n\n\n\n\n\n\n\n\n\nUn choc sur la profondeur de marché a pour effet un rallongement de la durée minimale de liquidation du fonds. Toutefois, les poids restent bien stables."
  },
  {
    "objectID": "risques/projets/liquidity.html#ii.-risque-de-crédit-et-risque-de-taux",
    "href": "risques/projets/liquidity.html#ii.-risque-de-crédit-et-risque-de-taux",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "II. Risque de crédit et risque de taux",
    "text": "II. Risque de crédit et risque de taux\nLe risque de crédit est le risque de perte engendrée par la defaillance d’une partie prenante à remplir ses engagements contractuels préalablement établis. C’est principal risque observé sur périmètre du retail.\nOn définit le taux d’intérêt comme le loyer de l’argent (annualisé dans la pratique).\nCette définition est celle historique car très intuitive. Il peut cependant s’averer que les taux d’interêt soient négatifs. Là cette definition devient donc limitée.\nDepéndamment de ce qu’on fait de son argent, le thésauriser, le prêter à la banque , ou encore à l’etat, il existe toujours un risque de perte qui subsiste. Il peut donc advenir, que dans le souci de sécuriser son argent, dans un contexte particulier (notamment incertain), le posseusseur soit disposé à payer pour securiser son argent : on parle de taux d’intérêt négatif.\nCela illustre la métaphore du loyer du coffre fort.\n\nGénéralités sur le pricing d’obligations\nCONTEXTE\nEn raison des obligations réglementaires auxquelles les banques sont soumises, elles ne peuvent pas prêter à toutes les entreprises ayant besoin de financement. C’est dans ce contexte que la notion d’obligation prend son sens. La banque agit alors en tant qu’intermédiaire entre l’entreprise et le marché, et perçoit des frais de commission. le marché prête un montant M à l’entreprise et reçoit des annuités et le nominal à maturité.\nUne obligation est, économiquement, un prêt-emprunt.\nDe manière générale, la valorisation d’un actif est l’espérance des flux actualisés au taux sans risque sous la probabilité risque neutre :\n\\[\n\\begin{aligned}\nX_0 &= \\mathbb{E}[e^{-rT} X_T] \\\\\n    &= e^{-rT} \\, c \\times PS(T)\n\\end{aligned}\n\\]\nN.B : Le taux de recouvrement historique est de 40 %.\nLe recouvrement s’applique uniquement au nominal.\nLa probabilité de survie \\(PS(T)\\) est généralement déterminée à partir du modèle à intensité de Poisson via :\n\\[\nPS(T) = e^{-\\lambda T}\n\\]\nConsidérons une obligation d’échéances \\(T_i\\), \\(i = 1, \\dots, n\\), de coupon \\(c\\) et de nominal \\(N\\).\nLes coupons et le nominal sont payés en cas de survie, et le recouvrement en cas de défaut.\nLa valeur de cette obligation à la date \\(t\\) vaut :\n\\[\nC_t = \\sum_{i=1}^{n} c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}}\n\\]\nLa probabilité de survenue du défaut à une date \\(t\\) vaut :\n\\[\n\\begin{aligned}\nPD(t)\n    &= PS(t) - PS(t + dt) \\\\\n    &= -\\frac{PS(t+dt) - PS(t)}{dt} \\, dt \\\\\n    &= -\\frac{dPS(t)}{dt} \\, dt \\\\\n    &= \\lambda e^{-\\lambda t} \\, dt\n\\end{aligned}\n\\]\nLa valeur actualisée du recouvrement vaut :\n\\[\n\\mathcal{R}_0\n= \\int_0^T R \\lambda e^{-\\lambda t} e^{-rt} \\, dt\n= \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\]\nDe manière générale, pour une date \\(t\\) :\n\\[\n\\mathcal{R}_t\n= \\int_t^T R \\lambda e^{-\\lambda u} e^{-ru} \\, du\n= \\lambda R \\, e^{(r+\\lambda)t}\n  \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n  \\mathbf{1}_{\\{T \\ge t\\}}\n\\]\nLa valeur totale de l’obligation est alors :\n\\[\n\\begin{aligned}\nB_t\n&= \\sum_{i=1}^{n}\n    c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}} \\\\\n&\\quad +\n\\left(\ne^{-(r+\\lambda)(T_n - t)}\n+\n\\lambda R \\, e^{(r+\\lambda)t}\n\\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\right)\n\\mathbf{1}_{\\{T_n \\ge t\\}}\n\\end{aligned}\n\\]\n\n\nImpementation de la valorisation d’un bond\n\n\nCode\n# Impementation de la fonctiuon de valorisation d'un bond\nimport numpy as np\n\ndef Bond(t,c,T,r,lamda, R = 0.4):\n    B = 0\n    for T_i in range(1,T+1):\n        B += np.exp(-(lamda + r)*(T_i - t))*(T_i&gt;=t)\n    B *= c\n    B += (np.exp(-(r + lamda)*(T_i-t)) + lamda * R  * (1-(np.exp(-(r + lamda)*(T_i -t)))) / (r + lamda))*(T_i&gt;=t)\n    return B\n\n\n\nExemple: Obligation au pair\n\nlamda = 0\n\n\nr = 0.02\n\n\nc = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nBond(t,c,T,r,lamda)\n\n\nCode\n# Exemple1: Obligation au pair\nlamda = 0\nr = 0.02\nc = 0.02\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.9981933497987289)\n\n\nAvec les paramètres ci-dessus considérés, on remarque que le prix de l’obligation est proche du nominal. En effet, le taux coupon est égal au taux de marché, ce qui indique que l’obligation est remunérée au taux du marché. Il s’agit donc d’une obligation au pair.\nAu cas où on aurait proposé une remunération supérieure à celle du marché, elle serait beaucoup plus attractive et sa valeur se serait appréciée. l’illustration est donnée ci dessous pour c= 0.03\n\n\nBond(0,0.03,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.03,10,0.02,0)\nprice\n\n\nnp.float64(1.0879246481591023)\n\n\nAvec une remunération inférieure à ce qu’aurait proposé le marché, on obtient une valorisation de l’obligation inférieure au nomimal\n\n\nBond(0,0.015,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.015,10,0.02,0)\nprice\n\n\nnp.float64(0.9533277006185421)\n\n\nEtant donné une intensité de défaut supérieure à zéro, on peut également calculer le coupon pour lequel l’obligation est au pair. On obtient, après calculs prsentés ci-dessous, un taux coupon de 2.6%.\n\n\nCode\n# Recherche du coupon  pour émettre une obligation au pair.\ndef dichot(t,T,r,lamda, P_MKT):\n    c_inf = 1e-8\n    c_sup = 1\n    epsi = 1e-8\n    c_moy = (c_inf + c_sup)/2\n    error = c_sup - c_inf\n\n    while error&gt;epsi:\n        p_hw = Bond(t,c_moy,T,r,lamda)\n        if p_hw &gt; P_MKT:\n            c_sup = c_moy\n        elif p_hw &lt; P_MKT:\n            c_inf = c_moy\n        c_moy = (c_inf + c_sup)/2\n        error = np.abs(c_sup - c_inf)\n\n    return c_moy\n\n\n\n\n\nTaux coupon pour émettre une obligation au pair\n\nlamda = 0.01\n\n\nr = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nCode\n# Taux coupon pour émettre une obligation au pair\nlamda = 0.01\nr = 0.02\nt = 0\nT = 10\ndichot(t,T,r,lamda, P_MKT = 1)\n\n\n0.026393926193952293\n\n\nEn maintenant une remunération égale à celle du marché , avec une intensité de défaut très grande (de l’ordre de 1000%), l’obligation tombe presque instantannément en défaut. Dans cette situation, la valeur du coupon vaut alors 39.92% qui est sensiblement proche du taux de recouvrement (40%).\n\n\n\nValeur du bond poiur une intensité de défaut à 1000%\n\n\nCode\n# Valeur du bond poiur une intensité de défaut à 1000%\nlamda = 10\nr = 0.02\nc = 0.03\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.39920293189432754)\n\n\n\n\n\nÉvolution du prix de l’obligation en fonction du temps\n\n\nCode\nimport matplotlib.pyplot as plt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB = np.array([Bond(t,c,T,r,lamda) for t in times])\nplt.plot(times,B)\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.title(\"Evolution du prix plein coupon du Bond\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nPlus on se rapproche de la date de détachement du coupon plus l’obligation devient attractive, elle prend donc de la valeur.\nChaque saut correspond à un détachement de coupons. une fois le coupon détaché de l’obligation, les flux à venir diminuent et la valeur de l’obligation se deprecie de la valeur du coupon qui a été détaché.\nCes sauts ne reflètent donc pas une dépréciation des bonds par le marché. Ce sont des sauts techniques. Raison pour laquelle on dit que le prix plein coupon est pollué par le coupon (en anglais Dirty price).\nOn va donc s’interesser par la suite au clean price ou pied de coupon qui correspond à au prix du bond moins le coupon couru.\n\\[\n\\tilde{B}_t = B_t - cc\n\\]\nOù (cc) est le coupon couru :\n\\[\ncc = c \\times (t - T^*)\n\\]\nEn retirant cette valeur de coupon couru, on supprime cet effet de saut après les detachements de coupons\n\nImplémentation du clean price\n\n\nCode\n# Implémentation du clean price\n\ndef CleanPrice(t,c,T,r,lamda, R = 0.4):\n    cc = c*(t - np.floor(t))*(t&lt;=T)\n    return Bond(t,c,T,r,lamda, R)-cc\n\n\n\n\nPieds coupon\n\n\nCode\n# Pieds coupon\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for t in times])\nB_pieds_coupon = np.array([CleanPrice(t,c,T,r,lamda) for t in times])\nplt.plot(times,B_pieds_coupon, label =\"Pieds coupon\")\nplt.plot(times,B_plein_coupon, label =\"Plein coupon\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nEn retirant cette valeur de coupon couru, on supprime l’effet de saut après les detachements de coupons, et on observe une courbe sans discontinuités\n\n\n\nEvolution du prix plein coupon et du clean price en fonction du temps pour differents coupons\n\n\nCode\nlamda = 0.01\nr = 0.02\nc = 0.01\nT = 10\n\ntimes = np.linspace(0,10, num = 100)\nB_plein_coupon1 = np.array([Bond(t,0.01,T,r,lamda) for t in times])\nB_pieds_coupon1 = np.array([CleanPrice(t,0.01,T,r,lamda) for t in times])\n\nB_plein_coupon5 = np.array([Bond(t,0.05,T,r,lamda) for t in times])\nB_pieds_coupon5 = np.array([CleanPrice(t,0.05,T,r,lamda) for t in times])\n\nplt.plot(times,B_pieds_coupon1, label =\"Pieds coupon 1%\")\nplt.plot(times,B_plein_coupon1, label =\"Plein coupon 1%\")\n\nplt.plot(times,B_pieds_coupon5, label =\"Pieds coupon 5%\")\nplt.plot(times,B_plein_coupon5, label =\"Plein coupon 5%\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nLes constats que l’on fait : - Lorsque c = 1% : La rémunération de l’obligation est inférieure à celle offerte par le marché. Ainsi, lors de son émission, sa valeur sera nécessairement inférieure à celle du pair, ce qui explique une valeur initiale proche de 87 %. À maturité, on reçoit 100 % du nominal, plus 1 % de ce dernier, correspondant au coupon. Le résultat final est donc sensiblement égal à 101 %.\n\nLorsque c = 5% : L’obligation rémunère plus que ce que le marché offre, ce qui la rend particulièrement attractive dès son émission. Ainsi, sa valeur de départ sera d’environ 120 %. Au fur et à mesure que les obligations sont détachées, les flux futurs diminuent, ce qui entraîne une dépréciation de sa valeur. À maturité, on reçoit 100 % du nominal, plus 5 % du montant nominal, correspondant au coupon. Le résultat final est donc de 105 %.\n\n\n\nEvolution du prix de l’obligation en fonction du taux d’intérêt\nDe l’expression analytique du prix du bond, on observe que le prix est décroissant du taux d’intérêt. La figure ci-dessous en donne une illustration, toute chose égale par ailleurs.\nOn vérifie graphiquement que le prix du bond est de 100% lorsque le taux d’intérêt est égal au taux sans risque \\(r^* = c - \\lambda (1 - R)\\).\n\nEvolution du prix du bond en fonction du taux d’intérêt\n\n\nCode\n# Evolution du prix du bond en fonction du taux d'intérêt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR = 0.4\ninterest = np.linspace(0,0.10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest,B_plein_coupon, label =\"Plein coupon\")\nplt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\nplt.xlabel(\"Taux d'intérêt (%)\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_17052\\2938139823.py:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\n\n\nText(0.5, 1.0, \"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotion de sensibilité\nLa sensibilité mesure la variation du prix du bond face à une variation d’un facteur de risque. On distingue entre autres la sensibilité de taux et la sensibilité de crédit.\nDans cette sectioin nous nous intéressons particulièrement à la sensibilité de taux. Elle est définie par la formule:\n\\[\n    Sensibilité = - \\frac{dB_t}{dr} \\frac{1}{B_t}\n\\]\nInterprétation: Lorsque le taux d’intérêt bouge de 1%, alors le prix du bond bouge de -sensibilité %.\nCette sensibilité peut également être vue comme le barycentre des différentes échéances pondérées par les flux actualisés. C’est la duration.\n\\[\n    \\frac{\\sum T_i\\times F_i}{\\sum F_i}\n\\]\nExemple: En considérant une obligation de maturité 3 ans payant des coupons annuels, avec intensité de défaut nul. Alors son prix et la sensibilité de taux sont données par:\n\\[\n\\begin{aligned}\nP &= c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial P}{\\partial r}\n&= -1c \\, e^{-1r} - 2c \\, e^{-2r} - 3c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Sensibilité}\n&= \\frac{1c \\, e^{-1r} + 2c \\, e^{-2r} + 3c \\, e^{-3r}}\n        {c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}}\n\\end{aligned}\n\\]\nD’où l’expression barycentrique des échéances pondérées par les flux.\n\n\nCode\n# Sensibilité de taux du prix du bond\ndelta_r = 1e-4 #1bp\n\n\ndef Sensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    return -(Bond(t,c,T,r+delta_r,lamda, R)-B_t)/(delta_r*B_t)\n\n\nOn considère un bond dont les caractérisiques sont les suivqantes:\n\\[\n    \\begin{cases}\n        \\lambda = 0.01\\\\\n        r = 0.02\\\\\n        c = 0.03\\\\\n        T = 10\\\\\n        R = 0.4\\\\\n    \\end{cases}\n\\]\nLa sensiblité de ce bond est de 8.64. Ainsi, lorsque le taux d’intérêt augmente de 1 point de pourcentage, le prix du bond diminue de 8.64%.\n\n\nCode\n# Sensibilité de taux d'intérêt\nSensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4)\n\n\nnp.float64(8.643982489105056)\n\n\n\nEvolution de la sensibilité de taux fonction de la maturité\n\n\nCode\n# Evolution de la sensibilité au taux d'intérêt en fonction de la maturité\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,c,T,r,lamda) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité')\n\n\n\n\n\n\n\n\n\n\nOn constate que la sensibilité de taux croît avec la maturité et tend à être linéaire.\nPour des paramètres extrêmes \\(c = \\lambda = r = 0\\), la sensibilité (duration) est identique à la maturité comme illustré sur la figure ci-dessous. Cette remarque met en évidence la relation mlathématique entre la sensibilité et la maturité présentée plus haut.\nConnaissant la maturité, pour une variation du taux d’intérêt on peut donc donner une estimation “grossière” de la sensibilité (en approximant la duration par 0.8*maturité, par exemple).\n\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,1e-6,T,1e-6,1e-6) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité (cas extrême)\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité (cas extrême)')\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR d’une obligation.\nL’estimation de la VaR sur un bond peut se faire à partir de la senssibilité de taux ou par la méthode de repricing.\n\n\nEstimation de la VaR par la sensibilité du taux d’intérêt\nL’approche par la sensibilité se présente de la manière suivante:\nOn suppose que la dynamique du taux d’intérêt est donnée par \\[\\Delta r \\sim \\mathcal{N}(0, \\sigma \\sqrt{\\Delta t})\\].\nSachant que \\[\\frac{\\Delta P}{P} = - Duration \\times \\Delta r\\]\nil suit que \\[\\frac{\\Delta P}{P} \\sim \\mathcal{N}(0, Duration \\times \\sigma \\sqrt{\\Delta t})\\]\nUne approche par la sensibilité de la VaR à 99% donne \\[VaR = Duration \\times \\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\\] Où \\(z_{99\\%}\\) est le quantile d’ordre 99% de la loi normale standard.\nPour les besoins de notre exercice, on pose \\(\\sigma = 1\\%\\).\n\n\nCode\nfrom scipy.stats import norm\n\n# Calcul de la VaR par la sensibilité du taux d'intérêt\ndef Sensi_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    duration = Sensi(t,c,T,r,lamda, R)\n    var = duration*sigma*np.sqrt(H)*norm.ppf(alpha)\n    return var\n\n\n\n\nCode\nsigma= 0.01\nH = 1/12\nt = 0\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR =0.40\n\nSensi_VaR(sigma , H, t,c,T,r,lamda)\n\n\nnp.float64(0.05804942383590022)\n\n\nLa VaR à 99% d’horizon 1 an sur le bond est de 5.8%. - On note que la VaR est une fonction linéaire croissante de la volatilité. C’est une conséquence directe de l’hypothèse de normalité des variations du taux d’intérêt. Ainsi, une augmentation de la volatilité du taux d’intérêt s’accompagne d’une augmentation de la volatilité du prix du bond. - La VaR est une fonction décroissante du taux d’intérêt. En effet, une augmentation du taux d’intérêt entraîne une diminution de la valeur des bonds, et par conséquent des valeurs extrêmes atteintes par celles-ci; d’où la VaR décroît. Toutefois l’évolution de la VaR en fonction du taux d’intérêt n’est pas linéaire.\n\n\nEvolution de la Z-VaR en fonction du taux d’intérêt\n\n\nCode\n# Evolution de la VaR en fonction de la volatilité\nvol = np.linspace(0,1e-2, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for sigma in vol])\n\nplt.plot(vol, var, label =\"VaR_99%\")\nplt.xlabel(\"Volatilité\")\nplt.ylabel(\"VaR \")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de la volatilité\")\n\n\nText(0.5, 1.0, 'VaR à 99% à horizon 1 an en fonction de la volatilité')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Evolution de la VaR en fonction du taux d'intérêt\ninterest = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nEvolution de la Z-VaR en fonction de l’intensité de défaut\nPour des intensités de défaut croissantes, la VaR du bond décroît de façon exponentielle, ceteris paribus. Tout comme avec le taux d’intérêt, une augmentation de l’intensité de défaut s’accompagne d’une imminence du défaut et par conséquent de la diminution de la valeur du bond. D’où une diminution de la VaR.\n\n\nCode\n# Evolution de la VaR en fonction de l'intensité de défaut\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de défaut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR par repricing de l’obligation\nLa méthodologie consiste à revaloriser le bond pour une variation du taux d’intérêt, puis d’en déduire la variation du prix du bond qui en résulte.\n\\[\n    \\Delta r^* =\\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\n\\]\n\\[\n    VaR_{99\\%} = \\frac{B(r+ \\Delta r^*) - B(r)}{B(r)}\n\\]\n\n\nApproche repricing\n\n\nCode\n# Approche repricing\n\ndef repricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    delta_r = sigma*np.sqrt(H)*norm.ppf(alpha)\n    B_rep = Bond(t,c,T,r+delta_r,lamda, R)\n    B_r = Bond(t,c,T,r,lamda, R)\n    \n    return  (B_r - B_rep)/B_r\n\n\n\n\nCode\nrepricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99)\n\n\nnp.float64(0.05627224378244837)\n\n\n\nLa VaR obtenue par l’approche repricing, sous les mêmes conditions que précédemment, est de 5.6%. Cette VaR est inférieure à celle obtenue sous l’hypothèse de normalité des variations du taux d’intérêt.\nCeci s’explique par le fait que la duration est une fonction convexe décroissante du taux d’intérêt et représente une approximation affine de la valeur du bond en le taux d’intérêt. De manière générale, l’approche par les sensibilités surestime la VaR.\nL’évolution de la VaR, obtenue par reprincing, en fonction du taux d’intérêt ou de l’intensité de défaut (voir les graphiques ci-dessus) à la même allure que la z-VaR.\nEn règle générale la VaR doit être inférieure au seuil de 20% de par la limite règlementaire. L’utilisation de la z-VaR peut donc constituer un manque à gagner pour les institutions financieres. En effet, elles peuvent être amenées à dérisquer leur portefeuille en limitant leur investissements pour des raisons purement techniques.\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\ninterest = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda)for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de saut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")"
  },
  {
    "objectID": "risques/projets/liquidity.html#iii.-risque-de-contrepartie",
    "href": "risques/projets/liquidity.html#iii.-risque-de-contrepartie",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "III. Risque de contrepartie",
    "text": "III. Risque de contrepartie\nDans le cadre d’un engagement contratuel incluant deux parties prenantes, le risque de contrepartie fait reférence au rique de perte suite au défaut d’une des parties à remplir les engagements contractuels pré-établis: c’est un risque bilatéral.\nAfin de se couvrir du defaut de l’émetteur d’un bond, l’acheteur du bond peut entrer dans un credit default swap (CDS). Dans un tel contrat, lorsque la contrepartie est correlée à l’émetteur de l’obligation on parle de wrong way risk ce qui a pour conséquence d’exposer davantage le souscripteur. La contrepartie doit donc être décorrelée de l’émetteur.\nLe risque de contrepartie est en général mitigé par les appels de marge (chambre de compensation). - Quand la qualité de l’émetteur se dégrade, son spread de crédit spread croît et le CDS s’apprécie. - Si la contrepartie fait défaut, le détenteur perd \\(CDS = 1-R\\). - Un mécanisme d’appel de marge permet de mitiger le risque.\n\nLien entre CDS et obligation: la formule du triangle de crédit\nUn estimation du taux sans risque d’une obligation: \\[\n\\begin{aligned}\nP &= \\sum_{i=1}^{n} c \\, e^{-(r+\\lambda)T_i}\n    + e^{-(r+\\lambda)T_n}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\int_0^T e^{-(r+\\lambda)t} \\, dt\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\nL’obligation est au pair si et seulement si P = 1 Ainsi, \\[\n    (c + \\lambda R) \\frac{ 1- e^{-(r + \\lambda )T}}{r + \\lambda} + e^{-(r + \\lambda)T} = 1\n\\]\nQuand T tend vers l’infini le taux sans risque vaut alors, \\[\n    (c + \\lambda R)  = r + \\lambda\n\\]\n\\[\n    (c - r )  = \\lambda (1 - R)\n\\] - \\(c-r\\): spread ou prime de risque - \\(\\lambda\\): intensité de défaut - \\(1-R\\): LGD \\[\n    Spread = PD \\times LGD\n\\]\n\n\nCDS: principe\n\nPremium Leg: est payé tant que l’émetteur est “en vie”. \\[ PL = \\int_0^T s \\times e^{-rt} \\times e^{-\\lambda t}dt = s \\frac{1-e^{-(r+\\lambda)T}}{r + \\lambda}\\]\nDefault Leg: compensation à la date t si le défaut y survient. \\[DL = \\int_0^T(1-R) e^{-rt}\\times e^{-\\lambda t} dt = \\lambda (1-R) \\frac{1-e^{-(r+\\lambda)T}}{r+\\lambda}\\].\n\nLa valeur initiale d’un spread étant nulle, les deux jambes sont égales et il on obtient:\n\\[\n    s = \\lambda (1-R)\n\\]\nUn estimation de ce spread peut donc être déduite de la formule du triangle de crédit.\n\n\nSensibilité de crédit\nLa sensibilité de crédit d’une obligation est la variation du prix de l’obligation lorsque varie le spread de crédit de la contrepartie. De la formule du triangle, la sensibilité de crédit peut être déduite de la sensibilité par rapport à l’intensité de défaut \\(\\lambda\\).\n\\[\n    \\frac{\\partial P}{\\partial s} = \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda}\n\\]\n\\[\n    \\frac{\\partial P}{\\partial s} \\times \\frac{1}{P}= \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda} \\times  \\frac{1}{P}\n\\]\nOn peut calculer une VaR taux et une VaR crédit. Toutefois, la VaR qui sera regardée de près est celle issue de la variation conjointe des taux et des spreads de crédit.\n\nSensibilité de crédit du bond\n\n\nCode\n# Sensibilité de crédit du bond\ndelta_lambda = 1e-4 #1bp\n\ndef Sensi_credit(t,c,T,r,lamda, R = 0.4, delta_lambda = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    variation = -(Bond(t,c,T,r,lamda+delta_lambda, R)-B_t)/(delta_r*B_t)\n    return variation/(1-R)\n\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda)\n\n\nnp.float64(8.82119086802735)\n\n\n\nUne variation d’un point de pourcentage du spread de crédit entraîne une variation de 8.82% de la valeur du bond. Cette sensibilité de crédit est assez proche de la sensibilité taux. En effet, on observe de l’écriture mathématique de la valeur du bond que le taux d’intérêt et l’intensité apparaissent conjointement de façon additive (sauf pour le terme de recouvrement).\nCes deux sensibilitées sont identiques pour un taux de recouvrement nul.\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda, R = 0.0, delta_lambda = 1e-4)\n\n\nnp.float64(8.77911216566412)\n\n\n\n\n\nEstimation de la VaR par variation conjointe du taux d’intérêt et du spread de crédit\nOn suppose que la dynamique du taux d’intérêt est donnée par l’EDS\n\\[\n    dr_t = \\sigma dW_t\n\\]\nLe spread ne prenant pas de valeur négative, on supposera qu’il est log-normal. Sa dynamique est donnée par l’EDS\n\\[\n    \\frac{ds_t}{s_t} = \\alpha dZ_t\n\\]\nEn supposant \\(dW_t dZ_t = \\rho\\) et \\(Z_t = \\rho W_t + \\sqrt{1-\\rho^2}V_t\\) tel que \\(dV_t dW_t = 0\\),\nLa dynamique du spread s’écrit alors \\[\n\\frac{ds_t}{s_t} = \\alpha (\\rho W_t + \\sqrt{1-\\rho^2}V_t)\n\\]\nEn appliquant un schéma de discrétisation d’Euler, on peut alors simuler les taux \\(r_t\\) et les spreads \\(s_t\\) par:\n\\[\n    r_H = r + \\sigma \\sqrt{H} W_H\n\\]\n\\[\n    s_H = s(1 + \\alpha \\rho \\sqrt{H} W_H + \\alpha \\sqrt{1-\\rho^2} \\sqrt{H} V_H)\n\\]\nOù H est le pas de discrétisation.\nLa méthodologie de détermination de la sensibilité du prix d’une obligation conjointement au taux d’intérêt et au spread de crédit est la suivante: - Calculer la valeur du bond pour les paramètres initiaux - Simuler les taux \\(r_H\\) et les spreads \\(s_H\\) - Pour chaque simulation, calculer la valeur du bond (cetertis paribus) - pour déterminer la variation des nouveaux prix par rapport au prix initial - calculer le quantile d’ordre 99% des variations obtenues\n\n\nCode\n# Sensibilité conjointe de taux et de crédit\n\ndef SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12, M=10_000):\n    \n    u1 = np.random.uniform(0,1, size = M)\n    u2 = np.random.uniform(0,1, size = M)\n    w = norm.ppf(u1)\n    v = norm.ppf(u2)\n    s = lamda*(1-R)\n    \n    r_H = r + sigma*np.sqrt(H)*w\n    s_H = s*(1 + alpha*rho*np.sqrt(H)*w + alpha*np.sqrt(1-rho**2)*np.sqrt(H)*v)\n    \n    P_0 = Bond(0,c,T,r,lamda, R)\n    P_1 = Bond(H,c,T,r_H,s_H/(1-R), R)\n    mu = (P_1 - P_0)/P_0\n    return  - np.quantile(mu,0.01)\n\n\n\n\nCode\nsigma = 0.01 # Volatilité du taux d'intérêt\nalpha =0.4   # Volatilité du spread\nrho = 0.4    # Corrélation entre le spread et le taux d'intérêt\n\nnp.random.seed(90) # Pour la reproductibilité\nSimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12)\n\n\nnp.float64(0.0602691861203251)\n\n\n\n\nCode\nnp.random.seed(90)\n\nrhos = np.linspace(0,1, num= 1000)\nvar = np.array([SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12) for rho in rhos])\n\nplt.plot(rhos, var)\n\n\n\n\n\n\n\n\n\nLa corrélation \\(\\rho\\) doit être positive, car une augmentation des taux d’intérêt entraîne une meilleure rémunération du marché par rapport aux obligations. Pour rester attractifs, les coupons doivent alors augmenter. Cette hausse des coupons accroît le coût de financement de l’émetteur, ce qui conduit à une augmentation du spread de crédit. Il en résulte une corrélation positive entre les taux d’intérêt et les spreads de crédit.\nUne augmentation conjointe des taux et des spreads accroît le risque de défaut, en raison de la corrélation positive entre ces deux facteurs de risque. Ainsi, une corrélation élevée reflète un manque de diversification, ce qui expose davantage le portefeuille au risque systémique"
  },
  {
    "objectID": "risques/projets/liquidity.html#iv.-risque-modèle",
    "href": "risques/projets/liquidity.html#iv.-risque-modèle",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "IV. Risque modèle",
    "text": "IV. Risque modèle\nLe risque modèle désigne le risque associé à l’utilisation d’un modèle mathématique ou statistique qui ne représente pas correctement la réalité ou qui mène à des décisions erronées en finance et en gestion des risques.\nEléments du suivi du risque modèle: - Sanity check: Ecart de performance - Backtesting: Application aux données antérieures - Comparaison avec desmodèles plus riches - Provisionner au titre du risque de modèle"
  },
  {
    "objectID": "risques/projets/liquidity.html#v.risque-climatique",
    "href": "risques/projets/liquidity.html#v.risque-climatique",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "V.Risque climatique",
    "text": "V.Risque climatique\nLe risque climatique désigne les risques financiers liés au changement climatique et aux politiques de transition vers une économie bas carbone. Sa modélisation reste complexe, notamment en raison du manque de données historiques et de l’absence d’un cadre réglementaire clairement défini.\nOn distingue deux principaux types de risques climatiques :\n\nRisque physique: l correspond aux pertes financières causées par des événements climatiques extrêmes. Il est défini par deux éléments clés : la fréquence et la sévérité des événements.\n\nRisque aigu : Événements rares mais très intenses ( ouragans, inondations).\nRisque chronique : Changements progressifs et durables (élévation du niveau de la mer).\n\nRisque de transition: il découle de l’adaptation des entreprises et des acteurs économiques aux exigences de transition écologique.\n\nRisque politique et réglementaire : Durcissement progressif des régulations environnementales (taxe carbone).\nOpportunités technologiques : Innovations favorisant la transition énergétique et redéfinissant les modèles économiques.\n\n\nLa resilience des institutions financieres notamment aux changement climatique est évaluée via des stress tests climatiques dont les scénarios sont issus des données du NGFS (Network for Greening financial System)."
  }
]