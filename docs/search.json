[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Marilene KOUGOUM",
    "section": "",
    "text": "Je suis Marilene KOUGOUM, récemment diplômée de l’ENSAI Rennes en tant qu’ingénieure en data science et gestion des risques. Cette formation m’a permis d’acquérir une base solide en modélisation des risques financiers - notamment les risques de marché et de crédit - ainsi qu’une bonne compréhension des cadres comptables et réglementaires structurant de secteur bancaire (Bâle III, IFRS 9, etc.).\nJe poursuis actuellement mes études à l’Université Paris Cité au sein du master M2MO, spécialisé en modélisation aléatoire, finance quantitative et data science.\nJ’ai créé ce site pour partager ce que j’apprends au fil de mon parcours : d’une part, pour aider mes cadets académiques à avoir un aperçu des notions enseignées en école d’ingénieur et en master quantitatif ; d’autre part, afin de vulgariser des concepts parfois techniques en finance quantitative, en data science ou en gestion des risques.\nLes contenus présents sur ce site proviennent de mes travaux académiques, de projets personnels ou d’enseignements réalisés sous la supervision de professeurs et de professionnels experts dans leurs domaines. Je ne prétends pas que ces contenus soient exhaustifs : ils reflètent mon apprentissage continu et ma volonté de partager.\nMerci de votre visite, et bonne exploration !"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Apprendre, comprendre, partager.",
    "section": "",
    "text": "Modèles stochastiques, pricing, produits dérivés.\n\n→ Explorer\n\n\n\n\nRisque de marché, risque de crédit, conformité bancaire.\n\n→ Explorer\n\n\n\n\nModèles prédictifs, séries temporelles, applications financières.\n\n→ Explorer\n\n\n\n\n\n\n\n\n\n\n\nSimulation stochastique, variance reduction, convergence.\n\nLire\n\n\n\n\n\nPD, LGD, IFRS 9 et modèles statistiques appliqués.\n\nLire\n\n\n\n\n\nARIMA, Prophet, LSTM : comparaison et analyse.\n\nLire"
  },
  {
    "objectID": "index.html#les-grands-axes-du-site",
    "href": "index.html#les-grands-axes-du-site",
    "title": " ",
    "section": "Les grands axes du site",
    "text": "Les grands axes du site\n\n\n\nFinance quantitative\n\n\nCalibration de processus stochastiques, Modèles de courbe de taux, Pricing…\n\n→ Explorer\n\n\n\nRisque & Régulation\n\n\nRisque de marché, risque de crédit, conformité bancaire…\n\n→ Explorer\n\n\n\nModélisation stat\n\n\nAnalyse des séries financières, Théorie des valeurs extrêmes, Modèles de durée…\n\n→ Explorer"
  },
  {
    "objectID": "index.html#derniers-projets",
    "href": "index.html#derniers-projets",
    "title": " ",
    "section": "Derniers projets",
    "text": "Derniers projets\n\n\n\n\nPricing d’options via Monte-Carlo\n\n\nSimulation stochastique, variance reduction, convergence.\n\nLire\n\n\n\n\nModélisation du risque de crédit\n\n\nPD, LGD, IFRS 9 et modèles statistiques appliqués.\n\nLire\n\n\n\n\nPrévision de séries temporelles\n\n\nARIMA, Prophet, LSTM : comparaison et analyse.\n\nLire"
  },
  {
    "objectID": "index.html#publications-récentes",
    "href": "index.html#publications-récentes",
    "title": " ",
    "section": "Publications récentes",
    "text": "Publications récentes\n\n\n\n\nCourbe de taux & modèle de Hull–White\n\n\nBootstrapping, valorisation caplets/swap­tion, calibration Hull–White\n\nLire\n\n\n\n\nVECM & Pairs Trading\n\n\nCointégration,ruptures structurelles, extensions (GARCH, régimes), usages en stratégie pairs trading\n\nLire\n\n\n\n\nGestion de la liquidité des actifs\n\n\nProfondeur de marché, Calcul du profil d’écoulement,liquidation avec/sans déformation.\n\nLire"
  },
  {
    "objectID": "risque-regulation.html",
    "href": "risque-regulation.html",
    "title": "Risque & Régulation",
    "section": "",
    "text": "Dans cette première partie, nous présentons les fondements théoriques de la gestion des risques bancaires.\nElle rassemble les concepts, définitions et structures méthodologiques nécessaires pour comprendre le fonctionnement global du risk management dans les institutions financières."
  },
  {
    "objectID": "risque-regulation.html#la-gestion-des-risques-une-fonction-stratégique",
    "href": "risque-regulation.html#la-gestion-des-risques-une-fonction-stratégique",
    "title": "Risque & Régulation",
    "section": "La Gestion des Risques : Une Fonction Stratégique",
    "text": "La Gestion des Risques : Une Fonction Stratégique\nAu cœur du système économique, les banques assurent une mission essentielle de circulation des capitaux entre épargnants et investisseurs. Cette position centrale les expose naturellement à divers risques financiers qu’elles doivent apprendre à maîtriser.\nLa gestion des risques a considérablement évolué : d’une fonction traditionnelle de contrôle, elle est devenue un véritable levier de création de valeur. Elle permet aujourd’hui une allocation optimale du capital et une tarification plus fine des produits financiers, tout en guidant les décisions stratégiques.\nLe risk manager moderne endosse un rôle crucial à l’interface des métiers. Véritable vigie, il anticipe les scénarios défavorables, conseille les équipes opérationnelles et éclaire la direction générale sur l’exposition aux risques."
  },
  {
    "objectID": "risque-regulation.html#panorama-des-risques-bancaires",
    "href": "risque-regulation.html#panorama-des-risques-bancaires",
    "title": "Risque & Régulation",
    "section": "Panorama des Risques Bancaires",
    "text": "Panorama des Risques Bancaires\nPour appréhender correctement la gestion des risques, il est fondamental d’en connaître les différentes facettes. Quatre catégories principales structurent le paysage risk management.\n\nLe risque de marché\nLe risque de marché apparaît lorsque la valeur des positions détenues par une banque évolue sous l’effet des mouvements des marchés financiers. Ces mouvements concernent notamment :\n\nles taux d’intérêt,\n\nles taux de change,\n\nles cours des actions,\n\nles matières premières,\n\net, plus globalement, l’ensemble des conditions économiques et financières.\n\nCe risque ne se résume pas à une liste de menaces distinctes : il traduit la manière dont un portefeuille réagit aux évolutions de son environnement. Certains instruments sont sensibles à de petites variations, tandis que d’autres réagissent davantage à des mouvements plus marqués ou à des périodes de forte instabilité, où les prix peuvent changer rapidement et où les relations entre actifs deviennent moins prévisibles.\n\n\nLe risque de crédit\nLe risque de crédit, le plus significatif pour une banque, correspond à la probabilité qu’une contrepartie ne honore pas ses engagements financiers.\n\n\nLe risque de liquidité\nIl se manifeste sous deux angles : - liquidité de marché : capacité à céder des actifs sans perte de valeur significative ; - liquidité de financement : aptitude à se refinancer à un coût raisonnable.\n\n\nLe risque opérationnel\nIl englobe les pertes potentielles liées à des défaillances internes (processus, personnes, systèmes) ou à des événements externes, incluant le risque juridique.\n\n\nAutres risques\nParmi les autres risques notables : - le risque de réputation,\n- le risque de modèle,\n- le risque systémique.\n\nCes risques sont profondément interconnectés. Une approche en silos pourrait conduire à sous-estimer des menaces transversales aux conséquences potentiellement graves."
  },
  {
    "objectID": "risque-regulation.html#la-théorie-des-mesures-de-risque",
    "href": "risque-regulation.html#la-théorie-des-mesures-de-risque",
    "title": "Risque & Régulation",
    "section": "La Théorie des Mesures de Risque",
    "text": "La Théorie des Mesures de Risque\nLa quantification rigoureuse du risque s’appuie sur des outils mathématiques sophistiqués. Une mesure de risque, notée ( (L) ) où ( L ) représente la perte aléatoire, détermine le capital économique nécessaire à sa couverture.\n\nMesures de risque cohérentes\nArtzner et ses co-auteurs ont défini quatre axiomes qui caractérisent une mesure de risque robuste :\n\nInvariance par translation\n[ (L + c) = (L) + c ]\nHomogénéité positive\n[ (L) = (L) ]\nMonotonie\n[ L_1 L_2 (L_1) (L_2) ]\nSous-additivité\n[ (L_1 + L_2) (L_1) + (L_2) ]\n\nCes propriétés garantissent que la mesure du risque se comporte de manière intuitive et économiquement rationnelle."
  },
  {
    "objectID": "risque-regulation.html#var-et-expected-shortfall",
    "href": "risque-regulation.html#var-et-expected-shortfall",
    "title": "Risque & Régulation",
    "section": "VaR et Expected Shortfall",
    "text": "VaR et Expected Shortfall\n\nValue-at-Risk (VaR)\nLa Value-at-Risk représente la perte maximale qui ne sera dépassée qu’avec une probabilité donnée :\n[ _(L) = { x : (L x) }. ]\nBien que simple et intuitive, elle souffre de deux limites majeures : - elle n’est pas cohérente,\n- elle ne renseigne pas sur la gravité des pertes extrêmes.\n\n\nExpected Shortfall (ES)\nL’Expected Shortfall correspond à la perte moyenne dans les pires scénarios :\n[ _(L) = [, L L _(L),]. ]\nSa cohérence et sa meilleure capture des risques extrêmes en font désormais la mesure privilégiée dans les réglementations récentes."
  },
  {
    "objectID": "risque-regulation.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "href": "risque-regulation.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "title": "Risque & Régulation",
    "section": "Le Cadre Réglementaire : de Bâle I à Bâle IV",
    "text": "Le Cadre Réglementaire : de Bâle I à Bâle IV\nBâle I (1988) : premier standard international avec le ratio Cooke (capital/RWA ≥ 8%).\nBâle II (2004) : approche en trois piliers, reconnaissance des modèles internes.\nBâle III (2010/2011) : renforcement de la qualité du capital, coussins, ratios LCR et NSFR.\nBâle IV (2017) : remplacement de la VaR par l’ES, nouvelle approche opérationnelle, restrictions sur modèles internes.\nCette évolution réglementaire témoigne de la recherche permanente d’un équilibre entre innovation financière et stabilité du système."
  },
  {
    "objectID": "risques/chapter_one.html",
    "href": "risques/chapter_one.html",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique",
    "href": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "La Gestion des Risques : une Fonction Stratégique",
    "text": "La Gestion des Risques : une Fonction Stratégique\nAu cœur du système économique, les banques jouent un rôle fondamental d’intermédiation en orientant les capitaux entre agents en excédent et agents en besoin de financement. Cette position centrale les expose naturellement à une variété de risques financiers qu’il est essentiel de comprendre et de maîtriser.\nAvec le temps, la fonction de gestion des risques s’est profondément transformée. Jadis perçue comme un simple dispositif de contrôle, elle constitue aujourd’hui un levier stratégique, permettant :\n\nune allocation optimale du capital,\nune tarification plus fine des produits,\nune meilleure anticipation des chocs,\nun pilotage éclairé des décisions stratégiques."
  },
  {
    "objectID": "risques/chapter_one.html#panorama-des-risques-bancaires",
    "href": "risques/chapter_one.html#panorama-des-risques-bancaires",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Panorama des Risques Bancaires",
    "text": "Panorama des Risques Bancaires\nPour appréhender correctement la gestion des risques, il est fondamental d’en connaître les différentes facettes. Quatre catégories principales structurent le paysage du risk management.\n\nLe risque de marché\nLe risque de marché apparaît lorsque la valeur des positions détenues par une banque évolue sous l’effet des mouvements des marchés financiers. Ces mouvements concernent notamment :\n\nles taux d’intérêt,\n\nles taux de change,\n\nles cours des actions,\n\nles matières premières,\n\net, plus globalement, l’ensemble des conditions économiques et financières.\n\nCe risque ne se résume pas à une liste de menaces distinctes : il traduit la manière dont un portefeuille réagit aux évolutions de son environnement. Certains instruments sont sensibles à de petites variations, tandis que d’autres réagissent davantage à des mouvements plus marqués ou à des périodes de forte instabilité, où les prix peuvent changer rapidement et où les relations entre actifs deviennent moins prévisibles.\n\n\nLe risque de crédit\nLe risque de crédit, le plus significatif pour une banque, correspond à la probabilité qu’une contrepartie ne honore pas ses engagements financiers.\n\n\nLe risque de liquidité\nIl se manifeste sous deux angles : - liquidité de marché : capacité à céder des actifs sans perte de valeur significative ; - liquidité de financement : aptitude à se refinancer à un coût raisonnable.\n\n\nLe risque opérationnel\nIl englobe les pertes potentielles liées à des défaillances internes (processus, personnes, systèmes) ou à des événements externes, incluant le risque juridique.\n\n\nAutres risques\nParmi les autres risques notables : - le risque de réputation,\n- le risque de modèle,\n- le risque systémique.\n\nCes risques sont profondément interconnectés. Une approche en silos pourrait conduire à sous-estimer des menaces transversales aux conséquences potentiellement graves."
  },
  {
    "objectID": "risques/chapter_one.html#la-théorie-des-mesures-de-risque",
    "href": "risques/chapter_one.html#la-théorie-des-mesures-de-risque",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "La Théorie des Mesures de Risque",
    "text": "La Théorie des Mesures de Risque\nLa quantification rigoureuse du risque s’appuie sur des outils mathématiques sophistiqués. Une mesure de risque, notée ( (L) ) où ( L ) représente une perte aléatoire, détermine le capital économique nécessaire à sa couverture.\n\nMesures de risque cohérentes\nArtzner et ses co-auteurs ont défini quatre axiomes qui caractérisent une mesure de risque robuste :\n\nInvariance par translation\n\\[\n\\mathcal{R}(L + c) = \\mathcal{R}(L) + c\n\\]\nHomogénéité positive\n\\[\n\\mathcal{R}(\\lambda L) = \\lambda \\mathcal{R}(L)\n\\]\nMonotonie\n\\[\nL_1 \\le L_2 \\;\\Rightarrow\\; \\mathcal{R}(L_1) \\le \\mathcal{R}(L_2)\n\\]\nSous-additivité\n\\[\n\\mathcal{R}(L_1 + L_2) \\le \\mathcal{R}(L_1) + \\mathcal{R}(L_2)\n\\]\n\nCes propriétés garantissent que la mesure du risque se comporte de manière intuitive et économiquement rationnelle.\nDans ce contexte, la Value-at-Risk (VaR) s’est imposée comme un outil largement utilisé en pratique et dans les réglementations prudentielles."
  },
  {
    "objectID": "risques/chapter_one.html#var-et-expected-shortfall",
    "href": "risques/chapter_one.html#var-et-expected-shortfall",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "VaR et Expected Shortfall",
    "text": "VaR et Expected Shortfall\n\nValue-at-Risk (VaR)\nLa VaR à un niveau de confiance \\(\\alpha\\) et sur un horizon donné représente la perte maximale qui ne sera dépassée qu’avec une probabilité \\(1 - \\alpha\\) :\n\\[\n\\text{VaR}_{\\alpha}(L)\n= \\inf \\{\\, x : \\mathbb{P}(L \\le x) \\ge \\alpha \\,\\}.\n\\]\nBien qu’intuitive et largement adoptée, elle présente deux limites importantes :\n\nelle peut violer la sous-additivité (donc n’est pas cohérente),\nelle ne dit rien de l’ampleur des pertes extrêmes situées au-delà du quantile.\n\nCes limitations ont conduit à privilégier une mesure plus robuste : l’Expected Shortfall.\n\n\n\nExpected Shortfall (ES)\nL’Expected Shortfall correspond à la perte moyenne observée dans les pires scénarios, c’est-à-dire au-delà de la VaR :\n\\[\n\\text{ES}_{\\alpha}(L)\n= \\mathbb{E}\\big[\\, L \\mid L \\ge \\text{VaR}_{\\alpha}(L) \\,\\big].\n\\]\nContrairement à la VaR, l’ES :\n\ntient compte de la sévérité des pertes extrêmes,\n\nrespecte les quatre axiomes de cohérence,\n\noffre une meilleure mesure du risque en conditions de stress.\n\nC’est pour ces raisons qu’elle a été adoptée dans les cadres réglementaires récents."
  },
  {
    "objectID": "risques/chapter_one.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "href": "risques/chapter_one.html#le-cadre-réglementaire-de-bâle-i-à-bâle-iv",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Le Cadre Réglementaire : de Bâle I à Bâle IV",
    "text": "Le Cadre Réglementaire : de Bâle I à Bâle IV\nL’évolution de la régulation prudentielle suit l’objectif de renforcer la résilience du système bancaire face aux chocs.\n\nBâle I (1988) : introduction du ratio Cooke (capital/RWA ≥ 8%).\n\nBâle II (2004) : architecture en trois piliers, reconnaissance des modèles internes.\n\nBâle III (2010–2011) : amélioration de la qualité du capital, coussins de conservation, ratios LCR et NSFR.\n\nBâle IV (2017) : remplacement de la VaR par l’ES pour le risque de marché, refonte de l’approche opérationnelle, encadrement accru des modèles internes."
  },
  {
    "objectID": "risques/index.html",
    "href": "risques/index.html",
    "title": "Risque & Régulation",
    "section": "",
    "text": "Dans cette première partie, nous présentons les bases de la gestion des risques bancaires. Elle rassemble les concepts, définitions et éléments méthodologiques de base.\nCette synthèse n’a pas vocation à être exhaustive : elle propose un cadre de référence minimal destiné à faciliter la compréhension des analyses, modèles et travaux présentés dans les sections suivantes."
  },
  {
    "objectID": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique-1",
    "href": "risques/chapter_one.html#la-gestion-des-risques-une-fonction-stratégique-1",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "La Gestion des Risques : Une Fonction Stratégique",
    "text": "La Gestion des Risques : Une Fonction Stratégique\nAu cœur du système économique, les banques assurent une mission essentielle de circulation des capitaux entre épargnants et investisseurs. Cette position centrale les expose naturellement à divers risques financiers qu’elles doivent apprendre à maîtriser.\nLa gestion des risques a considérablement évolué : d’une fonction traditionnelle de contrôle, elle est devenue un véritable levier de création de valeur. Elle permet aujourd’hui une allocation optimale du capital et une tarification plus fine des produits financiers, tout en guidant les décisions stratégiques.\nLe risk manager moderne endosse un rôle crucial à l’interface des métiers. Véritable vigie, il anticipe les scénarios défavorables, conseille les équipes opérationnelles et éclaire la direction générale sur l’exposition aux risques."
  },
  {
    "objectID": "risques/chapter_two.html",
    "href": "risques/chapter_two.html",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "risques/chapter_two.html#comprendre-le-risque-de-marché",
    "href": "risques/chapter_two.html#comprendre-le-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Comprendre le Risque de Marché",
    "text": "Comprendre le Risque de Marché\nLe risque de marché correspond à la possibilité de subir une perte liée à l’évolution défavorable des prix sur les marchés financiers. Contrairement au risque de crédit, qui concerne la solidité financière d’une contrepartie, le risque de marché affecte directement la valeur des actifs détenus dans le portefeuille de négociation d’une banque.\nIl découle de mouvements de facteurs tels que les taux d’intérêt, les taux de change, les cours des actions, les spreads de crédit ou les prix des matières premières. Sa gestion exige une compréhension fine de ces facteurs et de leurs interactions, en particulier dans des conditions de volatilité élevée ou de stress de marché."
  },
  {
    "objectID": "risques/chapter_two.html#les-différentes-formes-du-risque-de-marché",
    "href": "risques/chapter_two.html#les-différentes-formes-du-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Les Différentes Formes du Risque de Marché",
    "text": "Les Différentes Formes du Risque de Marché\n\nRisque de Taux d’Intérêt\nLe risque de taux traduit la sensibilité des instruments financiers aux variations des taux d’intérêt. Il se manifeste principalement à travers :\n\nle risque de niveau, lié à un déplacement parallèle de la courbe des taux ;\nle risque de pente, lorsque les taux courts et les taux longs évoluent différemment ;\nle risque de courbure, qui apparaît lorsque la convexité ou la forme de la courbe se modifie.\n\n\n\nRisque de Change\nLorsque des positions sont libellées dans différentes devises, les fluctuations des taux de change peuvent générer des gains ou des pertes. Le risque de change est particulièrement significatif pour les institutions opérant à l’international ou pour les portefeuilles multi-devises.\n\n\nRisque Actions\nCe risque correspond à l’exposition aux variations des cours boursiers. Il combine :\n\nun risque systématique, lié à l’évolution générale du marché ;\nun risque spécifique, propre à chaque entreprise ou secteur.\n\n\n\nRisque Matières Premières\nLes prix des matières premières (pétrole, gaz, métaux, produits agricoles) sont influencés par des facteurs géopolitiques, climatiques ou économiques, rendant les expositions potentiellement très volatiles."
  },
  {
    "objectID": "risques/chapter_two.html#les-mesures-de-sensibilité",
    "href": "risques/chapter_two.html#les-mesures-de-sensibilité",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Les Mesures de Sensibilité",
    "text": "Les Mesures de Sensibilité\nLa gestion du risque de marché repose largement sur l’analyse des sensibilités, qui mesurent la réaction du prix d’un instrument à la variation d’un facteur de risque.\n\nLa Duration\nPour les actifs de taux, la durée (duration) quantifie la sensibilité du prix d’un instrument à une variation du taux d’intérêt :\n\\[\nD = -\\frac{1}{P}\\,\\frac{\\partial P}{\\partial y},\n\\]\noù \\(P\\) désigne le prix de l’instrument et \\(y\\) son rendement.\n\n\nLes Grecques\nPour les produits dérivés (instruments financiers dont la valeur dépend d’un actif sous-jacent), les sensibilités sont décrites à l’aide des Grecques, qui capturent différentes dimensions du risque :\n\nDelta :\n\\[\n\\Delta = \\frac{\\partial P}{\\partial S}\n\\] mesure la sensibilité au prix du sous-jacent \\(S\\) ;\nGamma :\n\\[\n\\Gamma = \\frac{\\partial^2 P}{\\partial S^2}\n\\] capture la convexité de la relation entre \\(P\\) et \\(S\\) ;\nVega : sensibilité à la volatilité \\(\\sigma\\) ;\nTheta : sensibilité au temps (décroissance de valeur liée à l’écoulement du temps) ;\nRho : sensibilité aux variations du taux d’intérêt \\(r\\)."
  },
  {
    "objectID": "risques/chapter_two.html#value-at-risk-et-expected-shortfall-appliqués-au-risque-de-marché",
    "href": "risques/chapter_two.html#value-at-risk-et-expected-shortfall-appliqués-au-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Value-at-Risk et Expected Shortfall appliqués au Risque de Marché",
    "text": "Value-at-Risk et Expected Shortfall appliqués au Risque de Marché\nLa Value-at-Risk (VaR) et l’Expected Shortfall (ES) sont les mesures principales utilisées pour quantifier le risque de marché à l’échelle du portefeuille.\n\nApproches de Calcul de la VaR\n\nMéthode Historique\nLa VaR historique repose sur les réalisations passées des facteurs de marché. On calcule les pertes journalières successives et on retient le quantile empirique :\n\\[\n\\text{VaR}_{\\alpha} = L_{(n\\alpha)},\n\\]\noù \\(L_{(n\\alpha)}\\) désigne la perte correspondant au quantile \\(\\alpha\\).\n\n\nMéthode Paramétrique (Variance–Covariance)\nSous des hypothèses de normalité des rendements et de linéarité du portefeuille :\n\\[\n\\text{VaR}_{\\alpha} = \\mu + \\Phi^{-1}(\\alpha)\\,\\sigma,\n\\]\noù \\(\\mu\\) et \\(\\sigma\\) représentent respectivement la moyenne et l’écart-type des pertes.\n\n\nMéthode Monte Carlo\nCette approche consiste à simuler un grand nombre de scénarios, puis à en extraire le quantile :\n\\[\n\\text{VaR}_{\\alpha} = F_L^{-1}(\\alpha),\n\\]\noù \\(F_L\\) est la distribution simulée des pertes.\n\n\n\nExpected Shortfall\nL’Expected Shortfall mesure la perte moyenne au-delà du quantile défini par la VaR :\n\\[\n\\text{ES}_{\\alpha}\n= \\mathbb{E}[\\, L \\mid L &gt; \\text{VaR}_{\\alpha}(L) \\,].\n\\]\nIl fournit une information complémentaire sur la sévérité des pertes extrêmes, particulièrement importante en période de stress."
  },
  {
    "objectID": "risques/chapter_two.html#la-gestion-avancée-du-risque-de-marché",
    "href": "risques/chapter_two.html#la-gestion-avancée-du-risque-de-marché",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "La Gestion Avancée du Risque de Marché",
    "text": "La Gestion Avancée du Risque de Marché\n\nBacktesting\nLe backtesting consiste à confronter les prévisions de risque aux pertes réellement observées.\nDeux dimensions sont généralement testées :\n\nla couverture inconditionnelle, qui vérifie que le nombre de violations est cohérent avec le niveau de confiance ;\nl’indépendance, qui s’assure que les exceptions ne sont pas regroupées en périodes de tension.\n\n\n\nStress Testing\nLe stress testing évalue l’impact sur le portefeuille de scénarios extrêmes mais plausibles, qu’ils soient inspirés de périodes historiques (crise de 2008, choc COVID-19) ou construits de manière hypothétique.\n\n\nAnalyse de Scénarios\nCette méthode complète le stress testing en étudiant des évolutions coordonnées de plusieurs facteurs de risque, permettant une évaluation plus fine des vulnérabilités."
  },
  {
    "objectID": "risques/chapter_two.html#le-cadre-réglementaire-bâle-iv-et-la-frtb",
    "href": "risques/chapter_two.html#le-cadre-réglementaire-bâle-iv-et-la-frtb",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Le Cadre Réglementaire : Bâle IV et la FRTB",
    "text": "Le Cadre Réglementaire : Bâle IV et la FRTB\nLa réforme FRTB (Fundamental Review of the Trading Book) issue de Bâle IV modernise profondément la mesure du risque de marché.\n\nApproche Standardisée\nElle repose sur :\n\ndes sensibilités (delta, vega, curvature),\ndes classes de risque homogènes,\ndes horizons de liquidité différenciés selon les instruments.\n\n\n\nApproche par Modèles Internes\nPlus exigeante, elle impose notamment :\n\nl’utilisation de l’ES au lieu de la VaR ;\nla prise en compte des horizons de liquidité ;\ndes critères de backtesting renforcés.\n\n\n\nExigences Quantitatives\nLes principaux paramètres sont :\n\nun niveau de confiance de 97,5% pour l’ES ;\nune période de calcul intégrant des données de stress ;\nune diversification reconnue mais strictement encadrée."
  },
  {
    "objectID": "risques/chapter_two.html#perspectives-dévolution",
    "href": "risques/chapter_two.html#perspectives-dévolution",
    "title": "Chapitre 2 : Le Risque de Marché",
    "section": "Perspectives d’Évolution",
    "text": "Perspectives d’Évolution\nLe risque de marché évolue avec les transformations des marchés financiers et l’apparition de nouveaux facteurs de risque. Parmi eux :\n\nl’innovation financière et les nouveaux instruments complexes ;\nl’augmentation de la fréquence des chocs de marché ;\nla prise en compte des risques climatiques ;\nl’usage croissant de l’intelligence artificielle dans les modèles.\n\nCes évolutions imposent une adaptation continue des outils et des cadres prudentiels afin de garantir la résilience du système financier."
  },
  {
    "objectID": "risques/chapter_one.html#les-principaux-risques-bancaires",
    "href": "risques/chapter_one.html#les-principaux-risques-bancaires",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Les Principaux Risques Bancaires",
    "text": "Les Principaux Risques Bancaires\nLa première étape du risk management consiste à identifier les sources d’incertitude susceptibles d’affecter la situation financière de la banque. On distingue traditionnellement plusieurs grandes catégories de risques.\n\nRisque de Marché\nLe risque de marché se manifeste lorsque la valeur des positions détenues par une banque évolue défavorablement sous l’effet des variations des facteurs de marché :\n\ntaux d’intérêt,\ntaux de change,\ncours des actions,\nprix des matières premières,\nspreads de crédit,\nconditions macroéconomiques générales.\n\nCe risque reflète la sensibilité globale du portefeuille à l’environnement financier. Certains actifs réagissent fortement à de petites variations de marché, tandis que d’autres sont davantage sensibles aux mouvements amples ou aux périodes d’instabilité durant lesquelles les dépendances entre actifs deviennent moins prévisibles.\n\n\nRisque de Crédit\nLe risque de crédit correspond à la possibilité qu’une contrepartie ne respecte pas ses obligations financières. Il constitue généralement la composante la plus importante du risque bancaire.\n\n\nRisque de Liquidité\nCe risque présente deux dimensions :\n\nliquidité de marché : capacité à vendre un actif sans perte de valeur excessive,\nliquidité de financement : capacité à se refinancer à un coût raisonnable.\n\n\n\nRisque Opérationnel\nIl résulte de défaillances internes (processus, systèmes, erreurs humaines) ou d’événements externes, et englobe notamment le risque juridique.\n\n\nAutres Risques\nParmi les risques complémentaires :\n\nrisque de réputation,\n\nrisque de modèle,\n\nrisque systémique.\n\n\nLes risques bancaires sont fortement interconnectés : une approche fragmentée peut conduire à sous-estimer les vulnérabilités globales."
  },
  {
    "objectID": "risques/chapter_one.html#les-mesures-de-risque",
    "href": "risques/chapter_one.html#les-mesures-de-risque",
    "title": "Chapitre 1 : Introduction à la Gestion des Risques Bancaires",
    "section": "Les Mesures de Risque",
    "text": "Les Mesures de Risque\nLa mesure du risque repose sur des outils statistiques permettant de quantifier les pertes potentielles associées à une position ou un portefeuille.\nUne mesure de risque, notée \\(\\mathcal{R}(L)\\) pour une perte aléatoire \\(L\\), représente généralement le capital économique nécessaire pour couvrir cette perte dans un scénario défavorable.\n\nMesures de Risque Cohérentes\nArtzner et al. ont défini quatre propriétés fondamentales caractérisant une mesure de risque cohérente :\n\nInvariance par translation \\[\n\\mathcal{R}(L + c) = \\mathcal{R}(L) + c\n\\]\nHomogénéité positive \\[\n\\mathcal{R}(\\lambda L) = \\lambda \\mathcal{R}(L)\n\\]\nMonotonie \\[\nL_1 \\le L_2 \\;\\Rightarrow\\; \\mathcal{R}(L_1) \\le \\mathcal{R}(L_2)\n\\]\nSous-additivité \\[\n\\mathcal{R}(L_1 + L_2) \\le \\mathcal{R}(L_1) + \\mathcal{R}(L_2)\n\\]\n\nCes propriétés assurent un comportement économique raisonnable de la mesure du risque.\nDans ce cadre, la Value-at-Risk (VaR) est devenue l’outil le plus couramment utilisé en pratique, notamment dans les réglementations prudentielles."
  },
  {
    "objectID": "risques/chapter_three.html",
    "href": "risques/chapter_three.html",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "risques/chapter_three.html#comprendre-le-risque-de-crédit",
    "href": "risques/chapter_three.html#comprendre-le-risque-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Comprendre le Risque de Crédit",
    "text": "Comprendre le Risque de Crédit\nLe risque de crédit correspond à la possibilité qu’un emprunteur ou une contrepartie ne respecte pas ses obligations financières. Pour la plupart des institutions bancaires, il constitue la composante la plus importante du risque global, car il concerne directement la capacité des clients à rembourser leurs dettes.\nContrairement au risque de marché, qui résulte de la fluctuation des prix financiers, le risque de crédit se focalise sur la solvabilité. Sa gestion repose sur l’évaluation de la qualité des contreparties, la modélisation des comportements de défaut et l’estimation des pertes potentielles."
  },
  {
    "objectID": "risques/chapter_three.html#les-concepts-clés-du-risque-de-crédit",
    "href": "risques/chapter_three.html#les-concepts-clés-du-risque-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Les Concepts Clés du Risque de Crédit",
    "text": "Les Concepts Clés du Risque de Crédit\nLes modèles réglementaires et internes reposent sur quatre paramètres fondamentaux.\n\nProbabilité de Défaut (PD)\nLa probabilité de défaut (PD) mesure la probabilité qu’une contrepartie fasse défaut sur un horizon donné, le plus souvent un an :\n\\[\nPD = \\mathbb{P}(\\text{Défaut})\n\\]\n\n\nPerte en Cas de Défaut (LGD)\nLa LGD (Loss Given Default) représente la proportion de l’exposition qui serait perdue en cas de défaut après prise en compte des recouvrements :\n\\[\nLGD = 1 - \\text{Taux de Recouvrement}\n\\]\n\n\nExposition au Moment du Défaut (EAD)\nL’exposition au défaut (EAD) correspond au montant effectivement engagé au moment du défaut.\nPour les engagements conditionnels, elle inclut un facteur de conversion :\n\\[\nEAD = \\text{Encours Courant} + \\text{Engagements Non Utilisés} \\times CCF\n\\]\n\n\nPerte Attendue (EL)\nLes trois paramètres précédents permettent de définir la perte attendue, composante centrale de la gestion du risque de crédit :\n\\[\nEL = PD \\times LGD \\times EAD\n\\]"
  },
  {
    "objectID": "risques/chapter_three.html#les-marchés-du-crédit",
    "href": "risques/chapter_three.html#les-marchés-du-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Les Marchés du Crédit",
    "text": "Les Marchés du Crédit\n\nMarché des Prêts Bancaires\nLe marché du crédit bancaire se divise principalement en :\n\nBanque de détail : crédits aux particuliers et aux petites entreprises\n\nBanque corporate : financements aux grandes entreprises\n\nProcessus de scoring : modèles automatisés d’évaluation du risque\n\n\n\nMarché Obligataire\nLes entreprises et États peuvent se financer via l’émission d’obligations :\n\nObligations souveraines : exposition au risque de crédit de l’État\n\nObligations corporate : risque propre aux entreprises\n\nSpread de crédit : prime de risque exigée pour compenser un risque de défaut\n\n\n\nTitrisation et Dérivés de Crédit\n\nTitrisation\nLa titrisation permet de transformer des actifs illiquides en titres négociables :\n\nABS : Asset-Backed Securities\n\nMBS : Mortgage-Backed Securities\n\nCDO : Collateralized Debt Obligations\n\n\n\nDérivés de Crédit\nLe CDS (Credit Default Swap) est l’instrument de référence pour transférer le risque de crédit :\n\nl’acheteur de protection paie une prime périodique,\n\nle vendeur de protection indemnise en cas de défaut."
  },
  {
    "objectID": "risques/chapter_three.html#le-cadre-réglementaire-bâle-ii-iii",
    "href": "risques/chapter_three.html#le-cadre-réglementaire-bâle-ii-iii",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Le Cadre Réglementaire Bâle II / III",
    "text": "Le Cadre Réglementaire Bâle II / III\n\nApproche Standardisée\nLes pondérations de risque sont déterminées à partir des notations externes. Exemple simplifié :\n\n\n\nNotation\nPondération\n\n\n\n\nAAA–AA\n20%\n\n\nA+\n50%\n\n\nBBB\n100%\n\n\nBB\n150%\n\n\n\n\n\nApproche Fondation (FIRB)\nDans l’approche FIRB :\n\nLa banque estime la PD\n\nLa LGD et l’EAD sont standardisées\n\nLe capital est calculé via la formule réglementaire\n\n\n\nApproche Avancée (AIRB)\nL’approche AIRB permet à la banque d’estimer :\n\nPD\n\nLGD\n\nEAD\n\nmaturité effective (M)\n\nCes approches offrent plus de précision mais exigent une validation stricte des modèles."
  },
  {
    "objectID": "risques/chapter_three.html#modélisation-des-paramètres-de-crédit",
    "href": "risques/chapter_three.html#modélisation-des-paramètres-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Modélisation des Paramètres de Crédit",
    "text": "Modélisation des Paramètres de Crédit\n\nModélisation de la PD\n\nApproche par Cohortes\nLa méthode des cohortes repose sur l’historique de défauts :\n\\[\nPD_c = \\frac{\\sum D^c_T}{\\sum N^c_T}\n\\]\n\n\nApproche Statistique\nLes modèles statistiques, comme la régression logistique, relient la PD aux caractéristiques des emprunteurs :\n\\[\nPD = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n)}}\n\\]\n\n\n\nModélisation de la LGD\n\nLGD de Marché\nEstimée à partir des prix observés après défaut :\n\\[\nLGD_{\\text{Marché}} = 1 - \\frac{\\text{Prix Après Défaut}}{\\text{Nominal}}\n\\]\n\n\nLGD de Recouvrement\nBasée sur les flux de recouvrement actualisés :\n\\[\nLGD_{\\text{Recouvrement}} = \\frac{EAD - \\text{Recouvrements Actualisés}}{EAD}\n\\]\n\n\n\nModélisation de l’EAD\nPour les engagements conditionnels :\n\\[\nEAD = \\text{Encours} + \\text{Non Utilisé} \\times CCF\n\\]"
  },
  {
    "objectID": "risques/chapter_three.html#le-capital-économique-pour-le-risque-de-crédit",
    "href": "risques/chapter_three.html#le-capital-économique-pour-le-risque-de-crédit",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Le Capital Économique pour le Risque de Crédit",
    "text": "Le Capital Économique pour le Risque de Crédit\n\nValue-at-Risk Crédit\nLe capital économique est défini comme la différence entre la VaR et la perte attendue :\n\\[\nEC_{\\alpha} = VaR_{\\alpha} - EL\n\\]\n\n\nModèle de Vasicek\nDans le cadre du modèle à un facteur systémique :\n\\[\nVaR_{\\alpha}\n= \\Phi\\!\\left(\n\\frac{\\Phi^{-1}(PD) + \\sqrt{\\rho}\\,\\Phi^{-1}(\\alpha)}\n     {\\sqrt{1-\\rho}}\n\\right)\n\\]\nCe modèle constitue la base de la formule réglementaire IRB."
  },
  {
    "objectID": "risques/chapter_three.html#évolutions-règlementaires-récentes",
    "href": "risques/chapter_three.html#évolutions-règlementaires-récentes",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Évolutions Règlementaires Récentes",
    "text": "Évolutions Règlementaires Récentes\n\nBâle III\nParmi les renforcements :\n\nexigences plus strictes pour les expositions financières,\ncoussin de conservation du capital,\nexigences spécifiques pour les portefeuilles de trading.\n\n\n\nBâle IV\nLes réformes introduisent :\n\ndes planchers limitant la réduction des RWA via modèles internes,\nune révision des pondérations standardisées,\nun traitement plus prudent des expositions actions et immobilières."
  },
  {
    "objectID": "risques/chapter_three.html#perspectives-et-défis",
    "href": "risques/chapter_three.html#perspectives-et-défis",
    "title": "Chapitre 3 : Le Risque de Crédit",
    "section": "Perspectives et Défis",
    "text": "Perspectives et Défis\nLa gestion du risque de crédit doit s’adapter à plusieurs tendances majeures :\n\nintégration des risques climatiques,\n\nmontée en puissance des techniques de machine learning,\n\ncycles économiques plus incertains,\n\nexploitation de données massives.\n\nCes évolutions nécessitent une amélioration continue des méthodes d’évaluation et des cadres prudentiels."
  },
  {
    "objectID": "risques/chapter_four.html",
    "href": "risques/chapter_four.html",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "risques/chapter_four.html#comprendre-le-risque-de-contrepartie",
    "href": "risques/chapter_four.html#comprendre-le-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Comprendre le Risque de Contrepartie",
    "text": "Comprendre le Risque de Contrepartie\nLe risque de contrepartie correspond à la possibilité qu’une partie engagée dans une transaction financière ne respecte pas ses obligations contractuelles. Il est particulièrement important pour les produits dérivés (options, swaps, forwards) et certaines opérations de financement (prêts de titres, pensions livrées).\nContrairement au risque de crédit traditionnel, où le montant dû est connu à l’avance, le risque de contrepartie porte sur des expositions futures incertaines, dépendant de l’évolution des marchés. La valeur d’un contrat peut devenir positive ou négative selon les mouvements du sous-jacent, ce qui rend la mesure du risque plus complexe."
  },
  {
    "objectID": "risques/chapter_four.html#les-spécificités-du-risque-de-contrepartie",
    "href": "risques/chapter_four.html#les-spécificités-du-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Les Spécificités du Risque de Contrepartie",
    "text": "Les Spécificités du Risque de Contrepartie\n\nRisque Bilatéral\nDans la majorité des dérivés, chaque partie peut tour à tour être créancière ou débitrice selon l’évolution du marché. L’exposition est donc bilatérale et dépend du sens des variations du sous-jacent.\n\n\nValeur Incertaine des Expositions\nLa perte potentielle en cas de défaut n’est pas fixe : elle dépend du mark-to-market du contrat au moment où le défaut survient. Ainsi, le risque de contrepartie est intimement lié à la dynamique du marché et peut varier considérablement dans le temps."
  },
  {
    "objectID": "risques/chapter_four.html#mesure-de-lexposition",
    "href": "risques/chapter_four.html#mesure-de-lexposition",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Mesure de l’Exposition",
    "text": "Mesure de l’Exposition\nLa gestion du risque de contrepartie repose sur la définition de plusieurs mesures d’exposition potentielle ou attendue.\n\nExposition Potentielle Future (PFE)\nLa PFE représente une borne supérieure de l’exposition future pour un niveau de confiance donné. Elle joue un rôle analogue à une Value-at-Risk appliquée aux expositions positives :\n\\[\nPFE_{\\alpha}(t) = \\text{Quantile}_{\\alpha}\\big( \\max(V_t, 0) \\big)\n\\]\noù ( V_t ) est la valeur future du contrat.\n\n\nExposition Attendue (EE)\nL’exposition attendue à une date future correspond à la moyenne des expositions positives possibles :\n\\[\nEE(t) = \\mathbb{E}\\big[ \\max(V_t, 0) \\big]\n\\]\n\n\nExposition Attendue Positive (EPE)\nL’EPE est la moyenne pondérée de l’EE sur l’horizon considéré. C’est une mesure synthétique largement utilisée dans les approches réglementaires :\n\\[\nEPE = \\frac{1}{T} \\int_0^T EE(t)\\,dt\n\\]"
  },
  {
    "objectID": "risques/chapter_four.html#techniques-de-réduction-du-risque",
    "href": "risques/chapter_four.html#techniques-de-réduction-du-risque",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Techniques de Réduction du Risque",
    "text": "Techniques de Réduction du Risque\n\nCompensation (Netting)\nLes contrats conclus avec une même contrepartie peuvent être regroupés dans une convention de compensation, permettant de compenser les valeurs positives et négatives :\n\nSans netting : les expositions s’additionnent\n\nAvec netting : seule l’exposition nette est considérée\n\nCela réduit mécaniquement la perte potentielle en cas de défaut.\n\n\nCollatéralisation\nLes accords de collatéral (CSA) prévoient l’échange de garanties pour couvrir l’exposition courante. Les montants sont ajustés régulièrement pour refléter l’évolution de la valeur des positions.\nLe processus comprend généralement :\n\nCalcul quotidien du mark-to-market\n\nDétermination de l’exposition nette\n\nDépôt ou restitution de collatéral selon les seuils contractuels\n\n\n\nAppels de Marge\nLorsque la valeur du contrat varie défavorablement pour une partie, celle-ci peut être tenue de fournir des garanties additionnelles. Les appels de marge constituent un mécanisme essentiel pour limiter l’accumulation d’expositions."
  },
  {
    "objectID": "risques/chapter_four.html#le-cva-valorisation-du-risque-de-contrepartie",
    "href": "risques/chapter_four.html#le-cva-valorisation-du-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Le CVA : Valorisation du Risque de Contrepartie",
    "text": "Le CVA : Valorisation du Risque de Contrepartie\n\nDéfinition\nLe Credit Valuation Adjustment (CVA) correspond à la diminution de la valeur théorique d’un contrat dérivé liée au risque de défaut de la contrepartie. Il s’agit de l’écart entre la valeur du contrat dans un monde sans défaut et sa valeur ajustée du risque de crédit.\nFormellement :\n\\[\nCVA = V_{\\text{sans risque}} - V_{\\text{avec risque}}\n\\]\n\n\nImportance du CVA\nLors de la crise de 2008, une grande partie des pertes liées aux dérivés provenait de la variation du CVA plutôt que de défauts avérés. Le CVA est devenu un déterminant central de la valorisation et du capital réglementaire.\n\n\nFormule Simplifiée\nUne approximation classique du CVA est :\n\\[\nCVA = (1 - R)\\, \\sum_{t} EE(t)\\, PD(t)\n\\]\noù :\n\n( R ) : taux de recouvrement\n\n( EE(t) ) : exposition attendue\n\n( PD(t) ) : probabilité de défaut marginale\n\nCette formule met en lumière les trois composantes essentielles du CVA : exposition, risque de défaut, sévérité de la perte."
  },
  {
    "objectID": "risques/chapter_four.html#le-cadre-réglementaire",
    "href": "risques/chapter_four.html#le-cadre-réglementaire",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Le Cadre Réglementaire",
    "text": "Le Cadre Réglementaire\n\nBâle III — Capital CVA\nBâle III introduit un capital CVA destiné à couvrir non seulement le défaut mais aussi la dégradation de la qualité de crédit des contreparties. Deux approches coexistent :\n\n\nApproche Standardisée\nBasée sur des coefficients réglementaires et les notations externes des contreparties.\n\n\nApproche Avancée\nLes banques peuvent utiliser leurs propres modèles internes pour le calcul du CVA et du capital associé, sous conditions de validation prudentielle stricte."
  },
  {
    "objectID": "risques/chapter_four.html#défis-de-la-modélisation",
    "href": "risques/chapter_four.html#défis-de-la-modélisation",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Défis de la Modélisation",
    "text": "Défis de la Modélisation\n\nRisque de Wrong-Way\nLe risque de wrong-way survient lorsque l’exposition augmente précisément lorsque la contrepartie devient plus susceptible de faire défaut.\nExemple typique : contrepartie très exposée au prix d’une matière première qui s’effondre.\n\n\nQualité des Données\nLa mesure du CVA repose sur :\n\ndes données de marché (courbes de taux, volatilités, CDS),\ndes données contractuelles (CSA, netting),\nune modélisation du défaut.\n\nL’agrégation et la cohérence de ces données constituent un défi opérationnel majeur.\n\n\nComplexité des Modèles\nLes modèles doivent intégrer :\n\ndépendance entre exposition et défaut,\neffets de la compensation,\ndynamique du collatéral,\nmigrations de crédit.\n\nLa sophistication croissante des instruments accroît les exigences de modélisation."
  },
  {
    "objectID": "risques/chapter_four.html#bonnes-pratiques-pour-la-gestion-du-risque-de-contrepartie",
    "href": "risques/chapter_four.html#bonnes-pratiques-pour-la-gestion-du-risque-de-contrepartie",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Bonnes Pratiques pour la Gestion du Risque de Contrepartie",
    "text": "Bonnes Pratiques pour la Gestion du Risque de Contrepartie\n\nSuivi continu des expositions et de la qualité de crédit\n\nLimites d’exposition par contrepartie, secteur, notation\n\nDiversification des partenaires contractuels\n\nCommunication claire entre équipes (front office, risque, opérations)\n\nTests de stress centrés sur les expositions futures"
  },
  {
    "objectID": "risques/chapter_four.html#perspectives-et-défis",
    "href": "risques/chapter_four.html#perspectives-et-défis",
    "title": "Chapitre 4 : Le Risque de Contrepartie et le CVA",
    "section": "Perspectives et Défis",
    "text": "Perspectives et Défis\nLe risque de contrepartie évolue face à plusieurs enjeux stratégiques :\n\nintégration des risques climatiques dans les modèles,\nrôle croissant de la compensation centrale,\ndéveloppement de la blockchain et des smart contracts,\névolution des standards comptables et prudentiels.\n\nLa gestion rigoureuse du risque de contrepartie demeure essentielle pour la résilience du système financier."
  },
  {
    "objectID": "risques/index.html#mes-publications",
    "href": "risques/index.html#mes-publications",
    "title": "Risque & Régulation",
    "section": "Mes publications",
    "text": "Mes publications\n\n\n\n\n\nGestion de la liquidité des actifs\n\n\nProfondeur de marché, calcul du profil d’écoulement, liquidation avec ou sans impact de marché.\n\nLire"
  },
  {
    "objectID": "risques/index.html#section",
    "href": "risques/index.html#section",
    "title": "Risque & Régulation",
    "section": " ",
    "text": "Gestion de la liquidité des actifs\n\n\nProfondeur de marché, calcul du profil d’écoulement, liquidation avec ou sans impact de marché.\n\nLire\n\n\n\n\nGestion de risques multiples : théorie des copules\n\n\nModélisation de dependances via copules,Triangle de crédit, Calcul de VaR, intervalle de confiance.\n\nLire\n\n\n\n\nGestion de risques multiples : théorie des copules\n\n\nModélisation de dependances via copules,Triangle de crédit, Calcul de VaR, intervalle de confiance.\n\nLire"
  },
  {
    "objectID": "risques/projets/demo.html",
    "href": "risques/projets/demo.html",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "",
    "text": "Ce document présente une introduction à la gestion des risques en Asset Management.\nL’objectif principal est de comprendre et d’appliquer les méthodologies de base utilisées pour analyser, mesurer et suivre les risques financiers au sein d’un portefeuille.\nL’étude couvre differentes notions de risk management notamment:\n- les méthodes de mesure du risque de marché (sensibilité, volatilité, Value-at-Risk, Tracking Error, stress tests et profil de liquidité),\n- les composantes clés du risque de crédit (intensité de défaut, sensibilité des obligations aux variations de spreads, triangle du crédit),\n- le risque de contrepartie et ses mécanismes de mitigation via les appels de marge,\n- ainsi que les risques émergents tels que le risque climatique et le risque modèle.\n\n\nCode\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import norm\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize"
  },
  {
    "objectID": "risques/projets/demo.html#i.2.-suivi-du-porte-feuille",
    "href": "risques/projets/demo.html#i.2.-suivi-du-porte-feuille",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "I.2. Suivi du porte feuille",
    "text": "I.2. Suivi du porte feuille\n\nDans cette section on va determiner l’AUM du portefeuille, suivre ses performances sa volatilité et ses performances relatives par rapport au CAC40.\nNous effectuerons également un stress test sur le portefeuille pour ebaluer sa sensibilité relative à la période COVID.\n\n\nCode\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), weights*100, color='skyblue')\n\nplt.title(f\"Poids optimaux des actifs dans le portefeuille (Volatilité: {round(vol, 4)})\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Poids (en %)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, weights):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "risques/projets/demo.html#stress-test",
    "href": "risques/projets/demo.html#stress-test",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Stress test",
    "text": "Stress test\nLe stress test ou test de résistance est un exercice qui consiste à reproduire des scénarios extrêmes sur un portefeuille afin d’évaluer sa résilience en conjoncture défavorable. On distingue notamment deux types de stress test: - Le stress test historique qui vise à reproduire des scénarios de crises historisques, - Le stress test hypothétique qui consiste à considérer des scénarios théoriques extrêmes.\nNous allons dans la suite implémenter un stress test historique en reproduisant le choc COVID entre 19/02/2020 et 19/03/2020. Il s’agit s’agira de calculer la performance du fonds entre ces deux date et de la comparer à celle de son indice de référence.\n\nDonnées des actions en période covid\n\n\nCode\n# Données des actions en période covid\nstress_date_1 = \"2020-02-19\"\nstress_date_2 = \"2020-03-19\"\n\ntickers = ['SAN.PA', 'BN.PA',  'AIR.PA', 'AI.PA', 'ORA.PA', 'CAP.PA', 'VIV.PA', 'CA.PA', 'ENGI.PA', 'DG.PA']\ndf_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\ncac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\3326232529.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**********            20%                       ]  2 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\3326232529.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\naum_stress = df_stress@weights\nportfolio_stress = pd.DataFrame({'AUM': aum_stress})\n\n\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\nportfolio_stress.iloc[-1,] /portfolio_stress.iloc[0,]  - 1\n\n\nAUM   -0.335339\ndtype: float64\n\n\n\n\nRendement du benchmark enn période covid\n\n\nCode\n# Rendemnt du benchmark enn période covid\ncac40_stress.iloc[-1,] / cac40_stress.iloc[0,] -1\n\n\nTicker\n^FCHI   -0.385585\ndtype: float64\n\n\nLe fonds stressé a une performance absolue de -33.69% contre -38.55% pour son benchmark. Ces performances en période de crise COVID sont du même ordre de grandeur. Toutefois, la perte enregistrée sur le fonds est moins importante. Une analyse des performances des actions constituant le fonds montre que la sous-performance enegistrée est essentiellement portée par: Air Liquide, Capgenie et Engie.\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\ndf_stress.iloc[-1,] /df_stress.iloc[0,]  - 1\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1, color='skyblue')\n\nplt.title(f\"Performance de la composition du fond stressé\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Performance (%)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "risques/projets/demo.html#cadre-de-suivi-de-la-liquidité",
    "href": "risques/projets/demo.html#cadre-de-suivi-de-la-liquidité",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Cadre de suivi de la liquidité",
    "text": "Cadre de suivi de la liquidité\n\nLe suivi de la liquidité est d’une importance centrale en gestion de fonds et est encadré par des exigences règlementaires. Il s’agit entre autres pour le gestionnaire de pouvoir honorer ses engagements vis-à-vis de des investisseurs en cas de rachat de parts du fonds. Il faut donc s’assurer de la capacité à revendre les actifs composant le fonds dans des délais courts sans toutefois les brader.\nC’est ce qu’on appelle le risque de liquidité qui se défini plus formellement comme étant la facilité avec laquelle une entreprise peut échanger ses actifs contre du cash, même en situation soudaine de besoin de liquidité, sans subir de coûts anormaux par rapport aux autres acteurs du marché.\nLe suivi de la liquidité d’un fonds s’effectue par le calcul de son profil d’écoulement en condition normale et en condition stressée.\n\nLa méthodologie de calcul d’un profil d’écoulement\nLe fonds est constitué de n actifs en quantité \\(Q_i\\) chacun à liquider. Dans la pratique il existe une quantité maximale au delà de laquelle les échanges ne peuvent se faire sans subir de coûts anormaux: c’est la profondeur de marché. Le coût de liquidation est induit par les volume d’ordre émis.\nLa profondeur de marché est généralement estimée en examinant: - Les carnets d’ordres ou - Les volumes quotidiens échangés: On considère notamment le volume quotidien moyen échangé sur 3 mois appelé ADV ou ATV (Average Daily/Traded Volume). La profondeur de marché est alors estimée par \\(Q^* = 20\\% \\times ADV_{3 mois}\\)\n\nOn distingue quatre profils d’écoulement: - En codition normale: avec et sans déformation - En condition stressée: avec déformation et sans déformation\nDans une liquidation avec déformation, les actifs les plus liquides sont liquidés en premiers. La déformation du portefeuille s’illustre par les changements de poids des différents émetteurs dans le fonds. La liquidation sans déformation quant à elle consistera à liquider le fonds au rythme de l’actif le moins liquide afin de préserver les poids initiaux.\nN.B: Le profil de liquidation est représenté en proportion du portefeuille initial.\nOn utilise le profil de liquidité pour déterminer la taille optimal du fonds permettant de respecter les exigence de minimum de liquidité. Il s’agira de s’assurer que plus de 90% du fonds est liquidé en deux jours, laissant ainsi un maximum de 10 % dans la catégorie dite “poubelle” (c’est-à-dire les actifs moins liquides).\nEn condition stressée avec déformation: On se place dans une situation dans laquelle la profondeur de marché est réduite à la suite d’un choc conduisant à un assèchement du marché.\n\n\nCode\ndef liqudity(df_volume, ADV_rate=0.2, weight_adjust=1, label=\"Conditions normales avec déformation\", seed = 42):\n    \"\"\"\n    Fonction de calcul de la liquidité d'un portefeuille sur plusieurs jours.\n    \n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        DataFrame contenant les volumes quotidiens pour chaque actif (tickers).\n    ADV_rate : float, optionnel\n        Taux de liquidité journalière exprimé en pourcentage de l'ADV (par défaut 0.2, soit 20%).\n    weight_adjust : float, optionnel\n        Facteur de pondération pour ajuster la quantité générée aléatoirement (par défaut 1).\n    label : str, optionnel\n        Label pour décrire le scénario de liquidation (par défaut \"Conditions normales avec déformation\").\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        DataFrame contenant les informations suivantes pour chaque actif :\n            - 'Ticker' : Le symbole de l'actif.\n            - 'ADV' : Le volume moyen quotidien calculé sur une fenêtre de 60 jours.\n            - 'Prix' : Le prix actuel de l'actif (extrait de df_close).\n            - 'QUANTITE' : La quantité totale simulée de l'actif détenu par le portefeuille.\n            - 'QUANTITE LIQUIDABLE' : La quantité qui peut être liquidée quotidiennement (ADV_rate * ADV).\n            - 'NB_JOURS DE LIQUIDATION' : Nombre de jours estimés pour liquider entièrement l'actif.\n            - 'JOUR X' (où X est un numéro de jour) : Quantité restante après chaque jour de liquidation.\n            - 'POIDS X' : Poids relatif de chaque actif dans le portefeuille après chaque jour de liquidation.\n    \n    Description :\n    -------------\n    1. Calcul de l'Average Daily Volume (ADV) sur une fenêtre de 60 jours.\n    2. Création d'un DataFrame contenant les informations de liquidité de chaque actif.\n    3. Génération de quantités d'actifs aléatoires pour simuler un portefeuille.\n    4. Calcul de la quantité liquidable par jour (20 % de l'ADV par défaut).\n    5. Simulation de la liquidation progressive de chaque actif jour par jour jusqu'à épuisement.\n    6. Calcul des poids du portefeuille après chaque jour de liquidation.\n    \n    \"\"\"\n    # Calcul de la moyenne mobile sur 60 jours du volume de transactions (ADV : Average Daily Volume)\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de l'ADV le plus récent pour chaque ticker\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame contenant l'ADV, le prix actuel et d'autres informations pour chaque actif\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # On définit la colonne 'Ticker' comme index pour faciliter l'accès aux données\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Génération de quantités d'actifs simulées basées sur l'ADV\n    np.random.seed(seed)  # Pour des résultats reproductibles\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable : 20 % de l'ADV (peut être ajusté via 'ADV_rate')\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours nécessaires pour liquider chaque actif\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(\n        portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE']\n    )\n\n    # Initialisation pour le jour 0\n    i = 1\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    # Calcul de la quantité restante après le premier jour de liquidation\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    # Calcul itératif jusqu'à liquidation complète\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        # Calcul de la quantité restante après chaque jour supplémentaire\n        portfolio_liquidity[f'JOUR {i}'] = (\n            portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n        ).clip(lower=0)\n\n        # Mise à jour des poids du portefeuille pour ce jour\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    return portfolio_liquidity\n\n\n\n\nCode\ndef liquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales avec déformation\"):\n    \"\"\"\n    Fonction qui génère un profil de liquidité d'un portefeuille en fonction du nombre de jours nécessaires pour liquider tous les actifs.\n\n    Paramètres :\n    -----------\n    portfolio_liquidity : DataFrame\n        Le DataFrame généré par la fonction liqudity() contenant les quantités restantes par jour pour chaque actif.\n    label : str, optionnel\n        Titre du graphique affiché (par défaut \"Profil de liquidité en conditions normales avec déformation\").\n\n    Description :\n    -------------\n    1. Calcule la quantité restante d'actifs pour chaque jour jusqu'à liquidation totale.\n    2. Calcule le profil de liquidité en pourcentage du portefeuille initial liquidé au fil du temps.\n    3. Génère deux graphiques :\n       - Profil de liquidité en pourcentage du portefeuille initial.\n       - Évolution des poids des actifs au fil des jours.\n\n    Retourne :\n    ---------\n    Affiche deux graphiques côte à côte montrant la liquidation progressive et l'évolution des poids.\n    \"\"\"\n    # Calculer le profil de liquidité\n    liquidity_profile = [portfolio_liquidity['QUANTITE']]\n    for i in range(1, int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        liquidity_profile.append(portfolio_liquidity[f'JOUR {i}'])\n\n    liquidity_profile = pd.DataFrame(liquidity_profile)\n\n    # Calculer le profil en pourcentage de liquidité restante\n    profile = liquidity_profile @ portfolio_liquidity['Prix']\n    portefeuille_valeur = profile['QUANTITE']  # Récupérer la valeur du portefeuille\n    profile = 100 - profile[1:,] * 100 / portefeuille_valeur\n\n    # Créer une figure avec deux sous-graphiques côte à côte\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))  \n\n    # Premier graphique : Profil de liquidité\n    ax1 = axes[0]\n    profile.plot(ax=ax1, marker='o', legend=False, color='blue')\n    barplot = profile.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black', alpha=0.5)\n\n    # Affichage des valeurs sur les barres\n    for bar in barplot.patches:\n        value = round(bar.get_height(), 2)\n        ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{value}\",\n                 ha='center', va='bottom', fontsize=10, color='black')\n\n    ax1.set_title(f\"{label}\", fontsize=14)\n    ax1.set_xlabel(\"Jours\", fontsize=12)\n    ax1.set_ylabel(\"Liquidité du portefeuille (%)\", fontsize=12)\n    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Étiquette de valeur du portefeuille\n    ax1.text(0.95, 0.05, f\"Valeur portefeuille : {round(portefeuille_valeur)} euros\",\n             transform=ax1.transAxes, fontsize=12, color='red', ha='right', va='bottom',\n             bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"red\"))\n\n\n    # Deuxième graphique : Évolution des poids\n    ax2 = axes[1]\n    for i in range(int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        if f'POIDS {i}' in portfolio_liquidity.columns:\n            ax2.plot(portfolio_liquidity[f'POIDS {i}'], label=f'Jour {i}')\n        else:\n            print(f\"Colonne 'POIDS {i}' non trouvée.\")\n\n    ax2.set_title(\"Évolution des Poids au Fil des Jours\", fontsize=14)\n    ax2.set_xlabel(\"Index\", fontsize=12)\n    ax2.set_ylabel(\"Poids\", fontsize=12)\n    ax2.legend()\n    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Ajustement de l'affichage\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nProfil d’écoulement avec déformation\n\nProfil d’écoulement en conditions normales avec déformation\n\n\nCode\nportfolio_liquidity = liqudity(df_volume)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLe portefeuille étudié est liquidable en 8 jours au minimum. Au premier jour seul 23.17% du portefeuille est liquidable et atteint les 73.54% au 4eme jour. On observe que les poids des actifs composant le portefeuille sont déformés au fil des jours. Il y a alors une concentration du portefeuille sur les actifs les moins liquides. Cette situation est domageable pour les investisseurs restant dans le fonds, car il faudra plus de temps pour honorer les engagements du fonds vis-à-vis d’eux en cas de rachat ce qui augmente leur exposition au risque de liquidité..\nAfin de protéger ces investisseurs, le régulateur impose aux gérants de fonds de fixer des GATES qui doivent obligatoirement être présentés dans leurs prospectus. Cela consiste à donner une option au gérant pour restreindre les rachats quotidien, à 5% du fonds par exemple.\n\n\n\nProfil d’écoulement en conditions stressée avec déformation\nEn conjoncture défavorable, la profondeur de marché est réduite ce qui entraîne un contraction des volumes liquidables. Nous simulons un tel choc en réduisant la profondeur de marché à 10% l’ADV.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLa contraction de la profondeur de marché rallonge la durée minimale de liquidation du fonds à 15 jours contre 8 jours initialement. Ceci illustre bien l’exposition du fonds au risque de liquidité. Il devient donc indispensable pour le gérant du fonds de déterminer une taille optimale de celui-ci afin de réduire son exposition au risque de liquidité.\n\n\n\nTaille optimale du portefeuille\n\nOn peut ajuster la taille du portefeuille en faisant varier le paramètre weight_adjust de la fonction liqudity afin de déterminer la taille optimale du fonds liquidable en 1 jour.\nPour le cas étudié, le fonds optimal a une valeur de 123 184 238 euros dont 99.82% est liquidable en 1 jour.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\nEn repétant le scénario défavorable d’un choc qui réduit la profondeur de marché à 10% l’ADV, le fonds optimal ainsi constitué est liquidable à 99.82% en deux jours. Il est donc plus résilient.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\n\n\n\nProfil d’écoulement sans déformation\nRappelons qu’une politique de gestion sans déformation consiste à appliquer des rachats proformat. Ceci garantisse une stabilité de la composition du fonds.\n\n\nCode\ndef liqudity_proformat(df_volume, ADV_rate = 0.2, weight_adjust = 1):\n    \"\"\"\n    Fonction qui génère un DataFrame décrivant la liquidité d'un portefeuille sur plusieurs jours, en fonction de la quantité disponible par jour et des poids correspondants.\n\n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        Un DataFrame contenant les volumes quotidiens de chaque ticker.\n    ADV_rate : float, optionnel\n        Le pourcentage d'Average Daily Volume (ADV) disponible chaque jour pour être liquidé (par défaut : 0.2, soit 20 %).\n    weight_adjust : float, optionnel\n        Facteur d'ajustement appliqué pour moduler la quantité calculée (par défaut : 1).\n\n    Description :\n    -------------\n    1. Calcule l'ADV (volume moyen sur 60 jours) pour chaque actif.\n    2. Génère un DataFrame contenant l'ADV, les prix actuels, les quantités à liquider, et les quantités liquidables par jour.\n    3. Calcule le nombre de jours nécessaire pour liquider complètement chaque actif.\n    4. Produit une série de DataFrames pour chaque jour, montrant la quantité restante, les poids associés, et les vitesses de liquidation.\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        Un DataFrame contenant les informations de liquidité du portefeuille sur plusieurs jours.\n    \"\"\"\n    # Calcul de la moyenne mobile du volume (ADV) sur une fenêtre de 60 jours\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de la dernière ligne (les valeurs ADV les plus récentes)\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame pour la liquidité du portefeuille\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # Définir 'Ticker' comme index pour un accès plus facile\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Calcul de la quantité (QUANTITE) basée sur le volume le plus récent\n    np.random.seed(42)\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable par jour\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours de liquidation nécessaire\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE'])\n\n    i = 1\n    # Initialisation du premier jour\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity['QUANTITE']\n\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        portfolio_liquidity[f'JOUR {i}'] = (\n            (portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'] * portfolio_liquidity[f'SPEED {i-1}'].min())\n        ).clip(lower=0)\n\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n        portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity[f'JOUR {i}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity[f'JOUR {i}']\n\n    return portfolio_liquidity\n\n\n\nProfil d’écoulement en conditions normales sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales sans déformation\")\n\n\n\n\n\n\n\n\n\nLes actions sont liquidées à la vitesse de l’actif le moins liquide du portefeuille. Par conséquent, contrairement à un écoulement avec déformation, la quantité liquidable progresse plus lentement dans le temps. Par exemple, on observe qu’en quatre jours seuls 59,01 % du fonds peuvent être liquidés, contre 73,54 % dans un scénario avec déformation.\nPar ailleurs, les poids des différents actifs restent globalement stables au fil des jours, à l’exception du dernier jour où l’intégralité du fonds est liquidée. Cette stabilité implique qu’il n’y a pas de concentration progressive du risque de liquidité sur les actifs les moins liquides, ce qui constitue une caractéristique importante du mécanisme d’écoulement sans déformation.\n\n\nProfil d’écoulement en conditions stressée sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions stressée sans déformation\")\n\n\n\n\n\n\n\n\n\nUn choc sur la profondeur de marché a pour effet un rallongement de la durée minimale de liquidation du fonds. Toutefois, les poids restent bien stables."
  },
  {
    "objectID": "risques/projets/demo.html#risque-de-crédit-et-risque-de-taux",
    "href": "risques/projets/demo.html#risque-de-crédit-et-risque-de-taux",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque de crédit et risque de taux",
    "text": "Risque de crédit et risque de taux\nLe risque de crédit est le risque de perte engendrée par la defaillance d’une partie prenante à remplir ses engagements contractuels préalablement établis. C’est principal risque observé sur périmètre du retail.\nOn définit le taux d’intérêt comme le loyer de l’argent (annualisé dans la pratique).\nCette définition est celle historique car très intuitive. Il peut cependant s’averer que les taux d’interêt soient négatifs. Là cette definition devient donc limitée.\nDepéndamment de ce qu’on fait de son argent, le thésauriser, le prêter à la banque , ou encore à l’etat, il existe toujours un risque de perte qui subsiste. Il peut donc advenir, que dans le souci de sécuriser son argent, dans un contexte particulier (notamment incertain), le posseusseur soit disposé à payer pour securiser son argent : on parle de taux d’intérêt négatif.\nCela illustre la métaphore du loyer du coffre fort.\n\nGénéralités sur le pricing d’obligations\nCONTEXTE\nEn raison des obligations réglementaires auxquelles les banques sont soumises, elles ne peuvent pas prêter à toutes les entreprises ayant besoin de financement. C’est dans ce contexte que la notion d’obligation prend son sens. La banque agit alors en tant qu’intermédiaire entre l’entreprise et le marché, et perçoit des frais de commission. le marché prête un montant M à l’entreprise et reçoit des annuités et le nominal à maturité.\nUne obligation est, économiquement, un prêt-emprunt.\nDe manière générale, la valorisation d’un actif est l’espérance des flux actualisés au taux sans risque sous la probabilité risque neutre :\n\\[\n\\begin{aligned}\nX_0 &= \\mathbb{E}[e^{-rT} X_T] \\\\\n    &= e^{-rT} \\, c \\times PS(T)\n\\end{aligned}\n\\]\nN.B : Le taux de recouvrement historique est de 40 %.\nLe recouvrement s’applique uniquement au nominal.\nLa probabilité de survie \\(PS(T)\\) est généralement déterminée à partir du modèle à intensité de Poisson via :\n\\[\nPS(T) = e^{-\\lambda T}\n\\]\nConsidérons une obligation d’échéances \\(T_i\\), \\(i = 1, \\dots, n\\), de coupon \\(c\\) et de nominal \\(N\\).\nLes coupons et le nominal sont payés en cas de survie, et le recouvrement en cas de défaut.\nLa valeur de cette obligation à la date \\(t\\) vaut :\n\\[\nC_t = \\sum_{i=1}^{n} c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}}\n\\]\nLa probabilité de survenue du défaut à une date \\(t\\) vaut :\n\\[\n\\begin{aligned}\nPD(t)\n    &= PS(t) - PS(t + dt) \\\\\n    &= -\\frac{PS(t+dt) - PS(t)}{dt} \\, dt \\\\\n    &= -\\frac{dPS(t)}{dt} \\, dt \\\\\n    &= \\lambda e^{-\\lambda t} \\, dt\n\\end{aligned}\n\\]\nLa valeur actualisée du recouvrement vaut :\n\\[\n\\mathcal{R}_0\n= \\int_0^T R \\lambda e^{-\\lambda t} e^{-rt} \\, dt\n= \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\]\nDe manière générale, pour une date \\(t\\) :\n\\[\n\\mathcal{R}_t\n= \\int_t^T R \\lambda e^{-\\lambda u} e^{-ru} \\, du\n= \\lambda R \\, e^{(r+\\lambda)t}\n  \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n  \\mathbf{1}_{\\{T \\ge t\\}}\n\\]\nLa valeur totale de l’obligation est alors :\n\\[\n\\begin{aligned}\nB_t\n&= \\sum_{i=1}^{n}\n    c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}} \\\\\n&\\quad +\n\\left(\ne^{-(r+\\lambda)(T_n - t)}\n+\n\\lambda R \\, e^{(r+\\lambda)t}\n\\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\right)\n\\mathbf{1}_{\\{T_n \\ge t\\}}\n\\end{aligned}\n\\]\n\n\nImpementation de la valorisation d’un bond\n\n\nCode\n# Impementation de la fonctiuon de valorisation d'un bond\nimport numpy as np\n\ndef Bond(t,c,T,r,lamda, R = 0.4):\n    B = 0\n    for T_i in range(1,T+1):\n        B += np.exp(-(lamda + r)*(T_i - t))*(T_i&gt;=t)\n    B *= c\n    B += (np.exp(-(r + lamda)*(T_i-t)) + lamda * R  * (1-(np.exp(-(r + lamda)*(T_i -t)))) / (r + lamda))*(T_i&gt;=t)\n    return B\n\n\n\nExemple: Obligation au pair\n\nlamda = 0\n\n\nr = 0.02\n\n\nc = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nBond(t,c,T,r,lamda)\n\n\nCode\n# Exemple1: Obligation au pair\nlamda = 0\nr = 0.02\nc = 0.02\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.9981933497987289)\n\n\nAvec les paramètres ci-dessus considérés, on remarque que le prix de l’obligation est proche du nominal. En effet, le taux coupon est égal au taux de marché, ce qui indique que l’obligation est remunérée au taux du marché. Il s’agit donc d’une obligation au pair.\nAu cas où on aurait proposé une remunération supérieure à celle du marché, elle serait beaucoup plus attractive et sa valeur se serait appréciée. l’illustration est donnée ci dessous pour c= 0.03\n\n\nBond(0,0.03,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.03,10,0.02,0)\nprice\n\n\nnp.float64(1.0879246481591023)\n\n\nAvec une remunération inférieure à ce qu’aurait proposé le marché, on obtient une valorisation de l’obligation inférieure au nomimal\n\n\nBond(0,0.015,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.015,10,0.02,0)\nprice\n\n\nnp.float64(0.9533277006185421)\n\n\nEtant donné une intensité de défaut supérieure à zéro, on peut également calculer le coupon pour lequel l’obligation est au pair. On obtient, après calculs prsentés ci-dessous, un taux coupon de 2.6%.\n\n\nCode\n# Recherche du coupon  pour émettre une obligation au pair.\ndef dichot(t,T,r,lamda, P_MKT):\n    c_inf = 1e-8\n    c_sup = 1\n    epsi = 1e-8\n    c_moy = (c_inf + c_sup)/2\n    error = c_sup - c_inf\n\n    while error&gt;epsi:\n        p_hw = Bond(t,c_moy,T,r,lamda)\n        if p_hw &gt; P_MKT:\n            c_sup = c_moy\n        elif p_hw &lt; P_MKT:\n            c_inf = c_moy\n        c_moy = (c_inf + c_sup)/2\n        error = np.abs(c_sup - c_inf)\n\n    return c_moy\n\n\n\n\n\nTaux coupon pour émettre une obligation au pair\n\nlamda = 0.01\n\n\nr = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nCode\n# Taux coupon pour émettre une obligation au pair\nlamda = 0.01\nr = 0.02\nt = 0\nT = 10\ndichot(t,T,r,lamda, P_MKT = 1)\n\n\n0.026393926193952293\n\n\nEn maintenant une remunération égale à celle du marché , avec une intensité de défaut très grande (de l’ordre de 1000%), l’obligation tombe presque instantannément en défaut. Dans cette situation, la valeur du coupon vaut alors 39.92% qui est sensiblement proche du taux de recouvrement (40%).\n\n\n\nValeur du bond poiur une intensité de défaut à 1000%\n\n\nCode\n# Valeur du bond poiur une intensité de défaut à 1000%\nlamda = 10\nr = 0.02\nc = 0.03\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.39920293189432754)\n\n\n\n\n\nÉvolution du prix de l’obligation en fonction du temps\n\n\nCode\nimport matplotlib.pyplot as plt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB = np.array([Bond(t,c,T,r,lamda) for t in times])\nplt.plot(times,B)\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.title(\"Evolution du prix plein coupon du Bond\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nPlus on se rapproche de la date de détachement du coupon plus l’obligation devient attractive, elle prend donc de la valeur.\nChaque saut correspond à un détachement de coupons. une fois le coupon détaché de l’obligation, les flux à venir diminuent et la valeur de l’obligation se deprecie de la valeur du coupon qui a été détaché.\nCes sauts ne reflètent donc pas une dépréciation des bonds par le marché. Ce sont des sauts techniques. Raison pour laquelle on dit que le prix plein coupon est pollué par le coupon (en anglais Dirty price).\nOn va donc s’interesser par la suite au clean price ou pied de coupon qui correspond à au prix du bond moins le coupon couru.\n\\[\n\\tilde{B}_t = B_t - cc\n\\]\nOù (cc) est le coupon couru :\n\\[\ncc = c \\times (t - T^*)\n\\]\nEn retirant cette valeur de coupon couru, on supprime cet effet de saut après les detachements de coupons\n\nImplémentation du clean price\n\n\nCode\n# Implémentation du clean price\n\ndef CleanPrice(t,c,T,r,lamda, R = 0.4):\n    cc = c*(t - np.floor(t))*(t&lt;=T)\n    return Bond(t,c,T,r,lamda, R)-cc\n\n\n\n\nPieds coupon\n\n\nCode\n# Pieds coupon\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for t in times])\nB_pieds_coupon = np.array([CleanPrice(t,c,T,r,lamda) for t in times])\nplt.plot(times,B_pieds_coupon, label =\"Pieds coupon\")\nplt.plot(times,B_plein_coupon, label =\"Plein coupon\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nEn retirant cette valeur de coupon couru, on supprime l’effet de saut après les detachements de coupons, et on observe une courbe sans discontinuités\n\n\n\nEvolution du prix plein coupon et du clean price en fonction du temps pour differents coupons\n\n\nCode\nlamda = 0.01\nr = 0.02\nc = 0.01\nT = 10\n\ntimes = np.linspace(0,10, num = 100)\nB_plein_coupon1 = np.array([Bond(t,0.01,T,r,lamda) for t in times])\nB_pieds_coupon1 = np.array([CleanPrice(t,0.01,T,r,lamda) for t in times])\n\nB_plein_coupon5 = np.array([Bond(t,0.05,T,r,lamda) for t in times])\nB_pieds_coupon5 = np.array([CleanPrice(t,0.05,T,r,lamda) for t in times])\n\nplt.plot(times,B_pieds_coupon1, label =\"Pieds coupon 1%\")\nplt.plot(times,B_plein_coupon1, label =\"Plein coupon 1%\")\n\nplt.plot(times,B_pieds_coupon5, label =\"Pieds coupon 5%\")\nplt.plot(times,B_plein_coupon5, label =\"Plein coupon 5%\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nLes constats que l’on fait : - Lorsque c = 1% : La rémunération de l’obligation est inférieure à celle offerte par le marché. Ainsi, lors de son émission, sa valeur sera nécessairement inférieure à celle du pair, ce qui explique une valeur initiale proche de 87 %. À maturité, on reçoit 100 % du nominal, plus 1 % de ce dernier, correspondant au coupon. Le résultat final est donc sensiblement égal à 101 %.\n\nLorsque c = 5% : L’obligation rémunère plus que ce que le marché offre, ce qui la rend particulièrement attractive dès son émission. Ainsi, sa valeur de départ sera d’environ 120 %. Au fur et à mesure que les obligations sont détachées, les flux futurs diminuent, ce qui entraîne une dépréciation de sa valeur. À maturité, on reçoit 100 % du nominal, plus 5 % du montant nominal, correspondant au coupon. Le résultat final est donc de 105 %.\n\n\n\nEvolution du prix de l’obligation en fonction du taux d’intérêt\nDe l’expression analytique du prix du bond, on observe que le prix est décroissant du taux d’intérêt. La figure ci-dessous en donne une illustration, toute chose égale par ailleurs.\nOn vérifie graphiquement que le prix du bond est de 100% lorsque le taux d’intérêt est égal au taux sans risque \\(r^* = c - \\lambda (1 - R)\\).\n\nEvolution du prix du bond en fonction du taux d’intérêt\n\n\nCode\n# Evolution du prix du bond en fonction du taux d'intérêt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR = 0.4\ninterest = np.linspace(0,0.10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest,B_plein_coupon, label =\"Plein coupon\")\nplt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\nplt.xlabel(\"Taux d'intérêt (%)\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_30568\\2938139823.py:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\n\n\nText(0.5, 1.0, \"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotion de sensibilité\nLa sensibilité mesure la variation du prix du bond face à une variation d’un facteur de risque. On distingue entre autres la sensibilité de taux et la sensibilité de crédit.\nDans cette sectioin nous nous intéressons particulièrement à la sensibilité de taux. Elle est définie par la formule:\n\\[\n    Sensibilité = - \\frac{dB_t}{dr} \\frac{1}{B_t}\n\\]\nInterprétation: Lorsque le taux d’intérêt bouge de 1%, alors le prix du bond bouge de -sensibilité %.\nCette sensibilité peut également être vue comme le barycentre des différentes échéances pondérées par les flux actualisés. C’est la duration.\n\\[\n    \\frac{\\sum T_i\\times F_i}{\\sum F_i}\n\\]\nExemple: En considérant une obligation de maturité 3 ans payant des coupons annuels, avec intensité de défaut nul. Alors son prix et la sensibilité de taux sont données par:\n\\[\n\\begin{aligned}\nP &= c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial P}{\\partial r}\n&= -1c \\, e^{-1r} - 2c \\, e^{-2r} - 3c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Sensibilité}\n&= \\frac{1c \\, e^{-1r} + 2c \\, e^{-2r} + 3c \\, e^{-3r}}\n        {c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}}\n\\end{aligned}\n\\]\nD’où l’expression barycentrique des échéances pondérées par les flux.\n\n\nCode\n# Sensibilité de taux du prix du bond\ndelta_r = 1e-4 #1bp\n\n\ndef Sensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    return -(Bond(t,c,T,r+delta_r,lamda, R)-B_t)/(delta_r*B_t)\n\n\nOn considère un bond dont les caractérisiques sont les suivqantes:\n\\[\n    \\begin{cases}\n        \\lambda = 0.01\\\\\n        r = 0.02\\\\\n        c = 0.03\\\\\n        T = 10\\\\\n        R = 0.4\\\\\n    \\end{cases}\n\\]\nLa sensiblité de ce bond est de 8.64. Ainsi, lorsque le taux d’intérêt augmente de 1 point de pourcentage, le prix du bond diminue de 8.64%.\n\n\nCode\n# Sensibilité de taux d'intérêt\nSensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4)\n\n\nnp.float64(8.643982489105056)\n\n\n\nEvolution de la sensibilité de taux fonction de la maturité\n\n\nCode\n# Evolution de la sensibilité au taux d'intérêt en fonction de la maturité\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,c,T,r,lamda) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité')\n\n\n\n\n\n\n\n\n\n\nOn constate que la sensibilité de taux croît avec la maturité et tend à être linéaire.\nPour des paramètres extrêmes \\(c = \\lambda = r = 0\\), la sensibilité (duration) est identique à la maturité comme illustré sur la figure ci-dessous. Cette remarque met en évidence la relation mlathématique entre la sensibilité et la maturité présentée plus haut.\nConnaissant la maturité, pour une variation du taux d’intérêt on peut donc donner une estimation “grossière” de la sensibilité (en approximant la duration par 0.8*maturité, par exemple).\n\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,1e-6,T,1e-6,1e-6) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité (cas extrême)\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité (cas extrême)')\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR d’une obligation.\nL’estimation de la VaR sur un bond peut se faire à partir de la senssibilité de taux ou par la méthode de repricing.\n\n\nEstimation de la VaR par la sensibilité du taux d’intérêt\nL’approche par la sensibilité se présente de la manière suivante:\nOn suppose que la dynamique du taux d’intérêt est donnée par \\[\\Delta r \\sim \\mathcal{N}(0, \\sigma \\sqrt{\\Delta t})\\].\nSachant que \\[\\frac{\\Delta P}{P} = - Duration \\times \\Delta r\\]\nil suit que \\[\\frac{\\Delta P}{P} \\sim \\mathcal{N}(0, Duration \\times \\sigma \\sqrt{\\Delta t})\\]\nUne approche par la sensibilité de la VaR à 99% donne \\[VaR = Duration \\times \\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\\] Où \\(z_{99\\%}\\) est le quantile d’ordre 99% de la loi normale standard.\nPour les besoins de notre exercice, on pose \\(\\sigma = 1\\%\\).\n\n\nCode\nfrom scipy.stats import norm\n\n# Calcul de la VaR par la sensibilité du taux d'intérêt\ndef Sensi_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    duration = Sensi(t,c,T,r,lamda, R)\n    var = duration*sigma*np.sqrt(H)*norm.ppf(alpha)\n    return var\n\n\n\n\nCode\nsigma= 0.01\nH = 1/12\nt = 0\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR =0.40\n\nSensi_VaR(sigma , H, t,c,T,r,lamda)\n\n\nnp.float64(0.05804942383590022)\n\n\nLa VaR à 99% d’horizon 1 an sur le bond est de 5.8%. - On note que la VaR est une fonction linéaire croissante de la volatilité. C’est une conséquence directe de l’hypothèse de normalité des variations du taux d’intérêt. Ainsi, une augmentation de la volatilité du taux d’intérêt s’accompagne d’une augmentation de la volatilité du prix du bond. - La VaR est une fonction décroissante du taux d’intérêt. En effet, une augmentation du taux d’intérêt entraîne une diminution de la valeur des bonds, et par conséquent des valeurs extrêmes atteintes par celles-ci; d’où la VaR décroît. Toutefois l’évolution de la VaR en fonction du taux d’intérêt n’est pas linéaire.\n\n\nEvolution de la Z-VaR en fonction du taux d’intérêt\n\n\nCode\n# Evolution de la VaR en fonction de la volatilité\nvol = np.linspace(0,1e-2, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for sigma in vol])\n\nplt.plot(vol, var, label =\"VaR_99%\")\nplt.xlabel(\"Volatilité\")\nplt.ylabel(\"VaR \")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de la volatilité\")\n\n\nText(0.5, 1.0, 'VaR à 99% à horizon 1 an en fonction de la volatilité')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Evolution de la VaR en fonction du taux d'intérêt\ninterest = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nEvolution de la Z-VaR en fonction de l’intensité de défaut\nPour des intensités de défaut croissantes, la VaR du bond décroît de façon exponentielle, ceteris paribus. Tout comme avec le taux d’intérêt, une augmentation de l’intensité de défaut s’accompagne d’une imminence du défaut et par conséquent de la diminution de la valeur du bond. D’où une diminution de la VaR.\n\n\nCode\n# Evolution de la VaR en fonction de l'intensité de défaut\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de défaut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR par repricing de l’obligation\nLa méthodologie consiste à revaloriser le bond pour une variation du taux d’intérêt, puis d’en déduire la variation du prix du bond qui en résulte.\n\\[\n    \\Delta r^* =\\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\n\\]\n\\[\n    VaR_{99\\%} = \\frac{B(r+ \\Delta r^*) - B(r)}{B(r)}\n\\]\n\n\nApproche repricing\n\n\nCode\n# Approche repricing\n\ndef repricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    delta_r = sigma*np.sqrt(H)*norm.ppf(alpha)\n    B_rep = Bond(t,c,T,r+delta_r,lamda, R)\n    B_r = Bond(t,c,T,r,lamda, R)\n    \n    return  (B_r - B_rep)/B_r\n\n\n\n\nCode\nrepricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99)\n\n\nnp.float64(0.05627224378244837)\n\n\n\nLa VaR obtenue par l’approche repricing, sous les mêmes conditions que précédemment, est de 5.6%. Cette VaR est inférieure à celle obtenue sous l’hypothèse de normalité des variations du taux d’intérêt.\nCeci s’explique par le fait que la duration est une fonction convexe décroissante du taux d’intérêt et représente une approximation affine de la valeur du bond en le taux d’intérêt. De manière générale, l’approche par les sensibilités surestime la VaR.\nL’évolution de la VaR, obtenue par reprincing, en fonction du taux d’intérêt ou de l’intensité de défaut (voir les graphiques ci-dessus) à la même allure que la z-VaR.\nEn règle générale la VaR doit être inférieure au seuil de 20% de par la limite règlementaire. L’utilisation de la z-VaR peut donc constituer un manque à gagner pour les institutions financieres. En effet, elles peuvent être amenées à dérisquer leur portefeuille en limitant leur investissements pour des raisons purement techniques.\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\ninterest = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda)for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de saut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")"
  },
  {
    "objectID": "risques/projets/demo.html#risque-de-contrepartie",
    "href": "risques/projets/demo.html#risque-de-contrepartie",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque de contrepartie",
    "text": "Risque de contrepartie\nDans le cadre d’un engagement contratuel incluant deux parties prenantes, le risque de contrepartie fait reférence au rique de perte suite au défaut d’une des parties à remplir les engagements contractuels pré-établis: c’est un risque bilatéral.\nAfin de se couvrir du defaut de l’émetteur d’un bond, l’acheteur du bond peut entrer dans un credit default swap (CDS). Dans un tel contrat, lorsque la contrepartie est correlée à l’émetteur de l’obligation on parle de wrong way risk ce qui a pour conséquence d’exposer davantage le souscripteur. La contrepartie doit donc être décorrelée de l’émetteur.\nLe risque de contrepartie est en général mitigé par les appels de marge (chambre de compensation). - Quand la qualité de l’émetteur se dégrade, son spread de crédit spread croît et le CDS s’apprécie. - Si la contrepartie fait défaut, le détenteur perd \\(CDS = 1-R\\). - Un mécanisme d’appel de marge permet de mitiger le risque.\n\nLien entre CDS et obligation: la formule du triangle de crédit\nUn estimation du taux sans risque d’une obligation: \\[\n\\begin{aligned}\nP &= \\sum_{i=1}^{n} c \\, e^{-(r+\\lambda)T_i}\n    + e^{-(r+\\lambda)T_n}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\int_0^T e^{-(r+\\lambda)t} \\, dt\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\nL’obligation est au pair si et seulement si P = 1 Ainsi, \\[\n    (c + \\lambda R) \\frac{ 1- e^{-(r + \\lambda )T}}{r + \\lambda} + e^{-(r + \\lambda)T} = 1\n\\]\nQuand T tend vers l’infini le taux sans risque vaut alors, \\[\n    (c + \\lambda R)  = r + \\lambda\n\\]\n\\[\n    (c - r )  = \\lambda (1 - R)\n\\] - \\(c-r\\): spread ou prime de risque - \\(\\lambda\\): intensité de défaut - \\(1-R\\): LGD \\[\n    Spread = PD \\times LGD\n\\]\n\n\nCDS: principe\n\nPremium Leg: est payé tant que l’émetteur est “en vie”. \\[ PL = \\int_0^T s \\times e^{-rt} \\times e^{-\\lambda t}dt = s \\frac{1-e^{-(r+\\lambda)T}}{r + \\lambda}\\]\nDefault Leg: compensation à la date t si le défaut y survient. \\[DL = \\int_0^T(1-R) e^{-rt}\\times e^{-\\lambda t} dt = \\lambda (1-R) \\frac{1-e^{-(r+\\lambda)T}}{r+\\lambda}\\].\n\nLa valeur initiale d’un spread étant nulle, les deux jambes sont égales et il on obtient:\n\\[\n    s = \\lambda (1-R)\n\\]\nUn estimation de ce spread peut donc être déduite de la formule du triangle de crédit.\n\n\nSensibilité de crédit\nLa sensibilité de crédit d’une obligation est la variation du prix de l’obligation lorsque varie le spread de crédit de la contrepartie. De la formule du triangle, la sensibilité de crédit peut être déduite de la sensibilité par rapport à l’intensité de défaut \\(\\lambda\\).\n\\[\n    \\frac{\\partial P}{\\partial s} = \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda}\n\\]\n\\[\n    \\frac{\\partial P}{\\partial s} \\times \\frac{1}{P}= \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda} \\times  \\frac{1}{P}\n\\]\nOn peut calculer une VaR taux et une VaR crédit. Toutefois, la VaR qui sera regardée de près est celle issue de la variation conjointe des taux et des spreads de crédit.\n\nSensibilité de crédit du bond\n\n\nCode\n# Sensibilité de crédit du bond\ndelta_lambda = 1e-4 #1bp\n\ndef Sensi_credit(t,c,T,r,lamda, R = 0.4, delta_lambda = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    variation = -(Bond(t,c,T,r,lamda+delta_lambda, R)-B_t)/(delta_r*B_t)\n    return variation/(1-R)\n\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda)\n\n\nnp.float64(8.82119086802735)\n\n\n\nUne variation d’un point de pourcentage du spread de crédit entraîne une variation de 8.82% de la valeur du bond. Cette sensibilité de crédit est assez proche de la sensibilité taux. En effet, on observe de l’écriture mathématique de la valeur du bond que le taux d’intérêt et l’intensité apparaissent conjointement de façon additive (sauf pour le terme de recouvrement).\nCes deux sensibilitées sont identiques pour un taux de recouvrement nul.\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda, R = 0.0, delta_lambda = 1e-4)\n\n\nnp.float64(8.77911216566412)\n\n\n\n\n\nEstimation de la VaR par variation conjointe du taux d’intérêt et du spread de crédit\nOn suppose que la dynamique du taux d’intérêt est donnée par l’EDS\n\\[\n    dr_t = \\sigma dW_t\n\\]\nLe spread ne prenant pas de valeur négative, on supposera qu’il est log-normal. Sa dynamique est donnée par l’EDS\n\\[\n    \\frac{ds_t}{s_t} = \\alpha dZ_t\n\\]\nEn supposant \\(dW_t dZ_t = \\rho\\) et \\(Z_t = \\rho W_t + \\sqrt{1-\\rho^2}V_t\\) tel que \\(dV_t dW_t = 0\\),\nLa dynamique du spread s’écrit alors \\[\n\\frac{ds_t}{s_t} = \\alpha (\\rho W_t + \\sqrt{1-\\rho^2}V_t)\n\\]\nEn appliquant un schéma de discrétisation d’Euler, on peut alors simuler les taux \\(r_t\\) et les spreads \\(s_t\\) par:\n\\[\n    r_H = r + \\sigma \\sqrt{H} W_H\n\\]\n\\[\n    s_H = s(1 + \\alpha \\rho \\sqrt{H} W_H + \\alpha \\sqrt{1-\\rho^2} \\sqrt{H} V_H)\n\\]\nOù H est le pas de discrétisation.\nLa méthodologie de détermination de la sensibilité du prix d’une obligation conjointement au taux d’intérêt et au spread de crédit est la suivante: - Calculer la valeur du bond pour les paramètres initiaux - Simuler les taux \\(r_H\\) et les spreads \\(s_H\\) - Pour chaque simulation, calculer la valeur du bond (cetertis paribus) - pour déterminer la variation des nouveaux prix par rapport au prix initial - calculer le quantile d’ordre 99% des variations obtenues\n\n\nCode\n# Sensibilité conjointe de taux et de crédit\n\ndef SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12, M=10_000):\n    \n    u1 = np.random.uniform(0,1, size = M)\n    u2 = np.random.uniform(0,1, size = M)\n    w = norm.ppf(u1)\n    v = norm.ppf(u2)\n    s = lamda*(1-R)\n    \n    r_H = r + sigma*np.sqrt(H)*w\n    s_H = s*(1 + alpha*rho*np.sqrt(H)*w + alpha*np.sqrt(1-rho**2)*np.sqrt(H)*v)\n    \n    P_0 = Bond(0,c,T,r,lamda, R)\n    P_1 = Bond(H,c,T,r_H,s_H/(1-R), R)\n    mu = (P_1 - P_0)/P_0\n    return  - np.quantile(mu,0.01)\n\n\n\n\nCode\nsigma = 0.01 # Volatilité du taux d'intérêt\nalpha =0.4   # Volatilité du spread\nrho = 0.4    # Corrélation entre le spread et le taux d'intérêt\n\nnp.random.seed(90) # Pour la reproductibilité\nSimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12)\n\n\nnp.float64(0.0602691861203251)\n\n\n\n\nCode\nnp.random.seed(90)\n\nrhos = np.linspace(0,1, num= 1000)\nvar = np.array([SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12) for rho in rhos])\n\nplt.plot(rhos, var)\n\n\n\n\n\n\n\n\n\nLa corrélation \\(\\rho\\) doit être positive, car une augmentation des taux d’intérêt entraîne une meilleure rémunération du marché par rapport aux obligations. Pour rester attractifs, les coupons doivent alors augmenter. Cette hausse des coupons accroît le coût de financement de l’émetteur, ce qui conduit à une augmentation du spread de crédit. Il en résulte une corrélation positive entre les taux d’intérêt et les spreads de crédit.\nUne augmentation conjointe des taux et des spreads accroît le risque de défaut, en raison de la corrélation positive entre ces deux facteurs de risque. Ainsi, une corrélation élevée reflète un manque de diversification, ce qui expose davantage le portefeuille au risque systémique"
  },
  {
    "objectID": "risques/projets/demo.html#risque-modèle",
    "href": "risques/projets/demo.html#risque-modèle",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque modèle",
    "text": "Risque modèle\nLe risque modèle désigne le risque associé à l’utilisation d’un modèle mathématique ou statistique qui ne représente pas correctement la réalité ou qui mène à des décisions erronées en finance et en gestion des risques.\nEléments du suivi du risque modèle: - Sanity check: Ecart de performance - Backtesting: Application aux données antérieures - Comparaison avec desmodèles plus riches - Provisionner au titre du risque de modèle"
  },
  {
    "objectID": "risques/projets/demo.html#risque-climatique",
    "href": "risques/projets/demo.html#risque-climatique",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Risque climatique",
    "text": "Risque climatique\nLe risque climatique désigne les risques financiers liés au changement climatique et aux politiques de transition vers une économie bas carbone. Sa modélisation reste complexe, notamment en raison du manque de données historiques et de l’absence d’un cadre réglementaire clairement défini.\nOn distingue deux principaux types de risques climatiques :\n\nRisque physique: l correspond aux pertes financières causées par des événements climatiques extrêmes. Il est défini par deux éléments clés : la fréquence et la sévérité des événements.\n\nRisque aigu : Événements rares mais très intenses ( ouragans, inondations).\nRisque chronique : Changements progressifs et durables (élévation du niveau de la mer).\n\nRisque de transition: il découle de l’adaptation des entreprises et des acteurs économiques aux exigences de transition écologique.\n\nRisque politique et réglementaire : Durcissement progressif des régulations environnementales (taxe carbone).\nOpportunités technologiques : Innovations favorisant la transition énergétique et redéfinissant les modèles économiques.\n\n\nLa resilience des institutions financieres notamment aux changement climatique est évaluée via des stress tests climatiques dont les scénarios sont issus des données du NGFS (Network for Greening financial System)."
  },
  {
    "objectID": "risques/projets/demo.html#introduction",
    "href": "risques/projets/demo.html#introduction",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "",
    "text": "Ce document présente une introduction à la gestion des risques en Asset Management.\nL’objectif principal est de comprendre et d’appliquer les méthodologies de base utilisées pour analyser, mesurer et suivre les risques financiers au sein d’un portefeuille.\nL’étude couvre differentes notions de risk management notamment:\n- les méthodes de mesure du risque de marché (sensibilité, volatilité, Value-at-Risk, Tracking Error, stress tests et profil de liquidité),\n- les composantes clés du risque de crédit (intensité de défaut, sensibilité des obligations aux variations de spreads, triangle du crédit),\n- le risque de contrepartie et ses mécanismes de mitigation via les appels de marge,\n- ainsi que les risques émergents tels que le risque climatique et le risque modèle.\n\n\nCode\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import norm\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize"
  },
  {
    "objectID": "risques/projets/demo.html#i.-risques-de-marché",
    "href": "risques/projets/demo.html#i.-risques-de-marché",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "I. Risques de marché",
    "text": "I. Risques de marché\n\nI.1. Construction d’un portefeuille diversifié\nOn construit un portefeuille constitué de 10 entreprises du CAC40. Nous avons choisi des entreprises de secteurs variés afin de s’assurer que le portefeuille soit diversifié.\nAinsi, notre portefeuille est constitué des entreprises des secteurs suivants :\n\nBanque et assurance : BNP Paribas (BN.PA), Crédit Agricole (CA.PA)\n\nAéronautique et défense : Airbus (AIR.PA)\n\nTechnologie: Capgemini (CAP.PA)\nConcession et Construction: Vinci (DG.PA)\nTélécommunications : Orange (ORA.PA)\n\nÉnergie : Engie (ENGI.PA)\n\nLuxe & Consommation : LVMH (MC.PA)\n\nMédias & Divertissement : Vivendi (VIV.PA)\n\nIndustrie Chimique : Air Liquide (AI.PA)\n\nNous allons procéder à une optimisation de Markowitz visant à construire un portefeuille qui minimise la volatilité (risque) sous la contrainte d’un niveau de rendement modulable en fonction du profil de risque de l’investisseur.\n\n\nCode\n# On recupère les données sur un historique de deux ans\n\nend_date = '2025-03-16'  \nstart_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=2*365)).strftime('%Y-%m-%d')\ntickers = ['AI.PA', 'AIR.PA', 'BN.PA', 'CA.PA', 'CAP.PA', 'DG.PA', 'ENGI.PA', 'ORA.PA', 'SAN.PA', 'VIV.PA']\n\ndf = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\ncac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\388541553.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\n[                       0%                       ][                       0%                       ][**************        30%                       ]  3 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************80%*************          ]  8 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\388541553.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\ndf_close = df['Close']\ndf_volume = df['Volume']\n# Calcul des  rendements journaliers\nreturns = df_close.pct_change()\nreturns.dropna(inplace=True)\n\n\n\nAperçu des données\n\n\nCode\n#Aperçu des données\ndf.head(3)\n\n\n\n\n\n\n\n\nPrice\nClose\nVolume\n\n\nTicker\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-17\n126.875404\n110.644333\n49.379742\n14.572609\n157.424210\n91.923409\n10.256954\n8.935349\n79.612030\n8.390329\n1971502\n2171272\n3041249\n4474936\n719990\n3575676\n12989586\n20039590\n5122220\n7405822\n\n\n2023-03-20\n129.684891\n112.407204\n49.742428\n14.783135\n157.659653\n93.078445\n10.359279\n9.040172\n79.310608\n8.565088\n1139355\n1290939\n1697298\n2379408\n418826\n1207887\n7087933\n9795603\n2119074\n2624806\n\n\n2023-03-21\n129.288467\n115.250557\n50.186718\n14.955767\n160.579285\n95.104210\n10.579202\n9.144996\n79.948921\n8.741748\n745105\n1002062\n2018104\n2130100\n293112\n934999\n7313789\n9077226\n1449009\n2962587\n\n\n\n\n\n\n\nComme indicateur quantifiable de cette diversification nous presentons ci après la matrice de correlation entre les rendements journaliers des actifs du portefeuille pour notre période d’étude.\n\n\nCode\n# Calcul de la matrice de corrélation\ncorr_actifs = returns.corr().to_numpy()\n\n# Affichage du corrélogramme\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_actifs, annot=True, fmt='.2f', cmap='coolwarm', xticklabels=returns.columns, yticklabels=returns.columns)\nplt.title(\"Corrélogramme des actifs\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nI.1.1 Présentation du principe d’optimisation de Markovitz\nL’optimisation de Markowitz, également connue sous le nom de théorie moderne du portefeuille (MPT), permet de construire un portefeuille optimal en combinant des actifs de manière à minimiser la volatilité (ou le risque) tout en respectant une contrainte sur un niveau de rendement fixé. L’un des éléments clés de cette théorie est la détermination des poids de chaque actif dans le portefeuille, afin d’atteindre un compromis optimal entre le risque et le rendement.\nHypothèses principales :\n\nLes décisions sont prises en fonction du couple rendement-risque.\n\nLes investisseurs cherchent à diversifier leurs actifs pour réduire le risque.\n\nL’accès au taux sans risque est illimité.\n\nAucun coût de transaction n’est pris en compte.\n\nProgramme d’optimisation : L’objectif est de minimiser la volatilité sous la contrainte d’un rendement cible $ _T $ :\n\\[\n\\text{Minimiser } \\omega^t\\Sigma \\omega\n\\] Sous les contraintes :\n$$\n\\[\\begin{cases}\n    \\omega^t \\mu = \\mu_T \\\\\n    \\sum_{i=1}^{n} \\omega_i = 1\n\n\\end{cases}\\]\n$$\n\n\nI.1.2 Calcul des rendements annualisées et des volatilités annualisés des log_rendements pour chaque actif\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nI.1.3. Optimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nI.1.3. Optimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\n \n# Nombre d'actifs\nn_assets = len(yearly_return)\nmu = yearly_return\n\ndef portfolio_variance(weights):\n    return weights.T @ sigma @ weights\n\ndef weight_sum_constraint(weights):\n    return np.sum(weights) - 1\n\ndef target_return_constraint(weights, mu_target):\n    return weights.T @ mu - mu_target\n\ndef efficient_portfolio(mu_target, range_=(-0.1, None), positive=True):\n    # On initialise les poids de maniere équitable\n    init_weights = np.ones(n_assets) / n_assets\n\n    # Contraintes\n    constraints = [\n        {'type': 'eq', 'fun': weight_sum_constraint},\n        {'type': 'eq', 'fun': lambda w: target_return_constraint(w, mu_target)}\n    ]\n\n    # On verifie si on a définit une contrainte de positivité\n    if positive:\n        bounds = [(0, None) for _ in range(n_assets)]  # Chaque poids doit être &gt;= 0\n    else:\n        bounds = [range_ for _ in range(n_assets)] \n\n    # Optimisation\n    result = minimize(portfolio_variance, init_weights, bounds=bounds, method='SLSQP', constraints=constraints)\n\n    if result.success:\n        optimal_weights = result.x\n        portfolio_volatility = np.sqrt(portfolio_variance(optimal_weights))\n        return portfolio_volatility, optimal_weights\n    else:\n        return None, None\n\n\n\n\nI.1.4 Illustration pour un rendement cible de 10%\nPour un rendement annuel cible de 10%, correspondant à l’ordre de grandeur des rendements du CAC40 qui représente le benchmark du fond, les poids optimaux donnés par le l’optimisation de Markovitz et qui ont permis de minimiser le risque auquel est exposé le porte feuille ( 9.67% de volatilité) sont donnés ci après:\n\n\nCode\n# Rendement cible\nmu_target = 0.1 \n\n# les poids optimaux et la volatilité\nvol, weights = efficient_portfolio(mu_target)\nprint(\"Le portefeuille ainsi construit a un rendement de\",round(weights.T @ mu,4), \"et est soumis à une volatilité de l'ordre de :\", round(vol,4), \"\\n\")\nprint(\"---\"*50)\nprint(\"Les poids associés aux actifs du porte feuille sont listés ci après:\\n\")\ni = 0\nfor column in df_close.columns.tolist():\n    print(f\"{column}, {weights[i]:.4f}\")\n    i += 1\n\n\nLe portefeuille ainsi construit a un rendement de 0.1 et est soumis à une volatilité de l'ordre de : 0.0989 \n\n------------------------------------------------------------------------------------------------------------------------------------------------------\nLes poids associés aux actifs du porte feuille sont listés ci après:\n\nAI.PA, 0.0243\nAIR.PA, 0.0382\nBN.PA, 0.2031\nCA.PA, 0.1929\nCAP.PA, 0.1255\nDG.PA, 0.0000\nENGI.PA, 0.0732\nORA.PA, 0.2652\nSAN.PA, 0.0542\nVIV.PA, 0.0234\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), weights*100, color='skyblue')\n\nplt.title(f\"Poids optimaux des actifs dans le portefeuille (Volatilité: {round(vol, 4)})\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Poids (en %)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, weights):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nOn constate que le portefeuille optimal est assez diversifié.Les trois principales entreipes representées sont :\n\nTélécommunications : Orange (26.53%)\nBanque et assurance: BNP Paribas (20.24%), Credit Agricole (19.27%)\nTechnologie : Capgemini (12.52%)\nÉnergie : Engie (7.30%)\n\n\n\n\nI.2. Suivi du porte feuille\nDans cette section on va determiner l’AUM du portefeuille, suivre ses performances sa volatilité et ses performances relatives par rapport au CAC40.\nNous effectuerons également un stress test sur le portefeuille pour ebaluer sa sensibilité relative à la période COVID.\n\n\nCode\n# Poids des actions dans le portefeuille\nweights = pd.DataFrame({'Ticker': tickers ,\n     'Weight': weights})\nweights = weights.set_index('Ticker')['Weight']\n\n#print(weights)\n\n\n\nI.2.1. AUM (Asset under management)\nElle représente la valeur du porte feuille et est déterminée par la formule suivante\nAUM = (NAV)(Nombre_de_ parts)\nAvec\nLa Net Value Asset (NAV) représente la valeur nette d’une part du fonds (portefeuille) à un moment donné.\n\nDétermination de l’AUM\n\n\nCode\n# Détermination de l'AUM\naum = df_close@weights\nportfolio = pd.DataFrame({'AUM': aum})\nportfolio.head()\n\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-03-17\n47.527177\n\n\n2023-03-20\n47.829467\n\n\n2023-03-21\n48.500885\n\n\n2023-03-22\n48.360237\n\n\n2023-03-23\n48.621806\n\n\n\n\n\n\n\n\n\nCode\nportfolio.describe()\n\n\n\n\n\n\n\n\n\nAUM\n\n\n\n\ncount\n509.000000\n\n\nmean\n53.563795\n\n\nstd\n3.084850\n\n\nmin\n47.527177\n\n\n25%\n50.547954\n\n\n50%\n53.835647\n\n\n75%\n56.161594\n\n\nmax\n59.243527\n\n\n\n\n\n\n\nSur notre période d’etude, l’AUM en moyenne est de 59.10, et oscille entre 50.68 et 66.84.\n\n\n\nI.2.2 Performance du portefeuille\nDans le cadre du suivi de notre portfeuille, il est important d’identifier et de mesurer les risques auxquels il est exposé afin de mieux les encadrer. Dans cette section nous intéressons tout particulièrement au risque de marché. Ce risque peut être évalué à l’aide de:\n\nLa tracking error\nLa volatilité\nLa VaR\n\nqui sont des mesures ex-ante.\n\nOn s’intéresse aux performances du portefeuille construit ainsi que celles du benchmark\n\n\nCode\n# Performance du portefeuille\nportfolio['perf'] = portfolio['AUM'].pct_change()\nportfolio.dropna(inplace=True)\n\n# Performance du benchmark\ncac40_df = cac40['Close']\ncac40_df ['perf'] = cac40_df.pct_change()\ncac40_df.dropna(inplace=True)\n\nprint(\"Rendement annualisé du fond: \\t\", (1+portfolio.perf.mean())**252 - 1)\nprint(\"Rendement annualisé du CAC40: \\t\", (1+cac40_df.perf.mean())**252 - 1)\nprint(\"Performance relative du fond: \\t\", (1+portfolio.perf.mean())**252 - (1+cac40_df.perf.mean())**252 )\n\n\nRendement annualisé du fond:     0.08254785047977653\nRendement annualisé du CAC40:    0.08502178188176579\nPerformance relative du fond:    -0.00247393140198926\n\n\nLa performance absolue du fonds est de 8.24%, tandis que celle du CAC40 est de 8.50%. Pour évaluer la performance relative du fonds par rapport à son indice de référence, on calcule l’écart de performance : -0.256%. Cela signifie que le portefeuille sous-performe par rapport à son benchmark.\n\n\n\nI.2.2.1 Tracking error\nLa Tracking Error (TE) mesure l’écart de performance entre un fonds et son indice de référence (benchmark). Elle est définie comme l’écart-type des différences de rendement entre le fonds et le benchmark sur une période donnée. Plus la Tracking Error est faible, plus le fonds suit fidèlement son indice de référence.\n\\[\nTE = \\sqrt{\\frac{\\sum_1^n (r_{fond} - r_{benchmark})^2}{n-1}}\n\\]\n\n\nCode\n# Tracking error\nportfolio['benchmark'] = cac40_df['perf']\nportfolio['tracking_error'] = portfolio['perf'] - portfolio['benchmark']\nTError = portfolio['tracking_error'].std()*np.sqrt(252) # Annualisation\n\nTError\n\n\nnp.float64(0.10911787788473695)\n\n\nIl ressort que la dispersion des rendements du fond par rapport ceux de son indice de reférence est de 10.8% en moyenne.\n\n\nI.2.2.2 Volatilité\nAfin d’evaluer le risque de marché du fond, la volatilité complète l’information apportée par la tracking error et permet de se faire une idée sur la regularité et la dispersion des rendements du fond sur la periode d’étude. Il s’agit de l’écart-type des performances absolues du fond.\n\nVolatilité annualisé du portefeuille\n\n\nCode\n# Volatilité annualisé du portefeuille\nstd_dev = portfolio.std()*np.sqrt(252)\nstd_dev\n\n\nAUM               48.833280\nperf               0.132069\nbenchmark          0.128884\ntracking_error     0.109118\ndtype: float64\n\n\n\n\nVolatilité du benchmark\n\n\nCode\n# Volatilité du benchmark\ncac40_df.std()*np.sqrt(252)\n\n\nTicker\n^FCHI    5304.018890\nperf        0.128884\ndtype: float64\n\n\n\n\n\nI.2.2.3 Value at risk\nLa value-at-risk (VaR) est une mesure très répendue en gestion de risques. Elle permet d’evaluer les pertes extrêmes encourrues sur un portefeuille sur un horizon donné sous un niveau de confiance. Elle est généralement calculée sur un horizon 1 jour. Pour obtenir une VaR sur un horizon (H) plus grand, on utilise la méthode de scaling qui consiste à multpiplier la VaR 1 jour par la racine carrée de H.\nElle est donnée par \\(\\mathbf{P}(P \\& L &lt; VaR_H) = \\alpha\\)\nLa VaR est déterminée à l’aide diverses approches: - Approche historique: Elle consiste à calculer le quantile empirique d’ordre \\(\\alpha\\) des pertes historiques. C’est l’approche la plus utilisée en pratique. Cette méthode présente toutefois des limites en cas de changement soudain et inhabituel de conjoncture (crise etc.)\n\nApproche paramétrique: Ici on fait l’hypothèse que les P&L sont distribués suivant un loi dont les paramètres sont calibrés sur les données historiques. Comme toute méthode paramétrique, elle peut conduire à des estimations très biaisées si la loi supposée n’est pas adaptée aux données\n\n\nVisualisation de la distribution des log-rendements journaliers\n\n\nCode\nimport seaborn as sns\n\nsns.histplot(portfolio.perf)\nsns.kdeplot(portfolio.perf, color='red')\n\n\n\n\n\n\n\n\n\n\n\n\nOn s’interesse aux VaR sur un horizon de 20 jours\n\n\n(a) VaR historique\n\n\nCode\nVaR = portfolio.quantile(1-0.99)*np.sqrt(20) # 1-day VaR scaled to 20-day VaR\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.09788047199168412\nla VaR  Relative à 99% d'horizon 20 jours  est : -0.08993815186398733\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\764873701.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\764873701.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 9.66%. Le porte feuille a 99% de chance de sous performer son indice de reference de 8.96% sur un horizon de 20 jours.\n\n\n(b) VaR paramétrique\n\n\nCode\nVaR = (portfolio.mean() + norm.ppf(1-0.99)*portfolio.std())*np.sqrt(20)\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.08514683127950859\nla VaR  Relative  à 99% d'horizon 20 jours  est : -0.07155357198997868\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\937860044.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\937860044.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 8.47%. Le porte feuille a 99% de chance de sous performer son indice de reference de 7.10% sur un horizon de 20 jours.\nOn observe que les VaR paramétriques sont strictement inférieures aux VaR historiques. Ceci pourrait s’expliquer par le fait que les données ont des queues plus lourdes que la loi normale comme l’illustre le graphique présenté plus haut. De plus l’excess de kurtosis confirme cette observation. Par conséquent, la VaR obtenue sous une paramétrisation gaussienne sous-estime la perte maximale encourrue sur le portefeuille."
  },
  {
    "objectID": "risques/projets/demo.html#ii.-risque-de-crédit-et-risque-de-taux",
    "href": "risques/projets/demo.html#ii.-risque-de-crédit-et-risque-de-taux",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "II. Risque de crédit et risque de taux",
    "text": "II. Risque de crédit et risque de taux\nLe risque de crédit est le risque de perte engendrée par la defaillance d’une partie prenante à remplir ses engagements contractuels préalablement établis. C’est principal risque observé sur périmètre du retail.\nOn définit le taux d’intérêt comme le loyer de l’argent (annualisé dans la pratique).\nCette définition est celle historique car très intuitive. Il peut cependant s’averer que les taux d’interêt soient négatifs. Là cette definition devient donc limitée.\nDepéndamment de ce qu’on fait de son argent, le thésauriser, le prêter à la banque , ou encore à l’etat, il existe toujours un risque de perte qui subsiste. Il peut donc advenir, que dans le souci de sécuriser son argent, dans un contexte particulier (notamment incertain), le posseusseur soit disposé à payer pour securiser son argent : on parle de taux d’intérêt négatif.\nCela illustre la métaphore du loyer du coffre fort.\n\nGénéralités sur le pricing d’obligations\nCONTEXTE\nEn raison des obligations réglementaires auxquelles les banques sont soumises, elles ne peuvent pas prêter à toutes les entreprises ayant besoin de financement. C’est dans ce contexte que la notion d’obligation prend son sens. La banque agit alors en tant qu’intermédiaire entre l’entreprise et le marché, et perçoit des frais de commission. le marché prête un montant M à l’entreprise et reçoit des annuités et le nominal à maturité.\nUne obligation est, économiquement, un prêt-emprunt.\nDe manière générale, la valorisation d’un actif est l’espérance des flux actualisés au taux sans risque sous la probabilité risque neutre :\n\\[\n\\begin{aligned}\nX_0 &= \\mathbb{E}[e^{-rT} X_T] \\\\\n    &= e^{-rT} \\, c \\times PS(T)\n\\end{aligned}\n\\]\nN.B : Le taux de recouvrement historique est de 40 %.\nLe recouvrement s’applique uniquement au nominal.\nLa probabilité de survie \\(PS(T)\\) est généralement déterminée à partir du modèle à intensité de Poisson via :\n\\[\nPS(T) = e^{-\\lambda T}\n\\]\nConsidérons une obligation d’échéances \\(T_i\\), \\(i = 1, \\dots, n\\), de coupon \\(c\\) et de nominal \\(N\\).\nLes coupons et le nominal sont payés en cas de survie, et le recouvrement en cas de défaut.\nLa valeur de cette obligation à la date \\(t\\) vaut :\n\\[\nC_t = \\sum_{i=1}^{n} c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}}\n\\]\nLa probabilité de survenue du défaut à une date \\(t\\) vaut :\n\\[\n\\begin{aligned}\nPD(t)\n    &= PS(t) - PS(t + dt) \\\\\n    &= -\\frac{PS(t+dt) - PS(t)}{dt} \\, dt \\\\\n    &= -\\frac{dPS(t)}{dt} \\, dt \\\\\n    &= \\lambda e^{-\\lambda t} \\, dt\n\\end{aligned}\n\\]\nLa valeur actualisée du recouvrement vaut :\n\\[\n\\mathcal{R}_0\n= \\int_0^T R \\lambda e^{-\\lambda t} e^{-rt} \\, dt\n= \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\]\nDe manière générale, pour une date \\(t\\) :\n\\[\n\\mathcal{R}_t\n= \\int_t^T R \\lambda e^{-\\lambda u} e^{-ru} \\, du\n= \\lambda R \\, e^{(r+\\lambda)t}\n  \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n  \\mathbf{1}_{\\{T \\ge t\\}}\n\\]\nLa valeur totale de l’obligation est alors :\n\\[\n\\begin{aligned}\nB_t\n&= \\sum_{i=1}^{n}\n    c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}} \\\\\n&\\quad +\n\\left(\ne^{-(r+\\lambda)(T_n - t)}\n+\n\\lambda R \\, e^{(r+\\lambda)t}\n\\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\right)\n\\mathbf{1}_{\\{T_n \\ge t\\}}\n\\end{aligned}\n\\]\n\n\nImpementation de la valorisation d’un bond\n\n\nCode\n# Impementation de la fonctiuon de valorisation d'un bond\nimport numpy as np\n\ndef Bond(t,c,T,r,lamda, R = 0.4):\n    B = 0\n    for T_i in range(1,T+1):\n        B += np.exp(-(lamda + r)*(T_i - t))*(T_i&gt;=t)\n    B *= c\n    B += (np.exp(-(r + lamda)*(T_i-t)) + lamda * R  * (1-(np.exp(-(r + lamda)*(T_i -t)))) / (r + lamda))*(T_i&gt;=t)\n    return B\n\n\n\nExemple: Obligation au pair\n\nlamda = 0\n\n\nr = 0.02\n\n\nc = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nBond(t,c,T,r,lamda)\n\n\nCode\n# Exemple1: Obligation au pair\nlamda = 0\nr = 0.02\nc = 0.02\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.9981933497987289)\n\n\nAvec les paramètres ci-dessus considérés, on remarque que le prix de l’obligation est proche du nominal. En effet, le taux coupon est égal au taux de marché, ce qui indique que l’obligation est remunérée au taux du marché. Il s’agit donc d’une obligation au pair.\nAu cas où on aurait proposé une remunération supérieure à celle du marché, elle serait beaucoup plus attractive et sa valeur se serait appréciée. l’illustration est donnée ci dessous pour c= 0.03\n\n\nBond(0,0.03,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.03,10,0.02,0)\nprice\n\n\nnp.float64(1.0879246481591023)\n\n\nAvec une remunération inférieure à ce qu’aurait proposé le marché, on obtient une valorisation de l’obligation inférieure au nomimal\n\n\nBond(0,0.015,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.015,10,0.02,0)\nprice\n\n\nnp.float64(0.9533277006185421)\n\n\nEtant donné une intensité de défaut supérieure à zéro, on peut également calculer le coupon pour lequel l’obligation est au pair. On obtient, après calculs prsentés ci-dessous, un taux coupon de 2.6%.\n\n\nCode\n# Recherche du coupon  pour émettre une obligation au pair.\ndef dichot(t,T,r,lamda, P_MKT):\n    c_inf = 1e-8\n    c_sup = 1\n    epsi = 1e-8\n    c_moy = (c_inf + c_sup)/2\n    error = c_sup - c_inf\n\n    while error&gt;epsi:\n        p_hw = Bond(t,c_moy,T,r,lamda)\n        if p_hw &gt; P_MKT:\n            c_sup = c_moy\n        elif p_hw &lt; P_MKT:\n            c_inf = c_moy\n        c_moy = (c_inf + c_sup)/2\n        error = np.abs(c_sup - c_inf)\n\n    return c_moy\n\n\n\n\n\nTaux coupon pour émettre une obligation au pair\n\nlamda = 0.01\n\n\nr = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nCode\n# Taux coupon pour émettre une obligation au pair\nlamda = 0.01\nr = 0.02\nt = 0\nT = 10\ndichot(t,T,r,lamda, P_MKT = 1)\n\n\n0.026393926193952293\n\n\nEn maintenant une remunération égale à celle du marché , avec une intensité de défaut très grande (de l’ordre de 1000%), l’obligation tombe presque instantannément en défaut. Dans cette situation, la valeur du coupon vaut alors 39.92% qui est sensiblement proche du taux de recouvrement (40%).\n\n\n\nValeur du bond poiur une intensité de défaut à 1000%\n\n\nCode\n# Valeur du bond poiur une intensité de défaut à 1000%\nlamda = 10\nr = 0.02\nc = 0.03\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.39920293189432754)\n\n\n\n\n\nÉvolution du prix de l’obligation en fonction du temps\n\n\nCode\nimport matplotlib.pyplot as plt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB = np.array([Bond(t,c,T,r,lamda) for t in times])\nplt.plot(times,B)\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.title(\"Evolution du prix plein coupon du Bond\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nPlus on se rapproche de la date de détachement du coupon plus l’obligation devient attractive, elle prend donc de la valeur.\nChaque saut correspond à un détachement de coupons. une fois le coupon détaché de l’obligation, les flux à venir diminuent et la valeur de l’obligation se deprecie de la valeur du coupon qui a été détaché.\nCes sauts ne reflètent donc pas une dépréciation des bonds par le marché. Ce sont des sauts techniques. Raison pour laquelle on dit que le prix plein coupon est pollué par le coupon (en anglais Dirty price).\nOn va donc s’interesser par la suite au clean price ou pied de coupon qui correspond à au prix du bond moins le coupon couru.\n\\[\n\\tilde{B}_t = B_t - cc\n\\]\nOù (cc) est le coupon couru :\n\\[\ncc = c \\times (t - T^*)\n\\]\nEn retirant cette valeur de coupon couru, on supprime cet effet de saut après les detachements de coupons\n\nImplémentation du clean price\n\n\nCode\n# Implémentation du clean price\n\ndef CleanPrice(t,c,T,r,lamda, R = 0.4):\n    cc = c*(t - np.floor(t))*(t&lt;=T)\n    return Bond(t,c,T,r,lamda, R)-cc\n\n\n\n\nPieds coupon\n\n\nCode\n# Pieds coupon\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for t in times])\nB_pieds_coupon = np.array([CleanPrice(t,c,T,r,lamda) for t in times])\nplt.plot(times,B_pieds_coupon, label =\"Pieds coupon\")\nplt.plot(times,B_plein_coupon, label =\"Plein coupon\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nEn retirant cette valeur de coupon couru, on supprime l’effet de saut après les detachements de coupons, et on observe une courbe sans discontinuités\n\n\n\nEvolution du prix plein coupon et du clean price en fonction du temps pour differents coupons\n\n\nCode\nlamda = 0.01\nr = 0.02\nc = 0.01\nT = 10\n\ntimes = np.linspace(0,10, num = 100)\nB_plein_coupon1 = np.array([Bond(t,0.01,T,r,lamda) for t in times])\nB_pieds_coupon1 = np.array([CleanPrice(t,0.01,T,r,lamda) for t in times])\n\nB_plein_coupon5 = np.array([Bond(t,0.05,T,r,lamda) for t in times])\nB_pieds_coupon5 = np.array([CleanPrice(t,0.05,T,r,lamda) for t in times])\n\nplt.plot(times,B_pieds_coupon1, label =\"Pieds coupon 1%\")\nplt.plot(times,B_plein_coupon1, label =\"Plein coupon 1%\")\n\nplt.plot(times,B_pieds_coupon5, label =\"Pieds coupon 5%\")\nplt.plot(times,B_plein_coupon5, label =\"Plein coupon 5%\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nLes constats que l’on fait : - Lorsque c = 1% : La rémunération de l’obligation est inférieure à celle offerte par le marché. Ainsi, lors de son émission, sa valeur sera nécessairement inférieure à celle du pair, ce qui explique une valeur initiale proche de 87 %. À maturité, on reçoit 100 % du nominal, plus 1 % de ce dernier, correspondant au coupon. Le résultat final est donc sensiblement égal à 101 %.\n\nLorsque c = 5% : L’obligation rémunère plus que ce que le marché offre, ce qui la rend particulièrement attractive dès son émission. Ainsi, sa valeur de départ sera d’environ 120 %. Au fur et à mesure que les obligations sont détachées, les flux futurs diminuent, ce qui entraîne une dépréciation de sa valeur. À maturité, on reçoit 100 % du nominal, plus 5 % du montant nominal, correspondant au coupon. Le résultat final est donc de 105 %.\n\n\n\nEvolution du prix de l’obligation en fonction du taux d’intérêt\nDe l’expression analytique du prix du bond, on observe que le prix est décroissant du taux d’intérêt. La figure ci-dessous en donne une illustration, toute chose égale par ailleurs.\nOn vérifie graphiquement que le prix du bond est de 100% lorsque le taux d’intérêt est égal au taux sans risque \\(r^* = c - \\lambda (1 - R)\\).\n\nEvolution du prix du bond en fonction du taux d’intérêt\n\n\nCode\n# Evolution du prix du bond en fonction du taux d'intérêt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR = 0.4\ninterest = np.linspace(0,0.10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest,B_plein_coupon, label =\"Plein coupon\")\nplt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\nplt.xlabel(\"Taux d'intérêt (%)\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_32588\\2938139823.py:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\n\n\nText(0.5, 1.0, \"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotion de sensibilité\nLa sensibilité mesure la variation du prix du bond face à une variation d’un facteur de risque. On distingue entre autres la sensibilité de taux et la sensibilité de crédit.\nDans cette sectioin nous nous intéressons particulièrement à la sensibilité de taux. Elle est définie par la formule:\n\\[\n    Sensibilité = - \\frac{dB_t}{dr} \\frac{1}{B_t}\n\\]\nInterprétation: Lorsque le taux d’intérêt bouge de 1%, alors le prix du bond bouge de -sensibilité %.\nCette sensibilité peut également être vue comme le barycentre des différentes échéances pondérées par les flux actualisés. C’est la duration.\n\\[\n    \\frac{\\sum T_i\\times F_i}{\\sum F_i}\n\\]\nExemple: En considérant une obligation de maturité 3 ans payant des coupons annuels, avec intensité de défaut nul. Alors son prix et la sensibilité de taux sont données par:\n\\[\n\\begin{aligned}\nP &= c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial P}{\\partial r}\n&= -1c \\, e^{-1r} - 2c \\, e^{-2r} - 3c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Sensibilité}\n&= \\frac{1c \\, e^{-1r} + 2c \\, e^{-2r} + 3c \\, e^{-3r}}\n        {c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}}\n\\end{aligned}\n\\]\nD’où l’expression barycentrique des échéances pondérées par les flux.\n\n\nCode\n# Sensibilité de taux du prix du bond\ndelta_r = 1e-4 #1bp\n\n\ndef Sensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    return -(Bond(t,c,T,r+delta_r,lamda, R)-B_t)/(delta_r*B_t)\n\n\nOn considère un bond dont les caractérisiques sont les suivqantes:\n\\[\n    \\begin{cases}\n        \\lambda = 0.01\\\\\n        r = 0.02\\\\\n        c = 0.03\\\\\n        T = 10\\\\\n        R = 0.4\\\\\n    \\end{cases}\n\\]\nLa sensiblité de ce bond est de 8.64. Ainsi, lorsque le taux d’intérêt augmente de 1 point de pourcentage, le prix du bond diminue de 8.64%.\n\n\nCode\n# Sensibilité de taux d'intérêt\nSensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4)\n\n\nnp.float64(8.643982489105056)\n\n\n\nEvolution de la sensibilité de taux fonction de la maturité\n\n\nCode\n# Evolution de la sensibilité au taux d'intérêt en fonction de la maturité\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,c,T,r,lamda) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité')\n\n\n\n\n\n\n\n\n\n\nOn constate que la sensibilité de taux croît avec la maturité et tend à être linéaire.\nPour des paramètres extrêmes \\(c = \\lambda = r = 0\\), la sensibilité (duration) est identique à la maturité comme illustré sur la figure ci-dessous. Cette remarque met en évidence la relation mlathématique entre la sensibilité et la maturité présentée plus haut.\nConnaissant la maturité, pour une variation du taux d’intérêt on peut donc donner une estimation “grossière” de la sensibilité (en approximant la duration par 0.8*maturité, par exemple).\n\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,1e-6,T,1e-6,1e-6) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité (cas extrême)\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité (cas extrême)')\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR d’une obligation.\nL’estimation de la VaR sur un bond peut se faire à partir de la senssibilité de taux ou par la méthode de repricing.\n\n\nEstimation de la VaR par la sensibilité du taux d’intérêt\nL’approche par la sensibilité se présente de la manière suivante:\nOn suppose que la dynamique du taux d’intérêt est donnée par \\[\\Delta r \\sim \\mathcal{N}(0, \\sigma \\sqrt{\\Delta t})\\].\nSachant que \\[\\frac{\\Delta P}{P} = - Duration \\times \\Delta r\\]\nil suit que \\[\\frac{\\Delta P}{P} \\sim \\mathcal{N}(0, Duration \\times \\sigma \\sqrt{\\Delta t})\\]\nUne approche par la sensibilité de la VaR à 99% donne \\[VaR = Duration \\times \\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\\] Où \\(z_{99\\%}\\) est le quantile d’ordre 99% de la loi normale standard.\nPour les besoins de notre exercice, on pose \\(\\sigma = 1\\%\\).\n\n\nCode\nfrom scipy.stats import norm\n\n# Calcul de la VaR par la sensibilité du taux d'intérêt\ndef Sensi_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    duration = Sensi(t,c,T,r,lamda, R)\n    var = duration*sigma*np.sqrt(H)*norm.ppf(alpha)\n    return var\n\n\n\n\nCode\nsigma= 0.01\nH = 1/12\nt = 0\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR =0.40\n\nSensi_VaR(sigma , H, t,c,T,r,lamda)\n\n\nnp.float64(0.05804942383590022)\n\n\nLa VaR à 99% d’horizon 1 an sur le bond est de 5.8%. - On note que la VaR est une fonction linéaire croissante de la volatilité. C’est une conséquence directe de l’hypothèse de normalité des variations du taux d’intérêt. Ainsi, une augmentation de la volatilité du taux d’intérêt s’accompagne d’une augmentation de la volatilité du prix du bond. - La VaR est une fonction décroissante du taux d’intérêt. En effet, une augmentation du taux d’intérêt entraîne une diminution de la valeur des bonds, et par conséquent des valeurs extrêmes atteintes par celles-ci; d’où la VaR décroît. Toutefois l’évolution de la VaR en fonction du taux d’intérêt n’est pas linéaire.\n\n\nEvolution de la Z-VaR en fonction du taux d’intérêt\n\n\nCode\n# Evolution de la VaR en fonction de la volatilité\nvol = np.linspace(0,1e-2, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for sigma in vol])\n\nplt.plot(vol, var, label =\"VaR_99%\")\nplt.xlabel(\"Volatilité\")\nplt.ylabel(\"VaR \")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de la volatilité\")\n\n\nText(0.5, 1.0, 'VaR à 99% à horizon 1 an en fonction de la volatilité')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Evolution de la VaR en fonction du taux d'intérêt\ninterest = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nEvolution de la Z-VaR en fonction de l’intensité de défaut\nPour des intensités de défaut croissantes, la VaR du bond décroît de façon exponentielle, ceteris paribus. Tout comme avec le taux d’intérêt, une augmentation de l’intensité de défaut s’accompagne d’une imminence du défaut et par conséquent de la diminution de la valeur du bond. D’où une diminution de la VaR.\n\n\nCode\n# Evolution de la VaR en fonction de l'intensité de défaut\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de défaut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR par repricing de l’obligation\nLa méthodologie consiste à revaloriser le bond pour une variation du taux d’intérêt, puis d’en déduire la variation du prix du bond qui en résulte.\n\\[\n    \\Delta r^* =\\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\n\\]\n\\[\n    VaR_{99\\%} = \\frac{B(r+ \\Delta r^*) - B(r)}{B(r)}\n\\]\n\n\nApproche repricing\n\n\nCode\n# Approche repricing\n\ndef repricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    delta_r = sigma*np.sqrt(H)*norm.ppf(alpha)\n    B_rep = Bond(t,c,T,r+delta_r,lamda, R)\n    B_r = Bond(t,c,T,r,lamda, R)\n    \n    return  (B_r - B_rep)/B_r\n\n\n\n\nCode\nrepricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99)\n\n\nnp.float64(0.05627224378244837)\n\n\n\nLa VaR obtenue par l’approche repricing, sous les mêmes conditions que précédemment, est de 5.6%. Cette VaR est inférieure à celle obtenue sous l’hypothèse de normalité des variations du taux d’intérêt.\nCeci s’explique par le fait que la duration est une fonction convexe décroissante du taux d’intérêt et représente une approximation affine de la valeur du bond en le taux d’intérêt. De manière générale, l’approche par les sensibilités surestime la VaR.\nL’évolution de la VaR, obtenue par reprincing, en fonction du taux d’intérêt ou de l’intensité de défaut (voir les graphiques ci-dessus) à la même allure que la z-VaR.\nEn règle générale la VaR doit être inférieure au seuil de 20% de par la limite règlementaire. L’utilisation de la z-VaR peut donc constituer un manque à gagner pour les institutions financieres. En effet, elles peuvent être amenées à dérisquer leur portefeuille en limitant leur investissements pour des raisons purement techniques.\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\ninterest = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda)for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de saut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")"
  },
  {
    "objectID": "risques/projets/demo.html#iii.-risque-de-contrepartie",
    "href": "risques/projets/demo.html#iii.-risque-de-contrepartie",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "III. Risque de contrepartie",
    "text": "III. Risque de contrepartie\nDans le cadre d’un engagement contratuel incluant deux parties prenantes, le risque de contrepartie fait reférence au rique de perte suite au défaut d’une des parties à remplir les engagements contractuels pré-établis: c’est un risque bilatéral.\nAfin de se couvrir du defaut de l’émetteur d’un bond, l’acheteur du bond peut entrer dans un credit default swap (CDS). Dans un tel contrat, lorsque la contrepartie est correlée à l’émetteur de l’obligation on parle de wrong way risk ce qui a pour conséquence d’exposer davantage le souscripteur. La contrepartie doit donc être décorrelée de l’émetteur.\nLe risque de contrepartie est en général mitigé par les appels de marge (chambre de compensation). - Quand la qualité de l’émetteur se dégrade, son spread de crédit spread croît et le CDS s’apprécie. - Si la contrepartie fait défaut, le détenteur perd \\(CDS = 1-R\\). - Un mécanisme d’appel de marge permet de mitiger le risque.\n\nLien entre CDS et obligation: la formule du triangle de crédit\nUn estimation du taux sans risque d’une obligation: \\[\n\\begin{aligned}\nP &= \\sum_{i=1}^{n} c \\, e^{-(r+\\lambda)T_i}\n    + e^{-(r+\\lambda)T_n}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\int_0^T e^{-(r+\\lambda)t} \\, dt\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\nL’obligation est au pair si et seulement si P = 1 Ainsi, \\[\n    (c + \\lambda R) \\frac{ 1- e^{-(r + \\lambda )T}}{r + \\lambda} + e^{-(r + \\lambda)T} = 1\n\\]\nQuand T tend vers l’infini le taux sans risque vaut alors, \\[\n    (c + \\lambda R)  = r + \\lambda\n\\]\n\\[\n    (c - r )  = \\lambda (1 - R)\n\\] - \\(c-r\\): spread ou prime de risque - \\(\\lambda\\): intensité de défaut - \\(1-R\\): LGD \\[\n    Spread = PD \\times LGD\n\\]\n\n\nCDS: principe\n\nPremium Leg: est payé tant que l’émetteur est “en vie”. \\[ PL = \\int_0^T s \\times e^{-rt} \\times e^{-\\lambda t}dt = s \\frac{1-e^{-(r+\\lambda)T}}{r + \\lambda}\\]\nDefault Leg: compensation à la date t si le défaut y survient. \\[DL = \\int_0^T(1-R) e^{-rt}\\times e^{-\\lambda t} dt = \\lambda (1-R) \\frac{1-e^{-(r+\\lambda)T}}{r+\\lambda}\\].\n\nLa valeur initiale d’un spread étant nulle, les deux jambes sont égales et il on obtient:\n\\[\n    s = \\lambda (1-R)\n\\]\nUn estimation de ce spread peut donc être déduite de la formule du triangle de crédit.\n\n\nSensibilité de crédit\nLa sensibilité de crédit d’une obligation est la variation du prix de l’obligation lorsque varie le spread de crédit de la contrepartie. De la formule du triangle, la sensibilité de crédit peut être déduite de la sensibilité par rapport à l’intensité de défaut \\(\\lambda\\).\n\\[\n    \\frac{\\partial P}{\\partial s} = \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda}\n\\]\n\\[\n    \\frac{\\partial P}{\\partial s} \\times \\frac{1}{P}= \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda} \\times  \\frac{1}{P}\n\\]\nOn peut calculer une VaR taux et une VaR crédit. Toutefois, la VaR qui sera regardée de près est celle issue de la variation conjointe des taux et des spreads de crédit.\n\nSensibilité de crédit du bond\n\n\nCode\n# Sensibilité de crédit du bond\ndelta_lambda = 1e-4 #1bp\n\ndef Sensi_credit(t,c,T,r,lamda, R = 0.4, delta_lambda = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    variation = -(Bond(t,c,T,r,lamda+delta_lambda, R)-B_t)/(delta_r*B_t)\n    return variation/(1-R)\n\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda)\n\n\nnp.float64(8.82119086802735)\n\n\n\nUne variation d’un point de pourcentage du spread de crédit entraîne une variation de 8.82% de la valeur du bond. Cette sensibilité de crédit est assez proche de la sensibilité taux. En effet, on observe de l’écriture mathématique de la valeur du bond que le taux d’intérêt et l’intensité apparaissent conjointement de façon additive (sauf pour le terme de recouvrement).\nCes deux sensibilitées sont identiques pour un taux de recouvrement nul.\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda, R = 0.0, delta_lambda = 1e-4)\n\n\nnp.float64(8.77911216566412)\n\n\n\n\n\nEstimation de la VaR par variation conjointe du taux d’intérêt et du spread de crédit\nOn suppose que la dynamique du taux d’intérêt est donnée par l’EDS\n\\[\n    dr_t = \\sigma dW_t\n\\]\nLe spread ne prenant pas de valeur négative, on supposera qu’il est log-normal. Sa dynamique est donnée par l’EDS\n\\[\n    \\frac{ds_t}{s_t} = \\alpha dZ_t\n\\]\nEn supposant \\(dW_t dZ_t = \\rho\\) et \\(Z_t = \\rho W_t + \\sqrt{1-\\rho^2}V_t\\) tel que \\(dV_t dW_t = 0\\),\nLa dynamique du spread s’écrit alors \\[\n\\frac{ds_t}{s_t} = \\alpha (\\rho W_t + \\sqrt{1-\\rho^2}V_t)\n\\]\nEn appliquant un schéma de discrétisation d’Euler, on peut alors simuler les taux \\(r_t\\) et les spreads \\(s_t\\) par:\n\\[\n    r_H = r + \\sigma \\sqrt{H} W_H\n\\]\n\\[\n    s_H = s(1 + \\alpha \\rho \\sqrt{H} W_H + \\alpha \\sqrt{1-\\rho^2} \\sqrt{H} V_H)\n\\]\nOù H est le pas de discrétisation.\nLa méthodologie de détermination de la sensibilité du prix d’une obligation conjointement au taux d’intérêt et au spread de crédit est la suivante: - Calculer la valeur du bond pour les paramètres initiaux - Simuler les taux \\(r_H\\) et les spreads \\(s_H\\) - Pour chaque simulation, calculer la valeur du bond (cetertis paribus) - pour déterminer la variation des nouveaux prix par rapport au prix initial - calculer le quantile d’ordre 99% des variations obtenues\n\n\nCode\n# Sensibilité conjointe de taux et de crédit\n\ndef SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12, M=10_000):\n    \n    u1 = np.random.uniform(0,1, size = M)\n    u2 = np.random.uniform(0,1, size = M)\n    w = norm.ppf(u1)\n    v = norm.ppf(u2)\n    s = lamda*(1-R)\n    \n    r_H = r + sigma*np.sqrt(H)*w\n    s_H = s*(1 + alpha*rho*np.sqrt(H)*w + alpha*np.sqrt(1-rho**2)*np.sqrt(H)*v)\n    \n    P_0 = Bond(0,c,T,r,lamda, R)\n    P_1 = Bond(H,c,T,r_H,s_H/(1-R), R)\n    mu = (P_1 - P_0)/P_0\n    return  - np.quantile(mu,0.01)\n\n\n\n\nCode\nsigma = 0.01 # Volatilité du taux d'intérêt\nalpha =0.4   # Volatilité du spread\nrho = 0.4    # Corrélation entre le spread et le taux d'intérêt\n\nnp.random.seed(90) # Pour la reproductibilité\nSimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12)\n\n\nnp.float64(0.0602691861203251)\n\n\n\n\nCode\nnp.random.seed(90)\n\nrhos = np.linspace(0,1, num= 1000)\nvar = np.array([SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12) for rho in rhos])\n\nplt.plot(rhos, var)\n\n\n\n\n\n\n\n\n\nLa corrélation \\(\\rho\\) doit être positive, car une augmentation des taux d’intérêt entraîne une meilleure rémunération du marché par rapport aux obligations. Pour rester attractifs, les coupons doivent alors augmenter. Cette hausse des coupons accroît le coût de financement de l’émetteur, ce qui conduit à une augmentation du spread de crédit. Il en résulte une corrélation positive entre les taux d’intérêt et les spreads de crédit.\nUne augmentation conjointe des taux et des spreads accroît le risque de défaut, en raison de la corrélation positive entre ces deux facteurs de risque. Ainsi, une corrélation élevée reflète un manque de diversification, ce qui expose davantage le portefeuille au risque systémique"
  },
  {
    "objectID": "risques/projets/demo.html#iv.-risque-modèle",
    "href": "risques/projets/demo.html#iv.-risque-modèle",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "IV. Risque modèle",
    "text": "IV. Risque modèle\nLe risque modèle désigne le risque associé à l’utilisation d’un modèle mathématique ou statistique qui ne représente pas correctement la réalité ou qui mène à des décisions erronées en finance et en gestion des risques.\nEléments du suivi du risque modèle: - Sanity check: Ecart de performance - Backtesting: Application aux données antérieures - Comparaison avec desmodèles plus riches - Provisionner au titre du risque de modèle"
  },
  {
    "objectID": "risques/projets/demo.html#v.risque-climatique",
    "href": "risques/projets/demo.html#v.risque-climatique",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "V.Risque climatique",
    "text": "V.Risque climatique\nLe risque climatique désigne les risques financiers liés au changement climatique et aux politiques de transition vers une économie bas carbone. Sa modélisation reste complexe, notamment en raison du manque de données historiques et de l’absence d’un cadre réglementaire clairement défini.\nOn distingue deux principaux types de risques climatiques :\n\nRisque physique: l correspond aux pertes financières causées par des événements climatiques extrêmes. Il est défini par deux éléments clés : la fréquence et la sévérité des événements.\n\nRisque aigu : Événements rares mais très intenses ( ouragans, inondations).\nRisque chronique : Changements progressifs et durables (élévation du niveau de la mer).\n\nRisque de transition: il découle de l’adaptation des entreprises et des acteurs économiques aux exigences de transition écologique.\n\nRisque politique et réglementaire : Durcissement progressif des régulations environnementales (taxe carbone).\nOpportunités technologiques : Innovations favorisant la transition énergétique et redéfinissant les modèles économiques.\n\n\nLa resilience des institutions financieres notamment aux changement climatique est évaluée via des stress tests climatiques dont les scénarios sont issus des données du NGFS (Network for Greening financial System)."
  },
  {
    "objectID": "risques/projets/liquidity.html",
    "href": "risques/projets/liquidity.html",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "risques/projets/liquidity.html#introduction",
    "href": "risques/projets/liquidity.html#introduction",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Introduction",
    "text": "Introduction\nCe document présente une introduction à la gestion des risques en Asset Management.\nL’objectif principal est de comprendre et d’appliquer les méthodologies de base utilisées pour analyser, mesurer et suivre les risques financiers au sein d’un portefeuille.\nL’étude couvre differentes notions de risk management notamment:\n- les méthodes de mesure du risque de marché (sensibilité, volatilité, Value-at-Risk, Tracking Error, stress tests et profil de liquidité),\n- les composantes clés du risque de crédit (intensité de défaut, sensibilité des obligations aux variations de spreads, triangle du crédit),\n- le risque de contrepartie et ses mécanismes de mitigation via les appels de marge,\n- ainsi que les risques émergents tels que le risque climatique et le risque modèle.\n\n\nCode\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import norm\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize"
  },
  {
    "objectID": "risques/projets/liquidity.html#i.-risques-de-marché",
    "href": "risques/projets/liquidity.html#i.-risques-de-marché",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "I. Risques de marché",
    "text": "I. Risques de marché\n\nConstruction d’un portefeuille diversifié\nOn construit un portefeuille constitué de 10 entreprises du CAC40. Nous avons choisi des entreprises de secteurs variés afin de s’assurer que le portefeuille soit diversifié.\nAinsi, notre portefeuille est constitué des entreprises des secteurs suivants :\n\nBanque et assurance : BNP Paribas (BN.PA), Crédit Agricole (CA.PA)\n\nAéronautique et défense : Airbus (AIR.PA)\n\nTechnologie: Capgemini (CAP.PA)\nConcession et Construction: Vinci (DG.PA)\nTélécommunications : Orange (ORA.PA)\n\nÉnergie : Engie (ENGI.PA)\n\nLuxe & Consommation : LVMH (MC.PA)\n\nMédias & Divertissement : Vivendi (VIV.PA)\n\nIndustrie Chimique : Air Liquide (AI.PA)\n\nNous allons procéder à une optimisation de Markowitz visant à construire un portefeuille qui minimise la volatilité (risque) sous la contrainte d’un niveau de rendement modulable en fonction du profil de risque de l’investisseur.\n\n\nCode\n# On recupère les données sur un historique de deux ans\n\nend_date = '2025-03-16'  \nstart_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=2*365)).strftime('%Y-%m-%d')\ntickers = ['AI.PA', 'AIR.PA', 'BN.PA', 'CA.PA', 'CAP.PA', 'DG.PA', 'ENGI.PA', 'ORA.PA', 'SAN.PA', 'VIV.PA']\n\ndf = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\ncac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\388541553.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df = yf.download(tickers, start=start_date, end=end_date)[['Close', 'Volume']]\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**************        30%                       ]  3 of 10 completed[*******************   40%                       ]  4 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\388541553.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40 = yf.download('^FCHI', start=start_date, end=end_date)[['Close', 'Volume']]\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\ndf_close = df['Close']\ndf_volume = df['Volume']\n# Calcul des  rendements journaliers\nreturns = df_close.pct_change()\nreturns.dropna(inplace=True)\n\n\n\nAperçu des données\n\n\nCode\n#Aperçu des données\ndf.head(3)\n\n\n\n\n\n\n\n\nPrice\nClose\nVolume\n\n\nTicker\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\nAI.PA\nAIR.PA\nBN.PA\nCA.PA\nCAP.PA\nDG.PA\nENGI.PA\nORA.PA\nSAN.PA\nVIV.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-03-17\n126.875412\n110.644325\n49.379742\n14.572609\n157.424194\n91.923409\n10.256954\n8.935349\n79.612030\n8.390329\n1971502\n2171272\n3041249\n4474936\n719990\n3575676\n12989586\n20039590\n5122220\n7405822\n\n\n2023-03-20\n129.684906\n112.407211\n49.742428\n14.783134\n157.659653\n93.078461\n10.359279\n9.040173\n79.310600\n8.565088\n1139355\n1290939\n1697298\n2379408\n418826\n1207887\n7087933\n9795603\n2119074\n2624806\n\n\n2023-03-21\n129.288452\n115.250565\n50.186718\n14.955766\n160.579285\n95.104210\n10.579202\n9.144996\n79.948914\n8.741748\n745105\n1002062\n2018104\n2130100\n293112\n934999\n7313789\n9077226\n1449009\n2962587\n\n\n\n\n\n\n\nComme indicateur quantifiable de cette diversification nous presentons ci après la matrice de correlation entre les rendements journaliers des actifs du portefeuille pour notre période d’étude.\n\n\nCode\n# Calcul de la matrice de corrélation\ncorr_actifs = returns.corr().to_numpy()\n\n# Affichage du corrélogramme\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_actifs, annot=True, fmt='.2f', cmap='coolwarm', xticklabels=returns.columns, yticklabels=returns.columns)\nplt.title(\"Corrélogramme des actifs\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation du principe d’optimisation de Markovitz\nL’optimisation de Markowitz, également connue sous le nom de théorie moderne du portefeuille (MPT), permet de construire un portefeuille optimal en combinant des actifs de manière à minimiser la volatilité (ou le risque) tout en respectant une contrainte sur un niveau de rendement fixé. L’un des éléments clés de cette théorie est la détermination des poids de chaque actif dans le portefeuille, afin d’atteindre un compromis optimal entre le risque et le rendement.\nHypothèses principales :\n\nLes décisions sont prises en fonction du couple rendement-risque.\n\nLes investisseurs cherchent à diversifier leurs actifs pour réduire le risque.\n\nL’accès au taux sans risque est illimité.\n\nAucun coût de transaction n’est pris en compte.\n\nProgramme d’optimisation : L’objectif est de minimiser la volatilité sous la contrainte d’un rendement cible $ _T $ :\n\\[\n\\text{Minimiser } \\omega^t\\Sigma \\omega\n\\] Sous les contraintes :\n$$\n\\[\\begin{cases}\n    \\omega^t \\mu = \\mu_T \\\\\n    \\sum_{i=1}^{n} \\omega_i = 1\n\n\\end{cases}\\]\n$$\n\nCalcul des rendements annualisées et des volatilités annualisés des log_rendements pour chaque actif\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nOptimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\n# Rendement annualisé\n\nmean_daily_return= returns.mean()\nyearly_return = (1+ mean_daily_return)**252 -1\n\n# volatilité annualisé\nsigma = returns.cov().to_numpy()\nsigma = 252*sigma\n\n\n\n\nOptimisation du portefeuille\nL’investisseur a la latitude de shorter ou pas (via l’argument Positive de la foncion efficient_portfolio qui donne la latitude de lever la contrainte de positivité des coefficients), également de definir le niveau de rendement souhaité (via l’argument mu_target de la même fonction).\nDans le cadre de l’exercice, pour des raisons de simplification et pour rester cohérent avec la suite, nous avons opté pour un portefeuille sur des positions longues.\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\n \n# Nombre d'actifs\nn_assets = len(yearly_return)\nmu = yearly_return\n\ndef portfolio_variance(weights):\n    return weights.T @ sigma @ weights\n\ndef weight_sum_constraint(weights):\n    return np.sum(weights) - 1\n\ndef target_return_constraint(weights, mu_target):\n    return weights.T @ mu - mu_target\n\ndef efficient_portfolio(mu_target, range_=(-0.1, None), positive=True):\n    # On initialise les poids de maniere équitable\n    init_weights = np.ones(n_assets) / n_assets\n\n    # Contraintes\n    constraints = [\n        {'type': 'eq', 'fun': weight_sum_constraint},\n        {'type': 'eq', 'fun': lambda w: target_return_constraint(w, mu_target)}\n    ]\n\n    # On verifie si on a définit une contrainte de positivité\n    if positive:\n        bounds = [(0, None) for _ in range(n_assets)]  # Chaque poids doit être &gt;= 0\n    else:\n        bounds = [range_ for _ in range(n_assets)] \n\n    # Optimisation\n    result = minimize(portfolio_variance, init_weights, bounds=bounds, method='SLSQP', constraints=constraints)\n\n    if result.success:\n        optimal_weights = result.x\n        portfolio_volatility = np.sqrt(portfolio_variance(optimal_weights))\n        return portfolio_volatility, optimal_weights\n    else:\n        return None, None\n\n\n\n\nIllustration pour un rendement cible de 10%\nPour un rendement annuel cible de 10%, correspondant à l’ordre de grandeur des rendements du CAC40 qui représente le benchmark du fond, les poids optimaux donnés par le l’optimisation de Markovitz et qui ont permis de minimiser le risque auquel est exposé le porte feuille ( 9.67% de volatilité) sont donnés ci après:\n\n\nCode\n# Rendement cible\nmu_target = 0.1 \n\n# les poids optimaux et la volatilité\nvol, weights = efficient_portfolio(mu_target)\nprint(\"Le portefeuille ainsi construit a un rendement de\",round(weights.T @ mu,4), \"et est soumis à une volatilité de l'ordre de :\", round(vol,4), \"\\n\")\nprint(\"---\"*50)\nprint(\"Les poids associés aux actifs du porte feuille sont listés ci après:\\n\")\ni = 0\nfor column in df_close.columns.tolist():\n    print(f\"{column}, {weights[i]:.4f}\")\n    i += 1\n\n\nLe portefeuille ainsi construit a un rendement de 0.1 et est soumis à une volatilité de l'ordre de : 0.0989 \n\n------------------------------------------------------------------------------------------------------------------------------------------------------\nLes poids associés aux actifs du porte feuille sont listés ci après:\n\nAI.PA, 0.0243\nAIR.PA, 0.0382\nBN.PA, 0.2031\nCA.PA, 0.1929\nCAP.PA, 0.1255\nDG.PA, 0.0000\nENGI.PA, 0.0732\nORA.PA, 0.2652\nSAN.PA, 0.0542\nVIV.PA, 0.0234\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), weights*100, color='skyblue')\n\nplt.title(f\"Poids optimaux des actifs dans le portefeuille (Volatilité: {round(vol, 4)})\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Poids (en %)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, weights):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nOn constate que le portefeuille optimal est assez diversifié.Les trois principales entreipes representées sont :\n\nTélécommunications : Orange (26.53%)\nBanque et assurance: BNP Paribas (20.24%), Credit Agricole (19.27%)\nTechnologie : Capgemini (12.52%)\nÉnergie : Engie (7.30%)\n\n\n\n\nSuivi du porte feuille\nDans cette section on va determiner l’AUM du portefeuille, suivre ses performances sa volatilité et ses performances relatives par rapport au CAC40.\nNous effectuerons également un stress test sur le portefeuille pour ebaluer sa sensibilité relative à la période COVID.\n\n\nCode\n# Poids des actions dans le portefeuille\nweights = pd.DataFrame({'Ticker': tickers ,\n     'Weight': weights})\nweights = weights.set_index('Ticker')['Weight']\n\n#print(weights)\n\n\n\n\nAUM (Asset under management)\nElle représente la valeur du porte feuille et est déterminée par la formule suivante\nAUM = (NAV)(Nombre_de_ parts)\nAvec\nLa Net Value Asset (NAV) représente la valeur nette d’une part du fonds (portefeuille) à un moment donné.\n\nDétermination de l’AUM\n\n\nCode\n# Détermination de l'AUM\naum = df_close@weights\nportfolio = pd.DataFrame({'AUM': aum})\nportfolio.head()\n\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-03-17\n47.527164\n\n\n2023-03-20\n47.829456\n\n\n2023-03-21\n48.500874\n\n\n2023-03-22\n48.360226\n\n\n2023-03-23\n48.621797\n\n\n\n\n\n\n\n\n\nCode\nportfolio.describe()\n\n\n\n\n\n\n\n\n\nAUM\n\n\n\n\ncount\n509.000000\n\n\nmean\n53.563781\n\n\nstd\n3.084850\n\n\nmin\n47.527164\n\n\n25%\n50.547938\n\n\n50%\n53.835636\n\n\n75%\n56.161581\n\n\nmax\n59.243514\n\n\n\n\n\n\n\nSur notre période d’etude, l’AUM en moyenne est de 59.10, et oscille entre 50.68 et 66.84.\n\n\nPerformance du portefeuille\nDans le cadre du suivi de notre portfeuille, il est important d’identifier et de mesurer les risques auxquels il est exposé afin de mieux les encadrer. Dans cette section nous intéressons tout particulièrement au risque de marché. Ce risque peut être évalué à l’aide de:\n\nLa tracking error\nLa volatilité\nLa VaR\n\nqui sont des mesures ex-ante.\n\nOn s’intéresse aux performances du portefeuille construit ainsi que celles du benchmark\n\n\nCode\n# Performance du portefeuille\nportfolio['perf'] = portfolio['AUM'].pct_change()\nportfolio.dropna(inplace=True)\n\n# Performance du benchmark\ncac40_df = cac40['Close']\ncac40_df ['perf'] = cac40_df.pct_change()\ncac40_df.dropna(inplace=True)\n\nprint(\"Rendement annualisé du fond: \\t\", (1+portfolio.perf.mean())**252 - 1)\nprint(\"Rendement annualisé du CAC40: \\t\", (1+cac40_df.perf.mean())**252 - 1)\nprint(\"Performance relative du fond: \\t\", (1+portfolio.perf.mean())**252 - (1+cac40_df.perf.mean())**252 )\n\n\nRendement annualisé du fond:     0.0825478424160695\nRendement annualisé du CAC40:    0.08502178188176579\nPerformance relative du fond:    -0.0024739394656962954\n\n\nLa performance absolue du fonds est de 8.24%, tandis que celle du CAC40 est de 8.50%. Pour évaluer la performance relative du fonds par rapport à son indice de référence, on calcule l’écart de performance : -0.256%. Cela signifie que le portefeuille sous-performe par rapport à son benchmark.\n\n\n\n\nTracking error\nLa Tracking Error (TE) mesure l’écart de performance entre un fonds et son indice de référence (benchmark). Elle est définie comme l’écart-type des différences de rendement entre le fonds et le benchmark sur une période donnée. Plus la Tracking Error est faible, plus le fonds suit fidèlement son indice de référence.\n\\[\nTE = \\sqrt{\\frac{\\sum_1^n (r_{fond} - r_{benchmark})^2}{n-1}}\n\\]\n\n\nCode\n# Tracking error\nportfolio['benchmark'] = cac40_df['perf']\nportfolio['tracking_error'] = portfolio['perf'] - portfolio['benchmark']\nTError = portfolio['tracking_error'].std()*np.sqrt(252) # Annualisation\n\nTError\n\n\nnp.float64(0.1091178889465383)\n\n\nIl ressort que la dispersion des rendements du fond par rapport ceux de son indice de reférence est de 10.8% en moyenne.\n\n\nVolatilité\nAfin d’evaluer le risque de marché du fond, la volatilité complète l’information apportée par la tracking error et permet de se faire une idée sur la regularité et la dispersion des rendements du fond sur la periode d’étude. Il s’agit de l’écart-type des performances absolues du fond.\n\nVolatilité annualisé du portefeuille\n\n\nCode\n# Volatilité annualisé du portefeuille\nstd_dev = portfolio.std()*np.sqrt(252)\nstd_dev\n\n\nAUM               48.833282\nperf               0.132069\nbenchmark          0.128884\ntracking_error     0.109118\ndtype: float64\n\n\n\n\nVolatilité du benchmark\n\n\nCode\n# Volatilité du benchmark\ncac40_df.std()*np.sqrt(252)\n\n\nTicker\n^FCHI    5304.018890\nperf        0.128884\ndtype: float64\n\n\n\n\n\nValue at risk\nLa value-at-risk (VaR) est une mesure très répendue en gestion de risques. Elle permet d’evaluer les pertes extrêmes encourrues sur un portefeuille sur un horizon donné sous un niveau de confiance. Elle est généralement calculée sur un horizon 1 jour. Pour obtenir une VaR sur un horizon (H) plus grand, on utilise la méthode de scaling qui consiste à multpiplier la VaR 1 jour par la racine carrée de H.\nElle est donnée par \\(\\mathbf{P}(P \\& L &lt; VaR_H) = \\alpha\\)\nLa VaR est déterminée à l’aide diverses approches: - Approche historique: Elle consiste à calculer le quantile empirique d’ordre \\(\\alpha\\) des pertes historiques. C’est l’approche la plus utilisée en pratique. Cette méthode présente toutefois des limites en cas de changement soudain et inhabituel de conjoncture (crise etc.)\n\nApproche paramétrique: Ici on fait l’hypothèse que les P&L sont distribués suivant un loi dont les paramètres sont calibrés sur les données historiques. Comme toute méthode paramétrique, elle peut conduire à des estimations très biaisées si la loi supposée n’est pas adaptée aux données\n\n\nVisualisation de la distribution des log-rendements journaliers\n\n\nCode\nimport seaborn as sns\n\nsns.histplot(portfolio.perf)\nsns.kdeplot(portfolio.perf, color='red')\n\n\n\n\n\n\n\n\n\n\n\nOn s’interesse aux VaR sur un horizon de 20 jours\n\n\n(a) VaR historique\n\n\nCode\nVaR = portfolio.quantile(1-0.99)*np.sqrt(20) # 1-day VaR scaled to 20-day VaR\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.09788038676774849\nla VaR  Relative à 99% d'horizon 20 jours  est : -0.08993825225681096\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\764873701.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\764873701.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 9.66%. Le porte feuille a 99% de chance de sous performer son indice de reference de 8.96% sur un horizon de 20 jours.\n\n\n(b) VaR paramétrique\n\n\nCode\nVaR = (portfolio.mean() + norm.ppf(1-0.99)*portfolio.std())*np.sqrt(20)\nprint(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nprint(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nla VaR  P&L à 99% d'horizon 20 jours  est : -0.08514685012292246\nla VaR  Relative  à 99% d'horizon 20 jours  est : -0.0715535793718315\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\937860044.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  P&L à 99% d'horizon 20 jours  est :\",VaR[1])\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\937860044.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(\"la VaR  Relative  à 99% d'horizon 20 jours  est :\",VaR[3])\n\n\nLa perte maximale attendue sur un horizon de 20 jours avec un niveau de confiance de 99% est de 8.47%. Le porte feuille a 99% de chance de sous performer son indice de reference de 7.10% sur un horizon de 20 jours.\nOn observe que les VaR paramétriques sont strictement inférieures aux VaR historiques. Ceci pourrait s’expliquer par le fait que les données ont des queues plus lourdes que la loi normale comme l’illustre le graphique présenté plus haut. De plus l’excess de kurtosis confirme cette observation. Par conséquent, la VaR obtenue sous une paramétrisation gaussienne sous-estime la perte maximale encourrue sur le portefeuille."
  },
  {
    "objectID": "risques/projets/liquidity.html#stress-test",
    "href": "risques/projets/liquidity.html#stress-test",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Stress test",
    "text": "Stress test\nLe stress test ou test de résistance est un exercice qui consiste à reproduire des scénarios extrêmes sur un portefeuille afin d’évaluer sa résilience en conjoncture défavorable. On distingue notamment deux types de stress test: - Le stress test historique qui vise à reproduire des scénarios de crises historisques, - Le stress test hypothétique qui consiste à considérer des scénarios théoriques extrêmes.\nNous allons dans la suite implémenter un stress test historique en reproduisant le choc COVID entre 19/02/2020 et 19/03/2020. Il s’agit s’agira de calculer la performance du fonds entre ces deux date et de la comparer à celle de son indice de référence.\n\nDonnées des actions en période covid\n\n\nCode\n# Données des actions en période covid\nstress_date_1 = \"2020-02-19\"\nstress_date_2 = \"2020-03-19\"\n\ntickers = ['SAN.PA', 'BN.PA',  'AIR.PA', 'AI.PA', 'ORA.PA', 'CAP.PA', 'VIV.PA', 'CA.PA', 'ENGI.PA', 'DG.PA']\ndf_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\ncac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\3326232529.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  df_stress = yf.download(tickers, start = stress_date_1, end=stress_date_2)['Close']\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**********            20%                       ]  2 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************80%*************          ]  8 of 10 completed[*********************100%***********************]  10 of 10 completed\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\3326232529.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n  cac40_stress = yf.download('^FCHI', start = stress_date_1, end=stress_date_2)['Close']\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\naum_stress = df_stress@weights\nportfolio_stress = pd.DataFrame({'AUM': aum_stress})\n\n\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\nportfolio_stress.iloc[-1,] /portfolio_stress.iloc[0,]  - 1\n\n\nAUM   -0.335339\ndtype: float64\n\n\n\n\nRendement du benchmark enn période covid\n\n\nCode\n# Rendemnt du benchmark enn période covid\ncac40_stress.iloc[-1,] / cac40_stress.iloc[0,] -1\n\n\nTicker\n^FCHI   -0.385585\ndtype: float64\n\n\nLe fonds stressé a une performance absolue de -33.69% contre -38.55% pour son benchmark. Ces performances en période de crise COVID sont du même ordre de grandeur. Toutefois, la perte enregistrée sur le fonds est moins importante. Une analyse des performances des actions constituant le fonds montre que la sous-performance enegistrée est essentiellement portée par: Air Liquide, Capgenie et Engie.\n\n\nRendement du portefeuille en période de covid\n\n\nCode\n# Rendement du portefeuille en période de covid\ndf_stress.iloc[-1,] /df_stress.iloc[0,]  - 1\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(df_close.columns.tolist(), df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1, color='skyblue')\n\nplt.title(f\"Performance de la composition du fond stressé\")\nplt.xlabel(\"Actifs\")\nplt.ylabel(\"Performance (%)\")\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\n\n# Ajout des étiquettes sur chaque barre\nfor bar, weight in zip(bars, df_stress.iloc[-1,] /df_stress.iloc[0,]  - 1):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n             f'{round(weight*100, 2)}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "risques/projets/liquidity.html#cadre-de-suivi-de-la-liquidité",
    "href": "risques/projets/liquidity.html#cadre-de-suivi-de-la-liquidité",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "Cadre de suivi de la liquidité",
    "text": "Cadre de suivi de la liquidité\n\nLe suivi de la liquidité est d’une importance centrale en gestion de fonds et est encadré par des exigences règlementaires. Il s’agit entre autres pour le gestionnaire de pouvoir honorer ses engagements vis-à-vis de des investisseurs en cas de rachat de parts du fonds. Il faut donc s’assurer de la capacité à revendre les actifs composant le fonds dans des délais courts sans toutefois les brader.\nC’est ce qu’on appelle le risque de liquidité qui se défini plus formellement comme étant la facilité avec laquelle une entreprise peut échanger ses actifs contre du cash, même en situation soudaine de besoin de liquidité, sans subir de coûts anormaux par rapport aux autres acteurs du marché.\nLe suivi de la liquidité d’un fonds s’effectue par le calcul de son profil d’écoulement en condition normale et en condition stressée.\n\nLa méthodologie de calcul d’un profil d’écoulement\nLe fonds est constitué de n actifs en quantité \\(Q_i\\) chacun à liquider. Dans la pratique il existe une quantité maximale au delà de laquelle les échanges ne peuvent se faire sans subir de coûts anormaux: c’est la profondeur de marché. Le coût de liquidation est induit par les volume d’ordre émis.\nLa profondeur de marché est généralement estimée en examinant: - Les carnets d’ordres ou - Les volumes quotidiens échangés: On considère notamment le volume quotidien moyen échangé sur 3 mois appelé ADV ou ATV (Average Daily/Traded Volume). La profondeur de marché est alors estimée par \\(Q^* = 20\\% \\times ADV_{3 mois}\\)\n\nOn distingue quatre profils d’écoulement: - En codition normale: avec et sans déformation - En condition stressée: avec déformation et sans déformation\nDans une liquidation avec déformation, les actifs les plus liquides sont liquidés en premiers. La déformation du portefeuille s’illustre par les changements de poids des différents émetteurs dans le fonds. La liquidation sans déformation quant à elle consistera à liquider le fonds au rythme de l’actif le moins liquide afin de préserver les poids initiaux.\nN.B: Le profil de liquidation est représenté en proportion du portefeuille initial.\nOn utilise le profil de liquidité pour déterminer la taille optimal du fonds permettant de respecter les exigence de minimum de liquidité. Il s’agira de s’assurer que plus de 90% du fonds est liquidé en deux jours, laissant ainsi un maximum de 10 % dans la catégorie dite “poubelle” (c’est-à-dire les actifs moins liquides).\nEn condition stressée avec déformation: On se place dans une situation dans laquelle la profondeur de marché est réduite à la suite d’un choc conduisant à un assèchement du marché.\n\n\nCode\ndef liqudity(df_volume, ADV_rate=0.2, weight_adjust=1, label=\"Conditions normales avec déformation\", seed = 42):\n    \"\"\"\n    Fonction de calcul de la liquidité d'un portefeuille sur plusieurs jours.\n    \n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        DataFrame contenant les volumes quotidiens pour chaque actif (tickers).\n    ADV_rate : float, optionnel\n        Taux de liquidité journalière exprimé en pourcentage de l'ADV (par défaut 0.2, soit 20%).\n    weight_adjust : float, optionnel\n        Facteur de pondération pour ajuster la quantité générée aléatoirement (par défaut 1).\n    label : str, optionnel\n        Label pour décrire le scénario de liquidation (par défaut \"Conditions normales avec déformation\").\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        DataFrame contenant les informations suivantes pour chaque actif :\n            - 'Ticker' : Le symbole de l'actif.\n            - 'ADV' : Le volume moyen quotidien calculé sur une fenêtre de 60 jours.\n            - 'Prix' : Le prix actuel de l'actif (extrait de df_close).\n            - 'QUANTITE' : La quantité totale simulée de l'actif détenu par le portefeuille.\n            - 'QUANTITE LIQUIDABLE' : La quantité qui peut être liquidée quotidiennement (ADV_rate * ADV).\n            - 'NB_JOURS DE LIQUIDATION' : Nombre de jours estimés pour liquider entièrement l'actif.\n            - 'JOUR X' (où X est un numéro de jour) : Quantité restante après chaque jour de liquidation.\n            - 'POIDS X' : Poids relatif de chaque actif dans le portefeuille après chaque jour de liquidation.\n    \n    Description :\n    -------------\n    1. Calcul de l'Average Daily Volume (ADV) sur une fenêtre de 60 jours.\n    2. Création d'un DataFrame contenant les informations de liquidité de chaque actif.\n    3. Génération de quantités d'actifs aléatoires pour simuler un portefeuille.\n    4. Calcul de la quantité liquidable par jour (20 % de l'ADV par défaut).\n    5. Simulation de la liquidation progressive de chaque actif jour par jour jusqu'à épuisement.\n    6. Calcul des poids du portefeuille après chaque jour de liquidation.\n    \n    \"\"\"\n    # Calcul de la moyenne mobile sur 60 jours du volume de transactions (ADV : Average Daily Volume)\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de l'ADV le plus récent pour chaque ticker\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame contenant l'ADV, le prix actuel et d'autres informations pour chaque actif\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # On définit la colonne 'Ticker' comme index pour faciliter l'accès aux données\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Génération de quantités d'actifs simulées basées sur l'ADV\n    np.random.seed(seed)  # Pour des résultats reproductibles\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable : 20 % de l'ADV (peut être ajusté via 'ADV_rate')\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours nécessaires pour liquider chaque actif\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(\n        portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE']\n    )\n\n    # Initialisation pour le jour 0\n    i = 1\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    # Calcul de la quantité restante après le premier jour de liquidation\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    # Calcul itératif jusqu'à liquidation complète\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        # Calcul de la quantité restante après chaque jour supplémentaire\n        portfolio_liquidity[f'JOUR {i}'] = (\n            portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n        ).clip(lower=0)\n\n        # Mise à jour des poids du portefeuille pour ce jour\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    return portfolio_liquidity\n\n\n\n\nCode\ndef liquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales avec déformation\"):\n    \"\"\"\n    Fonction qui génère un profil de liquidité d'un portefeuille en fonction du nombre de jours nécessaires pour liquider tous les actifs.\n\n    Paramètres :\n    -----------\n    portfolio_liquidity : DataFrame\n        Le DataFrame généré par la fonction liqudity() contenant les quantités restantes par jour pour chaque actif.\n    label : str, optionnel\n        Titre du graphique affiché (par défaut \"Profil de liquidité en conditions normales avec déformation\").\n\n    Description :\n    -------------\n    1. Calcule la quantité restante d'actifs pour chaque jour jusqu'à liquidation totale.\n    2. Calcule le profil de liquidité en pourcentage du portefeuille initial liquidé au fil du temps.\n    3. Génère deux graphiques :\n       - Profil de liquidité en pourcentage du portefeuille initial.\n       - Évolution des poids des actifs au fil des jours.\n\n    Retourne :\n    ---------\n    Affiche deux graphiques côte à côte montrant la liquidation progressive et l'évolution des poids.\n    \"\"\"\n    # Calculer le profil de liquidité\n    liquidity_profile = [portfolio_liquidity['QUANTITE']]\n    for i in range(1, int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        liquidity_profile.append(portfolio_liquidity[f'JOUR {i}'])\n\n    liquidity_profile = pd.DataFrame(liquidity_profile)\n\n    # Calculer le profil en pourcentage de liquidité restante\n    profile = liquidity_profile @ portfolio_liquidity['Prix']\n    portefeuille_valeur = profile['QUANTITE']  # Récupérer la valeur du portefeuille\n    profile = 100 - profile[1:,] * 100 / portefeuille_valeur\n\n    # Créer une figure avec deux sous-graphiques côte à côte\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))  \n\n    # Premier graphique : Profil de liquidité\n    ax1 = axes[0]\n    profile.plot(ax=ax1, marker='o', legend=False, color='blue')\n    barplot = profile.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black', alpha=0.5)\n\n    # Affichage des valeurs sur les barres\n    for bar in barplot.patches:\n        value = round(bar.get_height(), 2)\n        ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{value}\",\n                 ha='center', va='bottom', fontsize=10, color='black')\n\n    ax1.set_title(f\"{label}\", fontsize=14)\n    ax1.set_xlabel(\"Jours\", fontsize=12)\n    ax1.set_ylabel(\"Liquidité du portefeuille (%)\", fontsize=12)\n    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Étiquette de valeur du portefeuille\n    ax1.text(0.95, 0.05, f\"Valeur portefeuille : {round(portefeuille_valeur)} euros\",\n             transform=ax1.transAxes, fontsize=12, color='red', ha='right', va='bottom',\n             bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"red\"))\n\n\n    # Deuxième graphique : Évolution des poids\n    ax2 = axes[1]\n    for i in range(int(portfolio_liquidity['NB_JOURS DE LIQUIDATION'].max()) + 1):\n        if f'POIDS {i}' in portfolio_liquidity.columns:\n            ax2.plot(portfolio_liquidity[f'POIDS {i}'], label=f'Jour {i}')\n        else:\n            print(f\"Colonne 'POIDS {i}' non trouvée.\")\n\n    ax2.set_title(\"Évolution des Poids au Fil des Jours\", fontsize=14)\n    ax2.set_xlabel(\"Index\", fontsize=12)\n    ax2.set_ylabel(\"Poids\", fontsize=12)\n    ax2.legend()\n    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n\n    # Ajustement de l'affichage\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nProfil d’écoulement avec déformation\n\nProfil d’écoulement en conditions normales avec déformation\n\n\nCode\nportfolio_liquidity = liqudity(df_volume)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLe portefeuille étudié est liquidable en 8 jours au minimum. Au premier jour seul 23.17% du portefeuille est liquidable et atteint les 73.54% au 4eme jour. On observe que les poids des actifs composant le portefeuille sont déformés au fil des jours. Il y a alors une concentration du portefeuille sur les actifs les moins liquides. Cette situation est domageable pour les investisseurs restant dans le fonds, car il faudra plus de temps pour honorer les engagements du fonds vis-à-vis d’eux en cas de rachat ce qui augmente leur exposition au risque de liquidité..\nAfin de protéger ces investisseurs, le régulateur impose aux gérants de fonds de fixer des GATES qui doivent obligatoirement être présentés dans leurs prospectus. Cela consiste à donner une option au gérant pour restreindre les rachats quotidien, à 5% du fonds par exemple.\n\n\n\nProfil d’écoulement en conditions stressée avec déformation\nEn conjoncture défavorable, la profondeur de marché est réduite ce qui entraîne un contraction des volumes liquidables. Nous simulons un tel choc en réduisant la profondeur de marché à 10% l’ADV.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity)\n\n\n\n\n\n\n\n\n\n\nLa contraction de la profondeur de marché rallonge la durée minimale de liquidation du fonds à 15 jours contre 8 jours initialement. Ceci illustre bien l’exposition du fonds au risque de liquidité. Il devient donc indispensable pour le gérant du fonds de déterminer une taille optimale de celui-ci afin de réduire son exposition au risque de liquidité.\n\n\n\nTaille optimale du portefeuille\n\nOn peut ajuster la taille du portefeuille en faisant varier le paramètre weight_adjust de la fonction liqudity afin de déterminer la taille optimale du fonds liquidable en 1 jour.\nPour le cas étudié, le fonds optimal a une valeur de 123 184 238 euros dont 99.82% est liquidable en 1 jour.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\nEn repétant le scénario défavorable d’un choc qui réduit la profondeur de marché à 10% l’ADV, le fonds optimal ainsi constitué est liquidable à 99.82% en deux jours. Il est donc plus résilient.\n\n\nCode\nportfolio_liquidity = liqudity(df_volume, ADV_rate=0.1, weight_adjust=0.141) # Taille optimale du portefeuille\nliquidity_profile(portfolio_liquidity)\n\n# Afficher la taille du portefeuille 123 184 238\n\n\n\n\n\n\n\n\n\n\n\n\nProfil d’écoulement sans déformation\nRappelons qu’une politique de gestion sans déformation consiste à appliquer des rachats proformat. Ceci garantisse une stabilité de la composition du fonds.\n\n\nCode\ndef liqudity_proformat(df_volume, ADV_rate = 0.2, weight_adjust = 1):\n    \"\"\"\n    Fonction qui génère un DataFrame décrivant la liquidité d'un portefeuille sur plusieurs jours, en fonction de la quantité disponible par jour et des poids correspondants.\n\n    Paramètres :\n    -----------\n    df_volume : DataFrame\n        Un DataFrame contenant les volumes quotidiens de chaque ticker.\n    ADV_rate : float, optionnel\n        Le pourcentage d'Average Daily Volume (ADV) disponible chaque jour pour être liquidé (par défaut : 0.2, soit 20 %).\n    weight_adjust : float, optionnel\n        Facteur d'ajustement appliqué pour moduler la quantité calculée (par défaut : 1).\n\n    Description :\n    -------------\n    1. Calcule l'ADV (volume moyen sur 60 jours) pour chaque actif.\n    2. Génère un DataFrame contenant l'ADV, les prix actuels, les quantités à liquider, et les quantités liquidables par jour.\n    3. Calcule le nombre de jours nécessaire pour liquider complètement chaque actif.\n    4. Produit une série de DataFrames pour chaque jour, montrant la quantité restante, les poids associés, et les vitesses de liquidation.\n\n    Retourne :\n    ---------\n    portfolio_liquidity : DataFrame\n        Un DataFrame contenant les informations de liquidité du portefeuille sur plusieurs jours.\n    \"\"\"\n    # Calcul de la moyenne mobile du volume (ADV) sur une fenêtre de 60 jours\n    ADV = df_volume.rolling(window=60).mean()\n\n    # Extraction de la dernière ligne (les valeurs ADV les plus récentes)\n    latest_ADV = ADV.iloc[-1,]\n\n    # Création d'un DataFrame pour la liquidité du portefeuille\n    portfolio_liquidity = pd.DataFrame({'Ticker': tickers, 'ADV': latest_ADV.values, 'Prix': df_close.iloc[-1,].values})\n\n    # Définir 'Ticker' comme index pour un accès plus facile\n    portfolio_liquidity = portfolio_liquidity.set_index('Ticker')\n\n    # Calcul de la quantité (QUANTITE) basée sur le volume le plus récent\n    np.random.seed(42)\n    portfolio_liquidity['QUANTITE'] = weight_adjust * 1.5 * np.random.uniform(0, 1, 10) * portfolio_liquidity['ADV']\n\n    # Calcul de la quantité liquidable par jour\n    portfolio_liquidity['QUANTITE LIQUIDABLE'] = ADV_rate * portfolio_liquidity['ADV']\n\n    # Calcul du nombre de jours de liquidation nécessaire\n    portfolio_liquidity['NB_JOURS DE LIQUIDATION'] = np.ceil(portfolio_liquidity['QUANTITE'] / portfolio_liquidity['QUANTITE LIQUIDABLE'])\n\n    i = 1\n    # Initialisation du premier jour\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity['QUANTITE'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {0}'] = portfolio_liquidity[f'POIDS {0}'] / portfolio_liquidity[f'POIDS {0}'].sum()\n\n    portfolio_liquidity[f'JOUR {i}'] = portfolio_liquidity['QUANTITE'] - portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE'])\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n    portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n    portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity['QUANTITE'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity['QUANTITE']\n\n    while portfolio_liquidity[f'JOUR {i}'].max() &gt; 0:\n        i += 1\n        portfolio_liquidity[f'JOUR {i}'] = (\n            (portfolio_liquidity[f'JOUR {i-1}'] - portfolio_liquidity[f'JOUR {i-1}'] * portfolio_liquidity[f'SPEED {i-1}'].min())\n        ).clip(lower=0)\n\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'JOUR {i}'] * portfolio_liquidity['Prix']\n        portfolio_liquidity[f'POIDS {i}'] = portfolio_liquidity[f'POIDS {i}'] / portfolio_liquidity[f'POIDS {i}'].sum()\n\n        portfolio_liquidity[f'SPEED {i}'] = portfolio_liquidity[f'JOUR {i}'].clip(upper=portfolio_liquidity['QUANTITE LIQUIDABLE']) / portfolio_liquidity[f'JOUR {i}']\n\n    return portfolio_liquidity\n\n\n\nProfil d’écoulement en conditions normales sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions normales sans déformation\")\n\n\n\n\n\n\n\n\n\nLes actions sont liquidées à la vitesse de l’actif le moins liquide du portefeuille. Par conséquent, contrairement à un écoulement avec déformation, la quantité liquidable progresse plus lentement dans le temps. Par exemple, on observe qu’en quatre jours seuls 59,01 % du fonds peuvent être liquidés, contre 73,54 % dans un scénario avec déformation.\nPar ailleurs, les poids des différents actifs restent globalement stables au fil des jours, à l’exception du dernier jour où l’intégralité du fonds est liquidée. Cette stabilité implique qu’il n’y a pas de concentration progressive du risque de liquidité sur les actifs les moins liquides, ce qui constitue une caractéristique importante du mécanisme d’écoulement sans déformation.\n\n\nProfil d’écoulement en conditions stressée sans déformation\n\n\nCode\nportfolio_liquidity = liqudity_proformat(df_volume, ADV_rate=0.1)\nliquidity_profile(portfolio_liquidity, label=\"Profil d'écoulement en conditions stressée sans déformation\")\n\n\n\n\n\n\n\n\n\nUn choc sur la profondeur de marché a pour effet un rallongement de la durée minimale de liquidation du fonds. Toutefois, les poids restent bien stables."
  },
  {
    "objectID": "risques/projets/liquidity.html#ii.-risque-de-crédit-et-risque-de-taux",
    "href": "risques/projets/liquidity.html#ii.-risque-de-crédit-et-risque-de-taux",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "II. Risque de crédit et risque de taux",
    "text": "II. Risque de crédit et risque de taux\nLe risque de crédit est le risque de perte engendrée par la defaillance d’une partie prenante à remplir ses engagements contractuels préalablement établis. C’est principal risque observé sur périmètre du retail.\nOn définit le taux d’intérêt comme le loyer de l’argent (annualisé dans la pratique).\nCette définition est celle historique car très intuitive. Il peut cependant s’averer que les taux d’interêt soient négatifs. Là cette definition devient donc limitée.\nDepéndamment de ce qu’on fait de son argent, le thésauriser, le prêter à la banque , ou encore à l’etat, il existe toujours un risque de perte qui subsiste. Il peut donc advenir, que dans le souci de sécuriser son argent, dans un contexte particulier (notamment incertain), le posseusseur soit disposé à payer pour securiser son argent : on parle de taux d’intérêt négatif.\nCela illustre la métaphore du loyer du coffre fort.\n\nGénéralités sur le pricing d’obligations\nCONTEXTE\nEn raison des obligations réglementaires auxquelles les banques sont soumises, elles ne peuvent pas prêter à toutes les entreprises ayant besoin de financement. C’est dans ce contexte que la notion d’obligation prend son sens. La banque agit alors en tant qu’intermédiaire entre l’entreprise et le marché, et perçoit des frais de commission. le marché prête un montant M à l’entreprise et reçoit des annuités et le nominal à maturité.\nUne obligation est, économiquement, un prêt-emprunt.\nDe manière générale, la valorisation d’un actif est l’espérance des flux actualisés au taux sans risque sous la probabilité risque neutre :\n\\[\n\\begin{aligned}\nX_0 &= \\mathbb{E}[e^{-rT} X_T] \\\\\n    &= e^{-rT} \\, c \\times PS(T)\n\\end{aligned}\n\\]\nN.B : Le taux de recouvrement historique est de 40 %.\nLe recouvrement s’applique uniquement au nominal.\nLa probabilité de survie \\(PS(T)\\) est généralement déterminée à partir du modèle à intensité de Poisson via :\n\\[\nPS(T) = e^{-\\lambda T}\n\\]\nConsidérons une obligation d’échéances \\(T_i\\), \\(i = 1, \\dots, n\\), de coupon \\(c\\) et de nominal \\(N\\).\nLes coupons et le nominal sont payés en cas de survie, et le recouvrement en cas de défaut.\nLa valeur de cette obligation à la date \\(t\\) vaut :\n\\[\nC_t = \\sum_{i=1}^{n} c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}}\n\\]\nLa probabilité de survenue du défaut à une date \\(t\\) vaut :\n\\[\n\\begin{aligned}\nPD(t)\n    &= PS(t) - PS(t + dt) \\\\\n    &= -\\frac{PS(t+dt) - PS(t)}{dt} \\, dt \\\\\n    &= -\\frac{dPS(t)}{dt} \\, dt \\\\\n    &= \\lambda e^{-\\lambda t} \\, dt\n\\end{aligned}\n\\]\nLa valeur actualisée du recouvrement vaut :\n\\[\n\\mathcal{R}_0\n= \\int_0^T R \\lambda e^{-\\lambda t} e^{-rt} \\, dt\n= \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\]\nDe manière générale, pour une date \\(t\\) :\n\\[\n\\mathcal{R}_t\n= \\int_t^T R \\lambda e^{-\\lambda u} e^{-ru} \\, du\n= \\lambda R \\, e^{(r+\\lambda)t}\n  \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n  \\mathbf{1}_{\\{T \\ge t\\}}\n\\]\nLa valeur totale de l’obligation est alors :\n\\[\n\\begin{aligned}\nB_t\n&= \\sum_{i=1}^{n}\n    c \\, e^{-(\\lambda+r)(T_i - t)} \\mathbf{1}_{\\{T_i \\ge t\\}} \\\\\n&\\quad +\n\\left(\ne^{-(r+\\lambda)(T_n - t)}\n+\n\\lambda R \\, e^{(r+\\lambda)t}\n\\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\right)\n\\mathbf{1}_{\\{T_n \\ge t\\}}\n\\end{aligned}\n\\]\n\n\nImpementation de la valorisation d’un bond\n\n\nCode\n# Impementation de la fonctiuon de valorisation d'un bond\nimport numpy as np\n\ndef Bond(t,c,T,r,lamda, R = 0.4):\n    B = 0\n    for T_i in range(1,T+1):\n        B += np.exp(-(lamda + r)*(T_i - t))*(T_i&gt;=t)\n    B *= c\n    B += (np.exp(-(r + lamda)*(T_i-t)) + lamda * R  * (1-(np.exp(-(r + lamda)*(T_i -t)))) / (r + lamda))*(T_i&gt;=t)\n    return B\n\n\n\nExemple: Obligation au pair\n\nlamda = 0\n\n\nr = 0.02\n\n\nc = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nBond(t,c,T,r,lamda)\n\n\nCode\n# Exemple1: Obligation au pair\nlamda = 0\nr = 0.02\nc = 0.02\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.9981933497987289)\n\n\nAvec les paramètres ci-dessus considérés, on remarque que le prix de l’obligation est proche du nominal. En effet, le taux coupon est égal au taux de marché, ce qui indique que l’obligation est remunérée au taux du marché. Il s’agit donc d’une obligation au pair.\nAu cas où on aurait proposé une remunération supérieure à celle du marché, elle serait beaucoup plus attractive et sa valeur se serait appréciée. l’illustration est donnée ci dessous pour c= 0.03\n\n\nBond(0,0.03,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.03,10,0.02,0)\nprice\n\n\nnp.float64(1.0879246481591023)\n\n\nAvec une remunération inférieure à ce qu’aurait proposé le marché, on obtient une valorisation de l’obligation inférieure au nomimal\n\n\nBond(0,0.015,10,0.02,0)\n\n\nCode\nprice = Bond(0,0.015,10,0.02,0)\nprice\n\n\nnp.float64(0.9533277006185421)\n\n\nEtant donné une intensité de défaut supérieure à zéro, on peut également calculer le coupon pour lequel l’obligation est au pair. On obtient, après calculs prsentés ci-dessous, un taux coupon de 2.6%.\n\n\nCode\n# Recherche du coupon  pour émettre une obligation au pair.\ndef dichot(t,T,r,lamda, P_MKT):\n    c_inf = 1e-8\n    c_sup = 1\n    epsi = 1e-8\n    c_moy = (c_inf + c_sup)/2\n    error = c_sup - c_inf\n\n    while error&gt;epsi:\n        p_hw = Bond(t,c_moy,T,r,lamda)\n        if p_hw &gt; P_MKT:\n            c_sup = c_moy\n        elif p_hw &lt; P_MKT:\n            c_inf = c_moy\n        c_moy = (c_inf + c_sup)/2\n        error = np.abs(c_sup - c_inf)\n\n    return c_moy\n\n\n\n\n\nTaux coupon pour émettre une obligation au pair\n\nlamda = 0.01\n\n\nr = 0.02\n\n\nt = 0\n\n\nT = 10\n\n\nCode\n# Taux coupon pour émettre une obligation au pair\nlamda = 0.01\nr = 0.02\nt = 0\nT = 10\ndichot(t,T,r,lamda, P_MKT = 1)\n\n\n0.026393926193952293\n\n\nEn maintenant une remunération égale à celle du marché , avec une intensité de défaut très grande (de l’ordre de 1000%), l’obligation tombe presque instantannément en défaut. Dans cette situation, la valeur du coupon vaut alors 39.92% qui est sensiblement proche du taux de recouvrement (40%).\n\n\n\nValeur du bond poiur une intensité de défaut à 1000%\n\n\nCode\n# Valeur du bond poiur une intensité de défaut à 1000%\nlamda = 10\nr = 0.02\nc = 0.03\nt = 0\nT = 10\nBond(t,c,T,r,lamda)\n\n\nnp.float64(0.39920293189432754)\n\n\n\n\n\nÉvolution du prix de l’obligation en fonction du temps\n\n\nCode\nimport matplotlib.pyplot as plt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB = np.array([Bond(t,c,T,r,lamda) for t in times])\nplt.plot(times,B)\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.title(\"Evolution du prix plein coupon du Bond\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\nPlus on se rapproche de la date de détachement du coupon plus l’obligation devient attractive, elle prend donc de la valeur.\nChaque saut correspond à un détachement de coupons. une fois le coupon détaché de l’obligation, les flux à venir diminuent et la valeur de l’obligation se deprecie de la valeur du coupon qui a été détaché.\nCes sauts ne reflètent donc pas une dépréciation des bonds par le marché. Ce sont des sauts techniques. Raison pour laquelle on dit que le prix plein coupon est pollué par le coupon (en anglais Dirty price).\nOn va donc s’interesser par la suite au clean price ou pied de coupon qui correspond à au prix du bond moins le coupon couru.\n\\[\n\\tilde{B}_t = B_t - cc\n\\]\nOù (cc) est le coupon couru :\n\\[\ncc = c \\times (t - T^*)\n\\]\nEn retirant cette valeur de coupon couru, on supprime cet effet de saut après les detachements de coupons\n\nImplémentation du clean price\n\n\nCode\n# Implémentation du clean price\n\ndef CleanPrice(t,c,T,r,lamda, R = 0.4):\n    cc = c*(t - np.floor(t))*(t&lt;=T)\n    return Bond(t,c,T,r,lamda, R)-cc\n\n\n\n\nPieds coupon\n\n\nCode\n# Pieds coupon\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\ntimes = np.linspace(0,10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for t in times])\nB_pieds_coupon = np.array([CleanPrice(t,c,T,r,lamda) for t in times])\nplt.plot(times,B_pieds_coupon, label =\"Pieds coupon\")\nplt.plot(times,B_plein_coupon, label =\"Plein coupon\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nEn retirant cette valeur de coupon couru, on supprime l’effet de saut après les detachements de coupons, et on observe une courbe sans discontinuités\n\n\n\nEvolution du prix plein coupon et du clean price en fonction du temps pour differents coupons\n\n\nCode\nlamda = 0.01\nr = 0.02\nc = 0.01\nT = 10\n\ntimes = np.linspace(0,10, num = 100)\nB_plein_coupon1 = np.array([Bond(t,0.01,T,r,lamda) for t in times])\nB_pieds_coupon1 = np.array([CleanPrice(t,0.01,T,r,lamda) for t in times])\n\nB_plein_coupon5 = np.array([Bond(t,0.05,T,r,lamda) for t in times])\nB_pieds_coupon5 = np.array([CleanPrice(t,0.05,T,r,lamda) for t in times])\n\nplt.plot(times,B_pieds_coupon1, label =\"Pieds coupon 1%\")\nplt.plot(times,B_plein_coupon1, label =\"Plein coupon 1%\")\n\nplt.plot(times,B_pieds_coupon5, label =\"Pieds coupon 5%\")\nplt.plot(times,B_plein_coupon5, label =\"Plein coupon 5%\")\nplt.xlabel(\"Temps\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond\")\n\n\nText(0.5, 1.0, 'Evolution du prix du Bond')\n\n\n\n\n\n\n\n\n\nLes constats que l’on fait : - Lorsque c = 1% : La rémunération de l’obligation est inférieure à celle offerte par le marché. Ainsi, lors de son émission, sa valeur sera nécessairement inférieure à celle du pair, ce qui explique une valeur initiale proche de 87 %. À maturité, on reçoit 100 % du nominal, plus 1 % de ce dernier, correspondant au coupon. Le résultat final est donc sensiblement égal à 101 %.\n\nLorsque c = 5% : L’obligation rémunère plus que ce que le marché offre, ce qui la rend particulièrement attractive dès son émission. Ainsi, sa valeur de départ sera d’environ 120 %. Au fur et à mesure que les obligations sont détachées, les flux futurs diminuent, ce qui entraîne une dépréciation de sa valeur. À maturité, on reçoit 100 % du nominal, plus 5 % du montant nominal, correspondant au coupon. Le résultat final est donc de 105 %.\n\n\n\nEvolution du prix de l’obligation en fonction du taux d’intérêt\nDe l’expression analytique du prix du bond, on observe que le prix est décroissant du taux d’intérêt. La figure ci-dessous en donne une illustration, toute chose égale par ailleurs.\nOn vérifie graphiquement que le prix du bond est de 100% lorsque le taux d’intérêt est égal au taux sans risque \\(r^* = c - \\lambda (1 - R)\\).\n\nEvolution du prix du bond en fonction du taux d’intérêt\n\n\nCode\n# Evolution du prix du bond en fonction du taux d'intérêt\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR = 0.4\ninterest = np.linspace(0,0.10, num = 100)\n\nB_plein_coupon = np.array([Bond(t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest,B_plein_coupon, label =\"Plein coupon\")\nplt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\nplt.xlabel(\"Taux d'intérêt (%)\")\nplt.ylabel(\"Prix du Bond (%)\")\nplt.legend()\nplt.title(\"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24212\\2938139823.py:12: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.axvline(x=(c - lamda*(1 - R) ), color='red', linestyle='--', label='Taux sans risque $r^* = c - \\lambda (1 - R) $')\n\n\nText(0.5, 1.0, \"Evolution du prix du Bond en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotion de sensibilité\nLa sensibilité mesure la variation du prix du bond face à une variation d’un facteur de risque. On distingue entre autres la sensibilité de taux et la sensibilité de crédit.\nDans cette sectioin nous nous intéressons particulièrement à la sensibilité de taux. Elle est définie par la formule:\n\\[\n    Sensibilité = - \\frac{dB_t}{dr} \\frac{1}{B_t}\n\\]\nInterprétation: Lorsque le taux d’intérêt bouge de 1%, alors le prix du bond bouge de -sensibilité %.\nCette sensibilité peut également être vue comme le barycentre des différentes échéances pondérées par les flux actualisés. C’est la duration.\n\\[\n    \\frac{\\sum T_i\\times F_i}{\\sum F_i}\n\\]\nExemple: En considérant une obligation de maturité 3 ans payant des coupons annuels, avec intensité de défaut nul. Alors son prix et la sensibilité de taux sont données par:\n\\[\n\\begin{aligned}\nP &= c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial P}{\\partial r}\n&= -1c \\, e^{-1r} - 2c \\, e^{-2r} - 3c \\, e^{-3r}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Sensibilité}\n&= \\frac{1c \\, e^{-1r} + 2c \\, e^{-2r} + 3c \\, e^{-3r}}\n        {c \\, e^{-1r} + c \\, e^{-2r} + c \\, e^{-3r}}\n\\end{aligned}\n\\]\nD’où l’expression barycentrique des échéances pondérées par les flux.\n\n\nCode\n# Sensibilité de taux du prix du bond\ndelta_r = 1e-4 #1bp\n\n\ndef Sensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    return -(Bond(t,c,T,r+delta_r,lamda, R)-B_t)/(delta_r*B_t)\n\n\nOn considère un bond dont les caractérisiques sont les suivqantes:\n\\[\n    \\begin{cases}\n        \\lambda = 0.01\\\\\n        r = 0.02\\\\\n        c = 0.03\\\\\n        T = 10\\\\\n        R = 0.4\\\\\n    \\end{cases}\n\\]\nLa sensiblité de ce bond est de 8.64. Ainsi, lorsque le taux d’intérêt augmente de 1 point de pourcentage, le prix du bond diminue de 8.64%.\n\n\nCode\n# Sensibilité de taux d'intérêt\nSensi(t,c,T,r,lamda, R = 0.4, delta_r = 1e-4)\n\n\nnp.float64(8.643982489105056)\n\n\n\nEvolution de la sensibilité de taux fonction de la maturité\n\n\nCode\n# Evolution de la sensibilité au taux d'intérêt en fonction de la maturité\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,c,T,r,lamda) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité')\n\n\n\n\n\n\n\n\n\n\nOn constate que la sensibilité de taux croît avec la maturité et tend à être linéaire.\nPour des paramètres extrêmes \\(c = \\lambda = r = 0\\), la sensibilité (duration) est identique à la maturité comme illustré sur la figure ci-dessous. Cette remarque met en évidence la relation mlathématique entre la sensibilité et la maturité présentée plus haut.\nConnaissant la maturité, pour une variation du taux d’intérêt on peut donc donner une estimation “grossière” de la sensibilité (en approximant la duration par 0.8*maturité, par exemple).\n\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nmaturity = range(1,21)\ncensi = np.array([Sensi(t,1e-6,T,1e-6,1e-6) for T in maturity])\n\nplt.plot(maturity, censi, label =\"Duration\")\nplt.xlabel(\"Maturité (année)\")\nplt.ylabel(\"Duration\")\nplt.legend()\nplt.title(\"Sensibilité du prix du bond en fonction de la maturité (cas extrême)\")\n\n\nText(0.5, 1.0, 'Sensibilité du prix du bond en fonction de la maturité (cas extrême)')\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR d’une obligation.\nL’estimation de la VaR sur un bond peut se faire à partir de la senssibilité de taux ou par la méthode de repricing.\n\n\nEstimation de la VaR par la sensibilité du taux d’intérêt\nL’approche par la sensibilité se présente de la manière suivante:\nOn suppose que la dynamique du taux d’intérêt est donnée par \\[\\Delta r \\sim \\mathcal{N}(0, \\sigma \\sqrt{\\Delta t})\\].\nSachant que \\[\\frac{\\Delta P}{P} = - Duration \\times \\Delta r\\]\nil suit que \\[\\frac{\\Delta P}{P} \\sim \\mathcal{N}(0, Duration \\times \\sigma \\sqrt{\\Delta t})\\]\nUne approche par la sensibilité de la VaR à 99% donne \\[VaR = Duration \\times \\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\\] Où \\(z_{99\\%}\\) est le quantile d’ordre 99% de la loi normale standard.\nPour les besoins de notre exercice, on pose \\(\\sigma = 1\\%\\).\n\n\nCode\nfrom scipy.stats import norm\n\n# Calcul de la VaR par la sensibilité du taux d'intérêt\ndef Sensi_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    duration = Sensi(t,c,T,r,lamda, R)\n    var = duration*sigma*np.sqrt(H)*norm.ppf(alpha)\n    return var\n\n\n\n\nCode\nsigma= 0.01\nH = 1/12\nt = 0\nlamda = 0.01\nr = 0.02\nc = 0.03\nT = 10\nR =0.40\n\nSensi_VaR(sigma , H, t,c,T,r,lamda)\n\n\nnp.float64(0.05804942383590022)\n\n\nLa VaR à 99% d’horizon 1 an sur le bond est de 5.8%. - On note que la VaR est une fonction linéaire croissante de la volatilité. C’est une conséquence directe de l’hypothèse de normalité des variations du taux d’intérêt. Ainsi, une augmentation de la volatilité du taux d’intérêt s’accompagne d’une augmentation de la volatilité du prix du bond. - La VaR est une fonction décroissante du taux d’intérêt. En effet, une augmentation du taux d’intérêt entraîne une diminution de la valeur des bonds, et par conséquent des valeurs extrêmes atteintes par celles-ci; d’où la VaR décroît. Toutefois l’évolution de la VaR en fonction du taux d’intérêt n’est pas linéaire.\n\n\nEvolution de la Z-VaR en fonction du taux d’intérêt\n\n\nCode\n# Evolution de la VaR en fonction de la volatilité\nvol = np.linspace(0,1e-2, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for sigma in vol])\n\nplt.plot(vol, var, label =\"VaR_99%\")\nplt.xlabel(\"Volatilité\")\nplt.ylabel(\"VaR \")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de la volatilité\")\n\n\nText(0.5, 1.0, 'VaR à 99% à horizon 1 an en fonction de la volatilité')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Evolution de la VaR en fonction du taux d'intérêt\ninterest = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nEvolution de la Z-VaR en fonction de l’intensité de défaut\nPour des intensités de défaut croissantes, la VaR du bond décroît de façon exponentielle, ceteris paribus. Tout comme avec le taux d’intérêt, une augmentation de l’intensité de défaut s’accompagne d’une imminence du défaut et par conséquent de la diminution de la valeur du bond. D’où une diminution de la VaR.\n\n\nCode\n# Evolution de la VaR en fonction de l'intensité de défaut\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([Sensi_VaR(sigma , H, t,c,T,r,lamda) for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de défaut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de saut\")\n\n\n\n\n\n\n\n\n\n\n\nEstimation de la VaR par repricing de l’obligation\nLa méthodologie consiste à revaloriser le bond pour une variation du taux d’intérêt, puis d’en déduire la variation du prix du bond qui en résulte.\n\\[\n    \\Delta r^* =\\sigma \\times  \\sqrt{\\Delta t} \\times z_{99\\%}\n\\]\n\\[\n    VaR_{99\\%} = \\frac{B(r+ \\Delta r^*) - B(r)}{B(r)}\n\\]\n\n\nApproche repricing\n\n\nCode\n# Approche repricing\n\ndef repricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99):\n    delta_r = sigma*np.sqrt(H)*norm.ppf(alpha)\n    B_rep = Bond(t,c,T,r+delta_r,lamda, R)\n    B_r = Bond(t,c,T,r,lamda, R)\n    \n    return  (B_r - B_rep)/B_r\n\n\n\n\nCode\nrepricing_VaR(sigma, H, t,c,T,r,lamda, R = 0.4, alpha=0.99)\n\n\nnp.float64(0.05627224378244837)\n\n\n\nLa VaR obtenue par l’approche repricing, sous les mêmes conditions que précédemment, est de 5.6%. Cette VaR est inférieure à celle obtenue sous l’hypothèse de normalité des variations du taux d’intérêt.\nCeci s’explique par le fait que la duration est une fonction convexe décroissante du taux d’intérêt et représente une approximation affine de la valeur du bond en le taux d’intérêt. De manière générale, l’approche par les sensibilités surestime la VaR.\nL’évolution de la VaR, obtenue par reprincing, en fonction du taux d’intérêt ou de l’intensité de défaut (voir les graphiques ci-dessus) à la même allure que la z-VaR.\nEn règle générale la VaR doit être inférieure au seuil de 20% de par la limite règlementaire. L’utilisation de la z-VaR peut donc constituer un manque à gagner pour les institutions financieres. En effet, elles peuvent être amenées à dérisquer leur portefeuille en limitant leur investissements pour des raisons purement techniques.\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\ninterest = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda) for r in interest])\n\nplt.plot(interest, var, label =\"VaR_99%\")\nplt.xlabel(\"Taux d'intérêt\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction du taux d'intérêt\")\n\n\n\n\n\n\n\n\n\n\n\nValeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\n\n\nCode\n# Valeur extrême de la sensibilité (cas extrême de nullité de lambda, r et c)\nlambdas = np.linspace(0,1, num=100)\nvar = np.array([repricing_VaR(sigma , H, t,c,T,r,lamda)for lamda in lambdas])\n\nplt.plot(lambdas, var, label =\"VaR_99%\")\nplt.xlabel(\"Intensité de saut\")\nplt.ylabel(\"VaR\")\nplt.legend()\nplt.title(\"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")\n\n\nText(0.5, 1.0, \"VaR à 99% à horizon 1 an en fonction de l'intensité de défaut\")"
  },
  {
    "objectID": "risques/projets/liquidity.html#iii.-risque-de-contrepartie",
    "href": "risques/projets/liquidity.html#iii.-risque-de-contrepartie",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "III. Risque de contrepartie",
    "text": "III. Risque de contrepartie\nDans le cadre d’un engagement contratuel incluant deux parties prenantes, le risque de contrepartie fait reférence au rique de perte suite au défaut d’une des parties à remplir les engagements contractuels pré-établis: c’est un risque bilatéral.\nAfin de se couvrir du defaut de l’émetteur d’un bond, l’acheteur du bond peut entrer dans un credit default swap (CDS). Dans un tel contrat, lorsque la contrepartie est correlée à l’émetteur de l’obligation on parle de wrong way risk ce qui a pour conséquence d’exposer davantage le souscripteur. La contrepartie doit donc être décorrelée de l’émetteur.\nLe risque de contrepartie est en général mitigé par les appels de marge (chambre de compensation). - Quand la qualité de l’émetteur se dégrade, son spread de crédit spread croît et le CDS s’apprécie. - Si la contrepartie fait défaut, le détenteur perd \\(CDS = 1-R\\). - Un mécanisme d’appel de marge permet de mitiger le risque.\n\nLien entre CDS et obligation: la formule du triangle de crédit\nUn estimation du taux sans risque d’une obligation: \\[\n\\begin{aligned}\nP &= \\sum_{i=1}^{n} c \\, e^{-(r+\\lambda)T_i}\n    + e^{-(r+\\lambda)T_n}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T_n}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\int_0^T e^{-(r+\\lambda)t} \\, dt\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP &= c \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n    + e^{-(r+\\lambda)T}\n    + \\lambda R \\, \\frac{1 - e^{-(r+\\lambda)T}}{r+\\lambda}\n\\end{aligned}\n\\]\nL’obligation est au pair si et seulement si P = 1 Ainsi, \\[\n    (c + \\lambda R) \\frac{ 1- e^{-(r + \\lambda )T}}{r + \\lambda} + e^{-(r + \\lambda)T} = 1\n\\]\nQuand T tend vers l’infini le taux sans risque vaut alors, \\[\n    (c + \\lambda R)  = r + \\lambda\n\\]\n\\[\n    (c - r )  = \\lambda (1 - R)\n\\] - \\(c-r\\): spread ou prime de risque - \\(\\lambda\\): intensité de défaut - \\(1-R\\): LGD \\[\n    Spread = PD \\times LGD\n\\]\n\n\nCDS: principe\n\nPremium Leg: est payé tant que l’émetteur est “en vie”. \\[ PL = \\int_0^T s \\times e^{-rt} \\times e^{-\\lambda t}dt = s \\frac{1-e^{-(r+\\lambda)T}}{r + \\lambda}\\]\nDefault Leg: compensation à la date t si le défaut y survient. \\[DL = \\int_0^T(1-R) e^{-rt}\\times e^{-\\lambda t} dt = \\lambda (1-R) \\frac{1-e^{-(r+\\lambda)T}}{r+\\lambda}\\].\n\nLa valeur initiale d’un spread étant nulle, les deux jambes sont égales et il on obtient:\n\\[\n    s = \\lambda (1-R)\n\\]\nUn estimation de ce spread peut donc être déduite de la formule du triangle de crédit.\n\n\nSensibilité de crédit\nLa sensibilité de crédit d’une obligation est la variation du prix de l’obligation lorsque varie le spread de crédit de la contrepartie. De la formule du triangle, la sensibilité de crédit peut être déduite de la sensibilité par rapport à l’intensité de défaut \\(\\lambda\\).\n\\[\n    \\frac{\\partial P}{\\partial s} = \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda}\n\\]\n\\[\n    \\frac{\\partial P}{\\partial s} \\times \\frac{1}{P}= \\frac{1}{1 - R} \\frac{\\partial P}{\\partial \\lambda} \\times  \\frac{1}{P}\n\\]\nOn peut calculer une VaR taux et une VaR crédit. Toutefois, la VaR qui sera regardée de près est celle issue de la variation conjointe des taux et des spreads de crédit.\n\nSensibilité de crédit du bond\n\n\nCode\n# Sensibilité de crédit du bond\ndelta_lambda = 1e-4 #1bp\n\ndef Sensi_credit(t,c,T,r,lamda, R = 0.4, delta_lambda = 1e-4):\n    B_t = Bond(t,c,T,r,lamda, R)\n    variation = -(Bond(t,c,T,r,lamda+delta_lambda, R)-B_t)/(delta_r*B_t)\n    return variation/(1-R)\n\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda)\n\n\nnp.float64(8.82119086802735)\n\n\n\nUne variation d’un point de pourcentage du spread de crédit entraîne une variation de 8.82% de la valeur du bond. Cette sensibilité de crédit est assez proche de la sensibilité taux. En effet, on observe de l’écriture mathématique de la valeur du bond que le taux d’intérêt et l’intensité apparaissent conjointement de façon additive (sauf pour le terme de recouvrement).\nCes deux sensibilitées sont identiques pour un taux de recouvrement nul.\n\n\n\nCode\nSensi_credit(t,c,T,r,lamda, R = 0.0, delta_lambda = 1e-4)\n\n\nnp.float64(8.77911216566412)\n\n\n\n\n\nEstimation de la VaR par variation conjointe du taux d’intérêt et du spread de crédit\nOn suppose que la dynamique du taux d’intérêt est donnée par l’EDS\n\\[\n    dr_t = \\sigma dW_t\n\\]\nLe spread ne prenant pas de valeur négative, on supposera qu’il est log-normal. Sa dynamique est donnée par l’EDS\n\\[\n    \\frac{ds_t}{s_t} = \\alpha dZ_t\n\\]\nEn supposant \\(dW_t dZ_t = \\rho\\) et \\(Z_t = \\rho W_t + \\sqrt{1-\\rho^2}V_t\\) tel que \\(dV_t dW_t = 0\\),\nLa dynamique du spread s’écrit alors \\[\n\\frac{ds_t}{s_t} = \\alpha (\\rho W_t + \\sqrt{1-\\rho^2}V_t)\n\\]\nEn appliquant un schéma de discrétisation d’Euler, on peut alors simuler les taux \\(r_t\\) et les spreads \\(s_t\\) par:\n\\[\n    r_H = r + \\sigma \\sqrt{H} W_H\n\\]\n\\[\n    s_H = s(1 + \\alpha \\rho \\sqrt{H} W_H + \\alpha \\sqrt{1-\\rho^2} \\sqrt{H} V_H)\n\\]\nOù H est le pas de discrétisation.\nLa méthodologie de détermination de la sensibilité du prix d’une obligation conjointement au taux d’intérêt et au spread de crédit est la suivante: - Calculer la valeur du bond pour les paramètres initiaux - Simuler les taux \\(r_H\\) et les spreads \\(s_H\\) - Pour chaque simulation, calculer la valeur du bond (cetertis paribus) - pour déterminer la variation des nouveaux prix par rapport au prix initial - calculer le quantile d’ordre 99% des variations obtenues\n\n\nCode\n# Sensibilité conjointe de taux et de crédit\n\ndef SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12, M=10_000):\n    \n    u1 = np.random.uniform(0,1, size = M)\n    u2 = np.random.uniform(0,1, size = M)\n    w = norm.ppf(u1)\n    v = norm.ppf(u2)\n    s = lamda*(1-R)\n    \n    r_H = r + sigma*np.sqrt(H)*w\n    s_H = s*(1 + alpha*rho*np.sqrt(H)*w + alpha*np.sqrt(1-rho**2)*np.sqrt(H)*v)\n    \n    P_0 = Bond(0,c,T,r,lamda, R)\n    P_1 = Bond(H,c,T,r_H,s_H/(1-R), R)\n    mu = (P_1 - P_0)/P_0\n    return  - np.quantile(mu,0.01)\n\n\n\n\nCode\nsigma = 0.01 # Volatilité du taux d'intérêt\nalpha =0.4   # Volatilité du spread\nrho = 0.4    # Corrélation entre le spread et le taux d'intérêt\n\nnp.random.seed(90) # Pour la reproductibilité\nSimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12)\n\n\nnp.float64(0.0602691861203251)\n\n\n\n\nCode\nnp.random.seed(90)\n\nrhos = np.linspace(0,1, num= 1000)\nvar = np.array([SimSpread(c,T,r,lamda, R, alpha, rho, H = 1/12) for rho in rhos])\n\nplt.plot(rhos, var)\n\n\n\n\n\n\n\n\n\nLa corrélation \\(\\rho\\) doit être positive, car une augmentation des taux d’intérêt entraîne une meilleure rémunération du marché par rapport aux obligations. Pour rester attractifs, les coupons doivent alors augmenter. Cette hausse des coupons accroît le coût de financement de l’émetteur, ce qui conduit à une augmentation du spread de crédit. Il en résulte une corrélation positive entre les taux d’intérêt et les spreads de crédit.\nUne augmentation conjointe des taux et des spreads accroît le risque de défaut, en raison de la corrélation positive entre ces deux facteurs de risque. Ainsi, une corrélation élevée reflète un manque de diversification, ce qui expose davantage le portefeuille au risque systémique"
  },
  {
    "objectID": "risques/projets/liquidity.html#iv.-risque-modèle",
    "href": "risques/projets/liquidity.html#iv.-risque-modèle",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "IV. Risque modèle",
    "text": "IV. Risque modèle\nLe risque modèle désigne le risque associé à l’utilisation d’un modèle mathématique ou statistique qui ne représente pas correctement la réalité ou qui mène à des décisions erronées en finance et en gestion des risques.\nEléments du suivi du risque modèle: - Sanity check: Ecart de performance - Backtesting: Application aux données antérieures - Comparaison avec desmodèles plus riches - Provisionner au titre du risque de modèle"
  },
  {
    "objectID": "risques/projets/liquidity.html#v.risque-climatique",
    "href": "risques/projets/liquidity.html#v.risque-climatique",
    "title": "Introduction à la gestion des risques en Asset Management",
    "section": "V.Risque climatique",
    "text": "V.Risque climatique\nLe risque climatique désigne les risques financiers liés au changement climatique et aux politiques de transition vers une économie bas carbone. Sa modélisation reste complexe, notamment en raison du manque de données historiques et de l’absence d’un cadre réglementaire clairement défini.\nOn distingue deux principaux types de risques climatiques :\n\nRisque physique: l correspond aux pertes financières causées par des événements climatiques extrêmes. Il est défini par deux éléments clés : la fréquence et la sévérité des événements.\n\nRisque aigu : Événements rares mais très intenses ( ouragans, inondations).\nRisque chronique : Changements progressifs et durables (élévation du niveau de la mer).\n\nRisque de transition: il découle de l’adaptation des entreprises et des acteurs économiques aux exigences de transition écologique.\n\nRisque politique et réglementaire : Durcissement progressif des régulations environnementales (taxe carbone).\nOpportunités technologiques : Innovations favorisant la transition énergétique et redéfinissant les modèles économiques.\n\n\nLa resilience des institutions financieres notamment aux changement climatique est évaluée via des stress tests climatiques dont les scénarios sont issus des données du NGFS (Network for Greening financial System)."
  },
  {
    "objectID": "risques/projets/CreditVaR.html",
    "href": "risques/projets/CreditVaR.html",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "",
    "text": "⬅ Retour\n\nCONTEXTE\n\nL’objectif de la présent étude est d’évaluer, au moyen d’une CVAR à 99%, le risque de crédit sur un portefeuille fictif composé de deux créances, issues du secteur bancaire, de même notionnel 1000 EUR et de même maturité 4 ans.\nLa première est une obligation BNP senior de taux de recouvrement de moyenne 60% et de volatilité 15%, et la seconde est une obligation Société Générale junior (ou subordonnée) de taux de recouvrement de moyenne 30% et de volatilité 25%. On suppose qu’il n’y a pas de dépendance entre les taux de recouvrement.\nLa perte de crédit attendue pour ce portefeuille peut s’écrire sous la forme mathématique suivante\n\\(L= EAD_{BNP} (1 - R_{BNP}) \\mathbf{1}_{\\tau_{BNP} \\leq 4} + EAD_{SG} (1 - R_{SG}) \\mathbf{1}_{\\tau_{SG} \\leq 4}\\) où\n\n\\(EAD_i\\) est la perte en cas de défaut. Dans notre cas elle au notionnel;\n\\(R_i\\) est le taux de recouvrement;\n\\(\\tau_i\\) est le moment de survenu du défaut.\n\nNous souhaitons retrouver une CVAR à 99% qui s’écrit \\(CVAR_{99\\%} = inf\\{x, \\mathbf{P}(L \\leq x) \\geq 99\\%\\}\\).S\nDans l’expression donnée de la perte, on note la présence de variables aléatoires (le taux de recouvrement et le moment de survenu du défaut). La caractérisation des pertes est donc conditionnée par la connaissance des distributions de ces variables. Nous allons nous evertuer tout le long de ce projet à modéliser cette VaR et pour ce faire, nous procédons suivant une méthodologie s’appuyant sur la théorie des copules que nous déroulons de manière détaillée dans le rapport suivant: ➡ Rapport\n\nRetrouvez l’essentiel des codes implémentés dans le cadre de projet ici ➡ Codes"
  },
  {
    "objectID": "risques/projets/CreditVaR.html#a.-effectuer-une-analyse-exploratoire-univariée-des-données-actions-de-ces-deux-entreprises",
    "href": "risques/projets/CreditVaR.html#a.-effectuer-une-analyse-exploratoire-univariée-des-données-actions-de-ces-deux-entreprises",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "3.a. Effectuer une analyse exploratoire univariée des données actions de ces deux entreprises",
    "text": "3.a. Effectuer une analyse exploratoire univariée des données actions de ces deux entreprises\nOn s’intéresse ici à l’historique des prix bruts des actions mais également à celui des log rendements car ils representent une mesure normalisée qui permet de mieux apprecier la volatilité des prix.\nLes log rendements étant définis comme suit :\n$$\n = ()\n$$\nOù :\n\n$ _t $ est le prix de l’action à l’instant t,\n$ _{t-1} $ est le prix de l’action à l’instant t-1.\n\n\n\nCode\n# Chargement des données\ndata = pd.read_csv(\"../projets/data.txt\", sep=\"\\s+\", header=0)\ndata.head()\n\n\n&lt;&gt;:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n&lt;&gt;:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_24072\\3643887668.py:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n  data = pd.read_csv(\"../projets/data.txt\", sep=\"\\s+\", header=0)\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n0\n42.36\n55.24\n\n\n1\n42.72\n55.59\n\n\n2\n43.20\n56.45\n\n\n3\n42.67\n55.55\n\n\n4\n41.81\n54.50\n\n\n\n\n\n\n\n\nStatistiques descriptives des rendements et des log rendements.\n\n\nCode\n## principales caractéristiques de tendances centrales et de dispersion  des prix des actions\ndata.describe()\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n1000.000000\n1000.000000\n\n\nmean\n31.906254\n43.532641\n\n\nstd\n9.630129\n10.007847\n\n\nmin\n14.056200\n21.667700\n\n\n25%\n23.450325\n36.065350\n\n\n50%\n34.353700\n46.824800\n\n\n75%\n40.229375\n51.291675\n\n\nmax\n48.330000\n60.680000\n\n\n\n\n\n\n\n\n\nCode\n## Calcul des log rendements\n\n\n\n\nCode\ndata_returns = np.log(data/data.shift(1)).dropna()\ndata_returns.head(3)\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n1\n0.008463\n0.006316\n\n\n2\n0.011173\n0.015352\n\n\n3\n-0.012344\n-0.016072\n\n\n\n\n\n\n\n\n\nCode\n## principales caractéristiques de tendances centrales et de dispersion log rendements des actions\ndata_returns.describe()\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n999.000000\n999.000000\n\n\nmean\n-0.000888\n-0.000687\n\n\nstd\n0.024441\n0.020731\n\n\nmin\n-0.123524\n-0.098292\n\n\n25%\n-0.013867\n-0.011830\n\n\n50%\n-0.000358\n-0.000547\n\n\n75%\n0.012526\n0.011223\n\n\nmax\n0.083225\n0.076478\n\n\n\n\n\n\n\nNous procédons ci après à des statistiques descriptives plus exhausives. Incluant notamment des test de stationnarité, l’évaluation de l’asymétrie et des niveaux d’aplatissement de nos distributions\n\n\nCode\nfrom scipy.stats import shapiro\nfrom statsmodels.tsa.stattools import adfuller\n\ndef analyze_data(df, name=\"Dataset\"):\n    \"\"\"\n    Analyse les données du DataFrame fourni et retourne un DataFrame avec :\n    - Statistiques descriptives\n    - Tests de stationnarité (Dickey-Fuller)\n\n    Paramètres :\n    df : DataFrame : Doit contenir les colonnes 'Close' et 'Rendement'\n    name : str : Nom du dataset (optionnel)\n\n    Retourne :\n    results : DataFrame avec toutes les analyses\n    \"\"\"\n    # Vérification de la présence des colonnes nécessaires\n    if not {'Close', 'Rendement'}.issubset(df.columns):\n        raise ValueError(\"Le DataFrame doit contenir les colonnes 'Close' et 'Rendement'.\")\n\n    # Statistiques descriptives\n    results = df[['Close', 'Rendement']].describe()\n    results['Indicateur'] = [\"Nombre\", \"Moyenne\", \"Ecart-Type\", \"Minimum\", \"Percentile 25%\", \"Médiane\", \"Percentile 75%\", \"Maximum\"]\n    results = results[['Indicateur', 'Close', 'Rendement']]\n\n    # Test de stationnarité (Dickey-Fuller)\n    _, p_value_rend = adfuller(df['Rendement'].dropna())[:2]\n    _, p_value_close = adfuller(df['Close'].dropna())[:2]\n\n    # Kurtosis \n    kurtosis_rend = df['Rendement'].dropna().kurt()\n    kurtosis_close = df['Close'].dropna().kurt()\n\n    # skewness\n    skew_rend = df['Rendement'].dropna().skew()\n    skew_close = df['Close'].dropna().skew()\n    results = pd.concat([results, pd.DataFrame([[\"Stationnarité (p)\", p_value_close, p_value_rend ]], columns=results.columns)])\n    results = pd.concat([results, pd.DataFrame([[\"Asymétrie\", skew_close , skew_rend ]], columns=results.columns)])\n    results = pd.concat([results, pd.DataFrame([[\"Excess Kurtosis\", kurtosis_close, kurtosis_rend ]], columns=results.columns)])\n  \n    return results\n\ndata_bnp = pd.concat([data[\"BNP\"], data_returns[\"BNP\"]], axis=1)\ndata_bnp.columns = ['Close', 'Rendement']  # Renommage des colonnes\n\nanalyze_data(data_bnp)\n\n\n\n\n\n\n\n\n\nIndicateur\nClose\nRendement\n\n\n\n\ncount\nNombre\n1000.000000\n999.000000\n\n\nmean\nMoyenne\n31.906254\n-0.000888\n\n\nstd\nEcart-Type\n9.630129\n0.024441\n\n\nmin\nMinimum\n14.056200\n-0.123524\n\n\n25%\nPercentile 25%\n23.450325\n-0.013867\n\n\n50%\nMédiane\n34.353700\n-0.000358\n\n\n75%\nPercentile 75%\n40.229375\n0.012526\n\n\nmax\nMaximum\n48.330000\n0.083225\n\n\n0\nStationnarité (p)\n0.794391\n0.000000\n\n\n0\nAsymétrie\n-0.310893\n-0.331426\n\n\n0\nExcess Kurtosis\n-1.233209\n2.016973\n\n\n\n\n\n\n\n\n\nCode\ndata_sg = pd.concat([data[\"SG\"], data_returns[\"SG\"]], axis=1)\ndata_sg.columns = ['Close', 'Rendement']  # Renommage des colonnes\n\nanalyze_data(data_sg)\n\n\n\n\n\n\n\n\n\nIndicateur\nClose\nRendement\n\n\n\n\ncount\nNombre\n1000.000000\n9.990000e+02\n\n\nmean\nMoyenne\n43.532641\n-6.874575e-04\n\n\nstd\nEcart-Type\n10.007847\n2.073120e-02\n\n\nmin\nMinimum\n21.667700\n-9.829201e-02\n\n\n25%\nPercentile 25%\n36.065350\n-1.182979e-02\n\n\n50%\nMédiane\n46.824800\n-5.468895e-04\n\n\n75%\nPercentile 75%\n51.291675\n1.122288e-02\n\n\nmax\nMaximum\n60.680000\n7.647830e-02\n\n\n0\nStationnarité (p)\n0.736159\n8.678531e-09\n\n\n0\nAsymétrie\n-0.489312\n-1.797332e-01\n\n\n0\nExcess Kurtosis\n-0.933549\n2.097393e+00\n\n\n\n\n\n\n\n\nCommentaire :\nLes cours des actions de BNP et de société générale sont tous deux assez variables sur la période d’étude. Les log rendements associés sont très volatiles, avec une dispersion allant jusqu’à l’ordre de 30 fois supérieur à la valeur absolue de la moyenne\n\n\n\nEvolution du cours des actions de BNP et SG ainsi que celle de leurs log rendements\nCette evolution permet d’illustrer de maniere visuelle la forte volatilité des rendements des actions de BNP et de SG\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nax.plot(data[\"BNP\"], 'r-', lw=1, alpha=0.8, label='BNP')\nax.plot(data[\"SG\"], 'b-', lw=1, alpha=0.8, label='SG')\nax.set_title(\"Évolution du cours de l'action de BNP et SG\", fontsize=16)\nax.set_xlabel('Temps', fontsize=14)\nax.set_ylabel(\"Cours de l'action\", fontsize=14)\nax.tick_params(axis='both', which='major', labelsize=12)\nax.grid(True, linestyle='--', alpha=0.7)\nax.legend(loc='best', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n\n# Graphique pour BNP\nax1.plot(data_returns[\"BNP\"], 'r-', lw=1, alpha=0.8, label='BNP')\nax1.set_title(\"Évolution du cours des rendements de l'action BNP\", fontsize=16)\nax1.set_xlabel('Temps', fontsize=14)\nax1.set_ylabel(\"Rendements des actions\", fontsize=14)\nax1.tick_params(axis='both', which='major', labelsize=12)\nax1.grid(True, linestyle='--', alpha=0.7)\nax1.legend(loc='best', fontsize=12)\n\n# Graphique pour SG\nax2.plot(data_returns[\"SG\"], 'b-', lw=1, alpha=0.8, label='SG')\nax2.set_title(\"Évolution du cours des rendements de l'action SG\", fontsize=16)\nax2.set_xlabel('Temps', fontsize=14)\nax2.set_ylabel(\"Rendements des actions\", fontsize=14)\nax2.tick_params(axis='both', which='major', labelsize=12)\nax2.grid(True, linestyle='--', alpha=0.7)\nax2.legend(loc='best', fontsize=12)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "risques/projets/CreditVaR.html#b-modéliser-judicieusement-les-distributions-univariées-des-facteurs-de-risques",
    "href": "risques/projets/CreditVaR.html#b-modéliser-judicieusement-les-distributions-univariées-des-facteurs-de-risques",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "3.b Modéliser judicieusement les distributions univariées des facteurs de risques",
    "text": "3.b Modéliser judicieusement les distributions univariées des facteurs de risques\n\nHistogrammes et courbes de densité des log rendements des actions de BNP et SG\n\n\nCode\ndef HistogPlot(data, label):\n    sns.kdeplot(data, color = \"r\", lw=2, alpha=0.7, label=label)  # Density\n    n, bins, patches = plt.hist(\n        data, bins=20, density=True, histtype=\"stepfilled\", alpha=0.5\n    )  # Histogram\n    \n    \n    ymax = plt.ylim()[1]  # Get the max y value for positioning text\n\n    # Ajout de la moyenne\n    plt.axvline(x=np.mean(data))\n    plt.text(np.mean(data), ymax * 0.5, \"Moyenne\", rotation=90)\n\n    # Ajout de la médiane\n    plt.axvline(x=np.median(data))\n    plt.text(np.median(data), ymax * 0.7, \"Médiane\", rotation=90)\n\n    # Adjout du mode\n    plt.axvline(x=(bins[n.argmax()] + bins[n.argmax() + 1]) / 2)\n    plt.text((bins[n.argmax()] + bins[n.argmax() + 1]) / 2, ymax * 0.2, \"Mode\", rotation=90)\n    plt.legend()\n\n\n# Plot\nplt.figure(figsize=(18, 5))\nax1 = plt.subplot(131)\nHistogPlot(data_returns[\"BNP\"], \"Densité des rendements d'actions BNP\")\nax2 = plt.subplot(132)\nHistogPlot(data_returns[\"SG\"], \"Densité des rendements d'action SG\")\nplt.show()\n\n\n\n\n\n\n\n\n\nDans les deux cas on observe des courbes en forme de cloche, et sensiblement symétrique, ce qui laisse présager une loi normale. Mais les kurtosis excessifs sont supérieurs à 2 et donc des kurtosis supérieurs à 5. Nous allons donc modéliser en plus des lois gaussiennes des lois à queues lourdes, notamment une Student et une Skew Student.\n\n\nModélisation des lois marginales associées aux log rendements\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import norm, t, kstest, skew\n\nfrom skew_student import*  # Import de la Skew-Student\n\ndef modeliser_distribution(data):\n    \"\"\"\n    Modélise une distribution normale, Student et Skew-Student, compare les ajustements et sélectionne la meilleure.\n    \n    Paramètre :\n    - data : array NumPy contenant l'échantillon de données.\n    \n    Retour :\n    - Un dictionnaire contenant la distribution sélectionnée et les p-values du KS test.\n    \"\"\"\n    \n    #  1. Estimation des paramètres\n    mu_norm, sigma_norm = norm.fit(data)  # Loi normale\n    df_t, loc_t, scale_t = t.fit(data)  # Loi de Student\n    mu_skew, sigma_skew, gamma_skew, nu_skew = optimize_parameters(data)  # Loi Skew-Student\n    \n    print(\"--\"*10,\"PARAMETRES ESTIMES\", \"--\"*10)\n    print(f\"Loi Normale - Moyenne: {mu_norm:.3f}, Écart-type: {sigma_norm:.3f}\")\n    print(f\"Loi de Student - df: {df_t:.3f}, Moyenne: {loc_t:.3f}, Échelle: {scale_t:.3f}\")\n    print(f\"Loi Skew-Student - μ: {mu_skew:.3f}, σ: {sigma_skew:.3f}, γ: {gamma_skew:.3f}, ν: {nu_skew:.3f}\")\n    print(\"--\"*30)\n    \n    #  2. Affichage des Q-Q Plots juxtaposés\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 3 colonnes pour ajouter la Skew-Student\n\n    # Q-Q Plot pour la loi Normale\n    stats.probplot(data, dist=\"norm\", sparams=(mu_norm, sigma_norm), plot=axes[0])\n    axes[0].set_title(\"Q-Q Plot - Loi Normale\")\n    axes[0].grid()\n\n    # Q-Q Plot pour la loi de Student\n    stats.probplot(data, dist=\"t\", sparams=(df_t, loc_t, scale_t), plot=axes[1])\n    axes[1].set_title(\"Q-Q Plot - Loi de Student\")\n    axes[1].grid()\n\n    # Q-Q Plot pour la loi Skew-Student\n    sorted_data = np.sort(data)\n    n = len(data)\n    quantiles_skew = skew_student_ppf((np.arange(1, n + 1) - 0.5) / n, mu_skew, sigma_skew, gamma_skew, nu_skew)\n\n    axes[2].scatter(quantiles_skew, sorted_data, color=\"blue\", alpha=0.6)\n    axes[2].plot(quantiles_skew, quantiles_skew, 'r--')  # Ligne théorique (y=x)\n    axes[2].set_title(\"Q-Q Plot - Loi Skew-Student\")\n    axes[2].grid()\n\n    plt.tight_layout()\n    plt.show()\n    \n    #  3. Test KS (Kolmogorov-Smirnov)\n    ks_stat_norm, ks_pval_norm = kstest(data, \"norm\", args=(mu_norm, sigma_norm))\n    ks_stat_t, ks_pval_t = kstest(data, \"t\", args=(df_t, loc_t, scale_t))\n    \n    # KS-Test pour la Skew-Student\n    np.random.seed(42)\n    sim_data = skew_student_sim(mu_skew, sigma_skew, gamma_skew, nu_skew, size = 100_000)\n    ks_stat_skew, ks_pval_skew =  kstest(data,sim_data)\n\n    print(f\"KS Test - Loi Normale: Stat={ks_stat_norm:.3f}, p-value={ks_pval_norm:.3f}\")\n    print(f\"KS Test - Loi de Student: Stat={ks_stat_t:.3f}, p-value={ks_pval_t:.3f}\")\n    print(f\"KS Test - Loi Skew-Student: Stat={ks_stat_skew:.3f}, p-value={ks_pval_skew:.3f}\")\n\n    #  4. Sélection de la meilleure distribution\n    p_values = {\"Normale\": ks_pval_norm, \"Student\": ks_pval_t, \"Skew-Student\": ks_pval_skew}\n    best_dist = max(p_values, key=p_values.get)  # Sélectionne la distribution avec la plus grande p-value\n    print(f\" Distribution sélectionnée: {best_dist}\")\n    \n    return {\n        \"Distribution\": best_dist, \n        \"p-value Normale\": ks_pval_norm, \n        \"p-value Student\": ks_pval_t, \n        \"p-value Skew-Student\": ks_pval_skew\n    }\n\n\n\n\nCode\nfrom skew_student import *\nfrom scipy.stats import kstest, norm, t, ks_2samp\n\n# Modélisation des rendement \nprint(\"Modélisation des rendements des actions BNP\\n\")\nresultats_BNP = modeliser_distribution(data_returns[\"BNP\"])\n\nprint(\"--\"*30,\"\\n\")\n\nprint(\"Modélisation des rendements des actions SG\\n\")\nresultats_SG = modeliser_distribution(data_returns[\"SG\"])\n\n\nModélisation des rendements des actions BNP\n\n\n\nC:\\Users\\mokom\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:728: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n  self.H.update(delta_x, delta_g)\nC:\\Users\\mokom\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:376: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n\n\n-------------------- PARAMETRES ESTIMES --------------------\nLoi Normale - Moyenne: -0.001, Écart-type: 0.024\nLoi de Student - df: 4.533, Moyenne: -0.001, Échelle: 0.019\nLoi Skew-Student - μ: 0.002, σ: 0.019, γ: -0.152, ν: 4.587\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nKS Test - Loi Normale: Stat=0.055, p-value=0.004\nKS Test - Loi de Student: Stat=0.026, p-value=0.487\nKS Test - Loi Skew-Student: Stat=0.023, p-value=0.681\n Distribution sélectionnée: Skew-Student\n------------------------------------------------------------ \n\nModélisation des rendements des actions SG\n\n-------------------- PARAMETRES ESTIMES --------------------\nLoi Normale - Moyenne: -0.001, Écart-type: 0.021\nLoi de Student - df: 4.593, Moyenne: -0.001, Échelle: 0.016\nLoi Skew-Student - μ: -0.002, σ: 0.016, γ: 0.073, ν: 4.585\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nKS Test - Loi Normale: Stat=0.049, p-value=0.015\nKS Test - Loi de Student: Stat=0.025, p-value=0.559\nKS Test - Loi Skew-Student: Stat=0.029, p-value=0.373\n Distribution sélectionnée: Student\n\n\n\nCommentaire\nA la lecture des QQ_plot, et à l’issu des tests d’adéquation de Kolmogorov smirnov:\n\nles rendements de BNP s’ajustent mieux à l’aide d’une skew-student (confirmation par le test de Kolmogorov-Smirnov, p-value = 0.68). Toutefois les queues de distribution semblent ne pas être bien ajustées.\nles rendements de SG s’ajustent aussi bien à l’aide d’une skew-student qu’à l’aide d’une student. Nous privilégions donc un modèle simple en retenant une student (p-value = 0.559, ks-test)."
  },
  {
    "objectID": "risques/projets/CreditVaR.html#dépendogramme",
    "href": "risques/projets/CreditVaR.html#dépendogramme",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Dépendogramme",
    "text": "Dépendogramme\n\n\nCode\ndata_uniform = data_returns.rank(method='average', pct=True)\n# Nuage de points\nplt.scatter(data_uniform['BNP'], data_uniform['SG'])\nplt.xlabel(\"BNP\")\nplt.ylabel(\"SG\")\nplt.title(\"Scatter plot BNP vs SG\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nL’analyse du dépendodogramme suggère une dependance de queue dans nos données principalement marquée à droite."
  },
  {
    "objectID": "risques/projets/CreditVaR.html#a.-copules-elliptiques-gaussienne-student.",
    "href": "risques/projets/CreditVaR.html#a.-copules-elliptiques-gaussienne-student.",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "5.a. Copules elliptiques : gaussienne, Student.",
    "text": "5.a. Copules elliptiques : gaussienne, Student.\n\n\nCode\nfrom copulae import GaussianCopula\n\n# 1. Correction des index dupliqués\ndata_returns = data_returns.reset_index(drop=True)\n\n# 2. Transformation des données en rangs (pour obtenir des valeurs uniformes dans [0, 1])\nrank_data = data_returns.rank() / (len(data_returns) + 1)\n\n# 3. Ajustement de la copule Gaussienne sur les rangs\ncopula_gaussian = GaussianCopula()  # 2 variables\ncopula_gaussian.fit(rank_data.values)\n\n# 4. Génération d'échantillons de la copule (valeurs sur [0, 1])\nsamples = copula_gaussian.random(len(data_returns))\n\n# 5. Transformation inverse des rangs vers l'échelle originale\n# Utilisation de np.percentile qui prend en argument les pourcentages (multipliez par 100)\nsimulated_BNP = np.percentile(data_returns['BNP'], samples[:, 0] * 100)\nsimulated_SG  = np.percentile(data_returns['SG'], samples[:, 1] * 100)\nsimulated_returns = pd.DataFrame({\n    'BNP': simulated_BNP,\n    'SG': simulated_SG\n})\n\n# 6. Visualisation\nplt.figure(figsize=(10, 5))\nplt.scatter(rank_data['BNP'], rank_data['SG'], alpha=0.5, label=\"Données réelles\", color=\"blue\")\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, label=\"Simulées (Copule Gaussienne)\", color=\"green\")\nplt.xlabel(\"BNP\")\nplt.ylabel(\"SG\")\nplt.title(\"Copule Gaussienne : Réelles vs simulées\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncopula_gaussian.summary()\n\n\n\n    Gaussian Copula Summary\n    Gaussian Copula with 2 dimensions\n    \n    \n\n    Parameters\n    Correlation Matrix\n\n\n\n1.000000\n0.860948\n\n\n0.860948\n1.000000"
  },
  {
    "objectID": "risques/projets/CreditVaR.html#copules-elliptiques-gaussienne-student.",
    "href": "risques/projets/CreditVaR.html#copules-elliptiques-gaussienne-student.",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Copules elliptiques : gaussienne, Student.",
    "text": "Copules elliptiques : gaussienne, Student.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t as t_dist\nfrom scipy.optimize import minimize\nfrom math import log, gamma, pi, sqrt\n\n# =============================================================================\n# 1) Fonctions de densité t-Student (univariée & bivariée) en log\n# =============================================================================\n\ndef log_t_pdf_univariate(x, df):\n    \"\"\"\n    Retourne ln[f_nu(x)] pour la loi t univariée de df degrés de liberté.\n    Formule :\n      f_nu(x) = Gamma((df+1)/2) / [ sqrt(df*pi)*Gamma(df/2) ] * [1 + x^2/df]^(-(df+1)/2)\n    \"\"\"\n    c1 = np.log(gamma((df+1)/2)) - np.log(gamma(df/2)) - 0.5*np.log(df*pi)\n    c2 = -(df+1)/2 * np.log(1 + x**2/df)\n    return c1 + c2\n\ndef log_t_pdf_bivariate(x, y, df, r):\n    \"\"\"\n    Retourne ln[f_{df,r}(x,y)] pour la t-Student bivariée, corrélation r, df degrés de liberté.\n\n      f_{df,r}(x,y) =\n         Gamma((df+2)/2) / [ Gamma(df/2) * (pi*df) * sqrt(1-r^2) ]\n         * [1 + 1/df * (x^2 - 2rxy + y^2)/(1-r^2)]^(-(df+2)/2)\n\n    On renvoie le log de cette densité.\n    \"\"\"\n    # Partie constante\n    log_num_gamma = np.log(gamma((df+2)/2)) - np.log(gamma(df/2))\n    log_denom = np.log(pi*df) + 0.5*np.log(1 - r**2)\n    cst = log_num_gamma - log_denom\n\n    # Partie quadratique\n    quad = (x**2 - 2*r*x*y + y**2)/(1 - r**2)\n    log_noyau = - (df+2)/2 * np.log(1 + quad/df)\n\n    return cst + log_noyau\n\n# =============================================================================\n# 2) Log-densité de la copule t-Student (bivariée)\n# =============================================================================\n\ndef log_student_copula_2d(u1, u2, df, r):\n    \"\"\"\n    Calcule ln[c_{df,r}(u1,u2)] = ln f_{df,r}(x,y) - [ln f_df(x) + ln f_df(y)],\n    où x = t_{df}^{-1}(u1) et y = t_{df}^{-1}(u2).\n\n    - u1, u2 dans [0,1].\n    - df &gt; 0, r in (-1,1).\n    \"\"\"\n    # Inversion via la quantile function (ppf) univariée\n    x = t_dist.ppf(u1, df)\n    y = t_dist.ppf(u2, df)\n\n    # Log densité jointe\n    log_joint = log_t_pdf_bivariate(x, y, df, r)\n    # Log densités marginales (univariées)\n    log_marg_x = log_t_pdf_univariate(x, df)\n    log_marg_y = log_t_pdf_univariate(y, df)\n\n    return log_joint - (log_marg_x + log_marg_y)\n\n# =============================================================================\n# 3) Log-vraisemblance négative\n# =============================================================================\n\ndef negative_log_likelihood(params, data):\n    \"\"\"\n    Calcule - somme [ ln c_{df,r}(u1,u2) ] sur tout l'échantillon.\n    data : array shape (n,2) contenant les (u1,u2) dans [0,1].\n    params : (alpha, beta) --&gt; r, df via reparamétrage :\n\n      r  = tanh(alpha)   --&gt; r in (-1,1)\n      df = 2 + log(1 + exp(beta))  --&gt; df &gt; 2 (par exemple)\n    \"\"\"\n    alpha, beta = params\n    r = np.tanh(alpha)  # in (-1,1)\n    df = 2.0 + np.log1p(np.exp(beta))  # &gt; 2\n\n    ll = 0.0\n    for (u1, u2) in data:\n        ll += log_student_copula_2d(u1, u2, df, r)\n\n    return -ll  # On renvoie la -log-vraisemblance\n\n# =============================================================================\n# 4) Fonction d'ajustement (fit) par MLE\n# =============================================================================\n\ndef fit_student_copula_2d(data, alpha0=0.0, beta0=1.0):\n    \"\"\"\n    Ajuste la copule t-Student bivariée à un échantillon de pseudo-observations 'data' (n,2).\n    Minimisation de la -log-vraisemblance w.r.t. (alpha,beta).\n      alpha0, beta0 : valeurs initiales pour (alpha, beta).\n\n    Retourne un dict contenant r, df, logLik, etc.\n    \"\"\"\n    res = minimize(\n        fun=negative_log_likelihood,\n        x0=np.array([alpha0, beta0]),\n        args=(data,),\n        method='Nelder-Mead'\n    )\n    alpha_hat, beta_hat = res.x\n    r_hat = np.tanh(alpha_hat)\n    df_hat = 2.0 + np.log1p(np.exp(beta_hat))\n\n    ll_hat = -res.fun  # log-vraisemblance au point optimum\n\n    return {\n        'r': r_hat,\n        'df': df_hat,\n        'logLik': ll_hat,\n        'success': res.success,\n        'message': res.message\n    }\n\n# =============================================================================\n# 5) Utilisation sur données data_returns\n# =============================================================================\n\n# 1) Transformer vos rendements en pseudo-observations dans [0,1]\nrank_data = data_returns.rank(method='average') / (len(data_returns) + 1)\nu_data = rank_data.values  # numpy array de taille (n,2)\n\n# 2) Ajustement de la copule t-Student bivariée par MLE\nresult = fit_student_copula_2d(u_data, alpha0=0.0, beta0=1.0)\n\nprint(\"=== Estimation Copule t-Student Bivariée ===\")\nprint(f\"Paramètre r estimé = {result['r']:.4f}\")\nprint(f\"Paramètre df estimé = {result['df']:.4f}\")\nprint(f\"Log-vraisemblance  = {result['logLik']:.4f}\")\nprint(\"Convergence :\", result['success'], \"|\", result['message'])\n\n\n=== Estimation Copule t-Student Bivariée ===\nParamètre r estimé = 0.8496\nParamètre df estimé = 2.0000\nLog-vraisemblance  = 719.7436\nConvergence : True | Optimization terminated successfully.\n\n\n\n\nCode\nfrom copulae import GaussianCopula, StudentCopula, GumbelCopula, FrankCopula, ClaytonCopula\nfrom scipy.stats import kendalltau, spearmanr\n\n# Chargement des données\ndata_returns = data_returns.reset_index(drop=True)\n\n# Transformation en pseudo-observations (rangs normalisés)\nrank_data = data_returns.rank() / (len(data_returns) + 1)\n\n# Définition des copules\ncopules = {\n    \"Gaussian\": GaussianCopula(dim=2),\n    \"Student-t\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Ajustement des copules aux données empiriques\nfit_results = {}\nfor name, cop in copules.items():\n    if name == \"Student-t\":\n        cop.fit(rank_data.values, fix_df=True)\n    else:\n        cop.fit(rank_data.values)\n    fit_results[name] = cop\n\n# Comparaison des mesures de dépendance\nempirical_tau, _ = kendalltau(data_returns[\"BNP\"], data_returns[\"SG\"])\nempirical_rho, _ = spearmanr(data_returns[\"BNP\"], data_returns[\"SG\"])\n\nresults = []\nfor name, cop in fit_results.items():\n    simulated_data = cop.random(len(data_returns))\n    sim_tau, _ = kendalltau(simulated_data[:, 0], simulated_data[:, 1])\n    sim_rho, _ = spearmanr(simulated_data[:, 0], simulated_data[:, 1])\n    \n    # Nouveau calcul de la log-vraisemblance via la densité jointe\n    log_likelihood = np.sum(np.log(cop.pdf(rank_data.values)))\n\n    results.append([name, log_likelihood, abs(sim_tau - empirical_tau), abs(sim_rho - empirical_rho)])\n\n# Affichage des résultats\nresults_df = pd.DataFrame(results, columns=[\"Copule\", \"Log-Vraisemblance\", \"Erreur Kendall Tau\", \"Erreur Spearman Rho\"])\nprint(results_df.sort_values(by=\"Log-Vraisemblance\", ascending=False))\n\n\n      Copule  Log-Vraisemblance  Erreur Kendall Tau  Erreur Spearman Rho\n1  Student-t         720.838328            0.003462             0.000855\n2     Gumbel         703.146279            0.007835             0.004465\n0   Gaussian         670.141798            0.000534             0.016872\n3      Frank         630.219249            0.001741             0.023079\n4    Clayton         517.723513            0.091739             0.073704\n\n\n\n\nCode\n# Visualisation des distributions simulées\nplt.figure(figsize=(12, 8))\nfor i, (name, cop) in enumerate(fit_results.items(), 1):\n    simulated_data = cop.random(len(data_returns))\n    plt.subplot(3, 2, i)\n    plt.scatter(rank_data[\"BNP\"], rank_data[\"SG\"], alpha=0.5, label=\"Données réelles\", color=\"blue\")\n    plt.scatter(simulated_data[:, 0], simulated_data[:, 1], alpha=0.5, label=f\"Simulées ({name})\", color=\"green\")\n    plt.title(f\"Copule {name}\")\n    plt.xlabel(\"BNP\")\n    plt.ylabel(\"SG\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import gaussian_kde\n\nrank_data = data_returns.rank() / (len(data_returns) + 1)\nu = rank_data.iloc[:, 0].values\nv = rank_data.iloc[:, 1].values\nn = len(u)\n\n# Calcul de la statistique empirique V_i (distribution de Kendall)\nmask = (u[:, np.newaxis] &lt;= u) & (v[:, np.newaxis] &lt;= v)\ncounts = mask.sum(axis=1)\nV = (counts - 1) / (n - 1)  # Ajustement pour l'auto-comparaison\nV_trié = np.sort(V)\n\n# Définition des copules à tester\ncopules = {\n    \"Gaussienne\": GaussianCopula(dim=2),\n    \"Student-t\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Paramètres de simulation\nnb_simulations = 100  # Augmenter pour un résultat plus lisse\n\n# Génération des graphiques de Kendall\nplt.figure(figsize=(10, 10))\n\nfor i, (nom, copule) in enumerate(copules.items(), 1):\n    # Ajustement de la copule aux données\n    if nom == \"Student-t\":\n        copule.fit(rank_data.values, fix_df=True)\n    else:\n        copule.fit(rank_data.values)\n    \n    # Simulation de plusieurs jeux de données et calcul des statistiques V\n    V_simulé = np.zeros((nb_simulations, n))\n    \n    for s in range(nb_simulations):\n        # Génération de données synthétiques à partir de la copule ajustée\n        données_simulées = copule.random(n)\n        \n        # Calcul de V pour les données simulées\n        u_sim = données_simulées[:, 0]\n        v_sim = données_simulées[:, 1]\n        mask_sim = (u_sim[:, np.newaxis] &lt;= u_sim) & (v_sim[:, np.newaxis] &lt;= v_sim)\n        counts_sim = mask_sim.sum(axis=1)\n        V_sim = (counts_sim - 1) / (n - 1)\n        V_simulé[s] = np.sort(V_sim)  # Tri des valeurs simulées\n    \n    # Calcul des valeurs moyennes attendues à partir des simulations\n    V_attendu = V_simulé.mean(axis=0)\n    \n    # Tracé du graphique de Kendall avec une ligne fine pour mieux voir l'ajustement\n    plt.subplot(3, 2, i)\n    plt.plot(V_attendu, V_trié, lw=1, color='blue', label='Données vs Copule')\n    plt.plot([0, 1], [0, 1], '--r', lw=1, label='Ajustement parfait')\n    plt.xlabel(f'Valeurs attendues ({nom})')\n    plt.ylabel('Valeurs empiriques')\n    plt.title(f'Copule {nom}')\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import gaussian_kde\n\n# Configuration\ngrid_size = 100        # Résolution de la grille\ncontour_levels = 15    # Nombre de niveaux de contour\n\n# Préparation des données : transformation en marges uniformes par rang\nrank_data = data_returns.rank() / (len(data_returns) + 1)\nu = rank_data.iloc[:, 0]\nv = rank_data.iloc[:, 1]\n\n# Création de la grille d'évaluation\nx = np.linspace(0, 1, grid_size)\ny = np.linspace(0, 1, grid_size)\nX, Y = np.meshgrid(x, y)\ngrid_points = np.vstack([X.ravel(), Y.ravel()]).T\n\n# Calcul de la KDE empirique sur les données observées\nempirical_kernel = gaussian_kde(rank_data.values.T)\nZ_empirical = empirical_kernel(grid_points.T).reshape(X.shape)\n\n# Définition des copules à comparer\ncopulas = {\n    \"Gaussian\": GaussianCopula(dim=2),\n    \"Student\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Nombre d'observations simulées pour estimer la KDE théorique\nn_sim = 5000\n\n# Création de la figure avec une ligne par copule et 2 colonnes (empirique vs théorique)\nfig, axs = plt.subplots(nrows=len(copulas), ncols=2, figsize=(15, 25))\nplt.subplots_adjust(hspace=0.4)\n\nfor idx, (name, copula) in enumerate(copulas.items()):\n    # Ajustement de la copule aux données\n    if name == \"Student\":\n        copula.fit(rank_data.values, fix_df=True)\n    else:\n        copula.fit(rank_data.values)\n    \n    # Simulation d'un échantillon théorique à partir de la copule ajustée\n    simulated_data = copula.random(n_sim)\n    \n    # Calcul de la KDE théorique à partir de l'échantillon simulé\n    theoretical_kernel = gaussian_kde(simulated_data.T)\n    Z_theoretical = theoretical_kernel(grid_points.T).reshape(X.shape)\n    \n    # Affichage de la KDE empirique\n    ax_emp = axs[idx, 0]\n    cf_emp = ax_emp.contourf(X, Y, Z_empirical, levels=contour_levels, cmap='viridis', alpha=0.8)\n    ax_emp.scatter(u, v, s=10, color='white', edgecolors='black', alpha=0.7)\n    ax_emp.set_title(f'{name} Copule - KDE Empirique')\n    ax_emp.set_xlabel('u')\n    ax_emp.set_ylabel('v')\n    ax_emp.grid(True, linestyle='--', alpha=0.5)\n    \n    # Affichage de la KDE théorique\n    ax_theo = axs[idx, 1]\n    cf_theo = ax_theo.contourf(X, Y, Z_theoretical, levels=contour_levels, cmap='viridis', alpha=0.8)\n    ax_theo.scatter(u, v, s=10, color='white', edgecolors='black', alpha=0.7)\n    ax_theo.set_title(f'Copule {name} - KDE Théorique')\n    ax_theo.set_xlabel('u')\n    ax_theo.set_ylabel('v')\n    ax_theo.grid(True, linestyle='--', alpha=0.5)\n    \n    # Ajout d'une colorbar pour le graphique théorique\n    fig.colorbar(cf_theo, ax=ax_theo, orientation='vertical', shrink=0.9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(20, 15))\n\nfor idx, (name, copula) in enumerate(copulas.items(), 1):\n    # Calcul de la densité théorique\n    # Fit copula to data\n    if name == \"Student\":\n        copula.fit(rank_data.values, fix_df = True)\n    else:\n        copula.fit(rank_data.values)\n        \n    pdf = copula.pdf(grid_points).reshape(X.shape)\n    \n    # Graphique 3D\n    density_values = empirical_kernel(np.vstack([u, v]))\n\n    ax = fig.add_subplot(3, 2, idx, projection='3d')\n    ax.plot_surface(X, Y, pdf, cmap='viridis', alpha=0.7)\n    ax.scatter(u, v, density_values, c='red', s=10, label='Données')\n    ax.set_title(f'Densité {name} Copula\\n(Surface théorique vs Points empiriques)')\n    ax.set_xlabel('u')\n    ax.set_ylabel('v')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "docs/risques/projets/Codes_copules.html",
    "href": "docs/risques/projets/Codes_copules.html",
    "title": "Gestion de risques multiples : implémentations",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "docs/risques/projets/Codes_copules.html#a.-effectuer-une-analyse-exploratoire-univariée-des-données-actions-de-ces-deux-entreprises",
    "href": "docs/risques/projets/Codes_copules.html#a.-effectuer-une-analyse-exploratoire-univariée-des-données-actions-de-ces-deux-entreprises",
    "title": "Gestion de risques multiples : implémentations",
    "section": "3.a. Effectuer une analyse exploratoire univariée des données actions de ces deux entreprises",
    "text": "3.a. Effectuer une analyse exploratoire univariée des données actions de ces deux entreprises\nOn s’intéresse ici à l’historique des prix bruts des actions mais également à celui des log rendements car ils representent une mesure normalisée qui permet de mieux apprecier la volatilité des prix.\nLes log rendements étant définis comme suit :\n$$\n = ()\n$$\nOù :\n\n$ _t $ est le prix de l’action à l’instant t,\n$ _{t-1} $ est le prix de l’action à l’instant t-1.\n\n\n\nCode\n# Chargement des données\ndata = pd.read_csv(\"data.txt\", sep=\"\\s+\", header=0)\ndata.head()\n\n\n&lt;&gt;:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n&lt;&gt;:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_18364\\1367165320.py:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n  data = pd.read_csv(\"data.txt\", sep=\"\\s+\", header=0)\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n0\n42.36\n55.24\n\n\n1\n42.72\n55.59\n\n\n2\n43.20\n56.45\n\n\n3\n42.67\n55.55\n\n\n4\n41.81\n54.50\n\n\n\n\n\n\n\n\nStatistiques descriptives des rendements et des log rendements.\n\n\nCode\n## principales caractéristiques de tendances centrales et de dispersion  des prix des actions\ndata.describe()\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n1000.000000\n1000.000000\n\n\nmean\n31.906254\n43.532641\n\n\nstd\n9.630129\n10.007847\n\n\nmin\n14.056200\n21.667700\n\n\n25%\n23.450325\n36.065350\n\n\n50%\n34.353700\n46.824800\n\n\n75%\n40.229375\n51.291675\n\n\nmax\n48.330000\n60.680000\n\n\n\n\n\n\n\n\n\nCode\n## Calcul des log rendements\n\n\n\n\nCode\ndata_returns = np.log(data/data.shift(1)).dropna()\ndata_returns.head(3)\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n1\n0.008463\n0.006316\n\n\n2\n0.011173\n0.015352\n\n\n3\n-0.012344\n-0.016072\n\n\n\n\n\n\n\n\n\nCode\n## principales caractéristiques de tendances centrales et de dispersion log rendements des actions\ndata_returns.describe()\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n999.000000\n999.000000\n\n\nmean\n-0.000888\n-0.000687\n\n\nstd\n0.024441\n0.020731\n\n\nmin\n-0.123524\n-0.098292\n\n\n25%\n-0.013867\n-0.011830\n\n\n50%\n-0.000358\n-0.000547\n\n\n75%\n0.012526\n0.011223\n\n\nmax\n0.083225\n0.076478\n\n\n\n\n\n\n\nNous procédons ci après à des statistiques descriptives plus exhausives. Incluant notamment des test de stationnarité, l’évaluation de l’asymétrie et des niveaux d’aplatissement de nos distributions\n\n\nCode\nfrom scipy.stats import shapiro\nfrom statsmodels.tsa.stattools import adfuller\n\ndef analyze_data(df, name=\"Dataset\"):\n    \"\"\"\n    Analyse les données du DataFrame fourni et retourne un DataFrame avec :\n    - Statistiques descriptives\n    - Tests de stationnarité (Dickey-Fuller)\n\n    Paramètres :\n    df : DataFrame : Doit contenir les colonnes 'Close' et 'Rendement'\n    name : str : Nom du dataset (optionnel)\n\n    Retourne :\n    results : DataFrame avec toutes les analyses\n    \"\"\"\n    # Vérification de la présence des colonnes nécessaires\n    if not {'Close', 'Rendement'}.issubset(df.columns):\n        raise ValueError(\"Le DataFrame doit contenir les colonnes 'Close' et 'Rendement'.\")\n\n    # Statistiques descriptives\n    results = df[['Close', 'Rendement']].describe()\n    results['Indicateur'] = [\"Nombre\", \"Moyenne\", \"Ecart-Type\", \"Minimum\", \"Percentile 25%\", \"Médiane\", \"Percentile 75%\", \"Maximum\"]\n    results = results[['Indicateur', 'Close', 'Rendement']]\n\n    # Test de stationnarité (Dickey-Fuller)\n    _, p_value_rend = adfuller(df['Rendement'].dropna())[:2]\n    _, p_value_close = adfuller(df['Close'].dropna())[:2]\n\n    # Kurtosis \n    kurtosis_rend = df['Rendement'].dropna().kurt()\n    kurtosis_close = df['Close'].dropna().kurt()\n\n    # skewness\n    skew_rend = df['Rendement'].dropna().skew()\n    skew_close = df['Close'].dropna().skew()\n    results = pd.concat([results, pd.DataFrame([[\"Stationnarité (p)\", p_value_close, p_value_rend ]], columns=results.columns)])\n    results = pd.concat([results, pd.DataFrame([[\"Asymétrie\", skew_close , skew_rend ]], columns=results.columns)])\n    results = pd.concat([results, pd.DataFrame([[\"Excess Kurtosis\", kurtosis_close, kurtosis_rend ]], columns=results.columns)])\n  \n    return results\n\ndata_bnp = pd.concat([data[\"BNP\"], data_returns[\"BNP\"]], axis=1)\ndata_bnp.columns = ['Close', 'Rendement']  # Renommage des colonnes\n\nanalyze_data(data_bnp)\n\n\n\n\n\n\n\n\n\nIndicateur\nClose\nRendement\n\n\n\n\ncount\nNombre\n1000.000000\n999.000000\n\n\nmean\nMoyenne\n31.906254\n-0.000888\n\n\nstd\nEcart-Type\n9.630129\n0.024441\n\n\nmin\nMinimum\n14.056200\n-0.123524\n\n\n25%\nPercentile 25%\n23.450325\n-0.013867\n\n\n50%\nMédiane\n34.353700\n-0.000358\n\n\n75%\nPercentile 75%\n40.229375\n0.012526\n\n\nmax\nMaximum\n48.330000\n0.083225\n\n\n0\nStationnarité (p)\n0.794391\n0.000000\n\n\n0\nAsymétrie\n-0.310893\n-0.331426\n\n\n0\nExcess Kurtosis\n-1.233209\n2.016973\n\n\n\n\n\n\n\n\n\nCode\ndata_sg = pd.concat([data[\"SG\"], data_returns[\"SG\"]], axis=1)\ndata_sg.columns = ['Close', 'Rendement']  # Renommage des colonnes\n\nanalyze_data(data_sg)\n\n\n\n\n\n\n\n\n\nIndicateur\nClose\nRendement\n\n\n\n\ncount\nNombre\n1000.000000\n9.990000e+02\n\n\nmean\nMoyenne\n43.532641\n-6.874575e-04\n\n\nstd\nEcart-Type\n10.007847\n2.073120e-02\n\n\nmin\nMinimum\n21.667700\n-9.829201e-02\n\n\n25%\nPercentile 25%\n36.065350\n-1.182979e-02\n\n\n50%\nMédiane\n46.824800\n-5.468895e-04\n\n\n75%\nPercentile 75%\n51.291675\n1.122288e-02\n\n\nmax\nMaximum\n60.680000\n7.647830e-02\n\n\n0\nStationnarité (p)\n0.736159\n8.678531e-09\n\n\n0\nAsymétrie\n-0.489312\n-1.797332e-01\n\n\n0\nExcess Kurtosis\n-0.933549\n2.097393e+00\n\n\n\n\n\n\n\n\nCommentaire :\nLes cours des actions de BNP et de société générale sont tous deux assez variables sur la période d’étude. Les log rendements associés sont très volatiles, avec une dispersion allant jusqu’à l’ordre de 30 fois supérieur à la valeur absolue de la moyenne\n\n\n\nEvolution du cours des actions de BNP et SG ainsi que celle de leurs log rendements\nCette evolution permet d’illustrer de maniere visuelle la forte volatilité des rendements des actions de BNP et de SG\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nax.plot(data[\"BNP\"], 'r-', lw=1, alpha=0.8, label='BNP')\nax.plot(data[\"SG\"], 'b-', lw=1, alpha=0.8, label='SG')\nax.set_title(\"Évolution du cours de l'action de BNP et SG\", fontsize=16)\nax.set_xlabel('Temps', fontsize=14)\nax.set_ylabel(\"Cours de l'action\", fontsize=14)\nax.tick_params(axis='both', which='major', labelsize=12)\nax.grid(True, linestyle='--', alpha=0.7)\nax.legend(loc='best', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n\n# Graphique pour BNP\nax1.plot(data_returns[\"BNP\"], 'r-', lw=1, alpha=0.8, label='BNP')\nax1.set_title(\"Évolution du cours des rendements de l'action BNP\", fontsize=16)\nax1.set_xlabel('Temps', fontsize=14)\nax1.set_ylabel(\"Rendements des actions\", fontsize=14)\nax1.tick_params(axis='both', which='major', labelsize=12)\nax1.grid(True, linestyle='--', alpha=0.7)\nax1.legend(loc='best', fontsize=12)\n\n# Graphique pour SG\nax2.plot(data_returns[\"SG\"], 'b-', lw=1, alpha=0.8, label='SG')\nax2.set_title(\"Évolution du cours des rendements de l'action SG\", fontsize=16)\nax2.set_xlabel('Temps', fontsize=14)\nax2.set_ylabel(\"Rendements des actions\", fontsize=14)\nax2.tick_params(axis='both', which='major', labelsize=12)\nax2.grid(True, linestyle='--', alpha=0.7)\nax2.legend(loc='best', fontsize=12)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "docs/risques/projets/Codes_copules.html#b-modéliser-judicieusement-les-distributions-univariées-des-facteurs-de-risques",
    "href": "docs/risques/projets/Codes_copules.html#b-modéliser-judicieusement-les-distributions-univariées-des-facteurs-de-risques",
    "title": "Gestion de risques multiples : implémentations",
    "section": "3.b Modéliser judicieusement les distributions univariées des facteurs de risques",
    "text": "3.b Modéliser judicieusement les distributions univariées des facteurs de risques\n\nHistogrammes et courbes de densité des log rendements des actions de BNP et SG\n\n\nCode\ndef HistogPlot(data, label):\n    sns.kdeplot(data, color = \"r\", lw=2, alpha=0.7, label=label)  # Density\n    n, bins, patches = plt.hist(\n        data, bins=20, density=True, histtype=\"stepfilled\", alpha=0.5\n    )  # Histogram\n    \n    \n    ymax = plt.ylim()[1]  # Get the max y value for positioning text\n\n    # Ajout de la moyenne\n    plt.axvline(x=np.mean(data))\n    plt.text(np.mean(data), ymax * 0.5, \"Moyenne\", rotation=90)\n\n    # Ajout de la médiane\n    plt.axvline(x=np.median(data))\n    plt.text(np.median(data), ymax * 0.7, \"Médiane\", rotation=90)\n\n    # Adjout du mode\n    plt.axvline(x=(bins[n.argmax()] + bins[n.argmax() + 1]) / 2)\n    plt.text((bins[n.argmax()] + bins[n.argmax() + 1]) / 2, ymax * 0.2, \"Mode\", rotation=90)\n    plt.legend()\n\n\n# Plot\nplt.figure(figsize=(18, 5))\nax1 = plt.subplot(131)\nHistogPlot(data_returns[\"BNP\"], \"Densité des rendements d'actions BNP\")\nax2 = plt.subplot(132)\nHistogPlot(data_returns[\"SG\"], \"Densité des rendements d'action SG\")\nplt.show()\n\n\n\n\n\n\n\n\n\nDans les deux cas on observe des courbes en forme de cloche, et sensiblement symétrique, ce qui laisse présager une loi normale. Mais les kurtosis excessifs sont supérieurs à 2 et donc des kurtosis supérieurs à 5. Nous allons donc modéliser en plus des lois gaussiennes des lois à queues lourdes, notamment une Student et une Skew Student.\n\n\nModélisation des lois marginales associées aux log rendements\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import norm, t, kstest, skew\n\nfrom skew_student import*  # Import de la Skew-Student\n\ndef modeliser_distribution(data):\n    \"\"\"\n    Modélise une distribution normale, Student et Skew-Student, compare les ajustements et sélectionne la meilleure.\n    \n    Paramètre :\n    - data : array NumPy contenant l'échantillon de données.\n    \n    Retour :\n    - Un dictionnaire contenant la distribution sélectionnée et les p-values du KS test.\n    \"\"\"\n    \n    #  1. Estimation des paramètres\n    mu_norm, sigma_norm = norm.fit(data)  # Loi normale\n    df_t, loc_t, scale_t = t.fit(data)  # Loi de Student\n    mu_skew, sigma_skew, gamma_skew, nu_skew = optimize_parameters(data)  # Loi Skew-Student\n    \n    print(\"--\"*10,\"PARAMETRES ESTIMES\", \"--\"*10)\n    print(f\"Loi Normale - Moyenne: {mu_norm:.3f}, Écart-type: {sigma_norm:.3f}\")\n    print(f\"Loi de Student - df: {df_t:.3f}, Moyenne: {loc_t:.3f}, Échelle: {scale_t:.3f}\")\n    print(f\"Loi Skew-Student - μ: {mu_skew:.3f}, σ: {sigma_skew:.3f}, γ: {gamma_skew:.3f}, ν: {nu_skew:.3f}\")\n    print(\"--\"*30)\n    \n    #  2. Affichage des Q-Q Plots juxtaposés\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 3 colonnes pour ajouter la Skew-Student\n\n    # Q-Q Plot pour la loi Normale\n    stats.probplot(data, dist=\"norm\", sparams=(mu_norm, sigma_norm), plot=axes[0])\n    axes[0].set_title(\"Q-Q Plot - Loi Normale\")\n    axes[0].grid()\n\n    # Q-Q Plot pour la loi de Student\n    stats.probplot(data, dist=\"t\", sparams=(df_t, loc_t, scale_t), plot=axes[1])\n    axes[1].set_title(\"Q-Q Plot - Loi de Student\")\n    axes[1].grid()\n\n    # Q-Q Plot pour la loi Skew-Student\n    sorted_data = np.sort(data)\n    n = len(data)\n    quantiles_skew = skew_student_ppf((np.arange(1, n + 1) - 0.5) / n, mu_skew, sigma_skew, gamma_skew, nu_skew)\n\n    axes[2].scatter(quantiles_skew, sorted_data, color=\"blue\", alpha=0.6)\n    axes[2].plot(quantiles_skew, quantiles_skew, 'r--')  # Ligne théorique (y=x)\n    axes[2].set_title(\"Q-Q Plot - Loi Skew-Student\")\n    axes[2].grid()\n\n    plt.tight_layout()\n    plt.show()\n    \n    #  3. Test KS (Kolmogorov-Smirnov)\n    ks_stat_norm, ks_pval_norm = kstest(data, \"norm\", args=(mu_norm, sigma_norm))\n    ks_stat_t, ks_pval_t = kstest(data, \"t\", args=(df_t, loc_t, scale_t))\n    \n    # KS-Test pour la Skew-Student\n    np.random.seed(42)\n    sim_data = skew_student_sim(mu_skew, sigma_skew, gamma_skew, nu_skew, size = 100_000)\n    ks_stat_skew, ks_pval_skew =  kstest(data,sim_data)\n\n    print(f\"KS Test - Loi Normale: Stat={ks_stat_norm:.3f}, p-value={ks_pval_norm:.3f}\")\n    print(f\"KS Test - Loi de Student: Stat={ks_stat_t:.3f}, p-value={ks_pval_t:.3f}\")\n    print(f\"KS Test - Loi Skew-Student: Stat={ks_stat_skew:.3f}, p-value={ks_pval_skew:.3f}\")\n\n    #  4. Sélection de la meilleure distribution\n    p_values = {\"Normale\": ks_pval_norm, \"Student\": ks_pval_t, \"Skew-Student\": ks_pval_skew}\n    best_dist = max(p_values, key=p_values.get)  # Sélectionne la distribution avec la plus grande p-value\n    print(f\" Distribution sélectionnée: {best_dist}\")\n    \n    return {\n        \"Distribution\": best_dist, \n        \"p-value Normale\": ks_pval_norm, \n        \"p-value Student\": ks_pval_t, \n        \"p-value Skew-Student\": ks_pval_skew\n    }\n\n\n\n\nCode\nfrom skew_student import *\nfrom scipy.stats import kstest, norm, t, ks_2samp\n\n# Modélisation des rendement \nprint(\"Modélisation des rendements des actions BNP\\n\")\nresultats_BNP = modeliser_distribution(data_returns[\"BNP\"])\n\nprint(\"--\"*30,\"\\n\")\n\nprint(\"Modélisation des rendements des actions SG\\n\")\nresultats_SG = modeliser_distribution(data_returns[\"SG\"])\n\n\nModélisation des rendements des actions BNP\n\n\n\nC:\\Users\\mokom\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:728: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n  self.H.update(delta_x, delta_g)\nC:\\Users\\mokom\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:376: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n\n\n-------------------- PARAMETRES ESTIMES --------------------\nLoi Normale - Moyenne: -0.001, Écart-type: 0.024\nLoi de Student - df: 4.533, Moyenne: -0.001, Échelle: 0.019\nLoi Skew-Student - μ: 0.002, σ: 0.019, γ: -0.152, ν: 4.587\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nKS Test - Loi Normale: Stat=0.055, p-value=0.004\nKS Test - Loi de Student: Stat=0.026, p-value=0.487\nKS Test - Loi Skew-Student: Stat=0.023, p-value=0.681\n Distribution sélectionnée: Skew-Student\n------------------------------------------------------------ \n\nModélisation des rendements des actions SG\n\n-------------------- PARAMETRES ESTIMES --------------------\nLoi Normale - Moyenne: -0.001, Écart-type: 0.021\nLoi de Student - df: 4.593, Moyenne: -0.001, Échelle: 0.016\nLoi Skew-Student - μ: -0.002, σ: 0.016, γ: 0.073, ν: 4.585\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nKS Test - Loi Normale: Stat=0.049, p-value=0.015\nKS Test - Loi de Student: Stat=0.025, p-value=0.559\nKS Test - Loi Skew-Student: Stat=0.029, p-value=0.373\n Distribution sélectionnée: Student\n\n\n\nCommentaire\nA la lecture des QQ_plot, et à l’issu des tests d’adéquation de Kolmogorov smirnov:\n\nles rendements de BNP s’ajustent mieux à l’aide d’une skew-student (confirmation par le test de Kolmogorov-Smirnov, p-value = 0.68). Toutefois les queues de distribution semblent ne pas être bien ajustées.\nles rendements de SG s’ajustent aussi bien à l’aide d’une skew-student qu’à l’aide d’une student. Nous privilégions donc un modèle simple en retenant une student (p-value = 0.559, ks-test)."
  },
  {
    "objectID": "docs/risques/projets/Codes_copules.html#dépendogramme",
    "href": "docs/risques/projets/Codes_copules.html#dépendogramme",
    "title": "Gestion de risques multiples : implémentations",
    "section": "Dépendogramme",
    "text": "Dépendogramme\n\n\nCode\ndata_uniform = data_returns.rank(method='average', pct=True)\n# Nuage de points\nplt.scatter(data_uniform['BNP'], data_uniform['SG'])\nplt.xlabel(\"BNP\")\nplt.ylabel(\"SG\")\nplt.title(\"Scatter plot BNP vs SG\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nL’analyse du dépendodogramme suggère une dependance de queue dans nos données principalement marquée à droite."
  },
  {
    "objectID": "docs/risques/projets/Codes_copules.html#a.-copules-elliptiques-gaussienne-student.",
    "href": "docs/risques/projets/Codes_copules.html#a.-copules-elliptiques-gaussienne-student.",
    "title": "Gestion de risques multiples : implémentations",
    "section": "5.a. Copules elliptiques : gaussienne, Student.",
    "text": "5.a. Copules elliptiques : gaussienne, Student.\n\n\nCode\nfrom copulae import GaussianCopula\n\n# 1. Correction des index dupliqués\ndata_returns = data_returns.reset_index(drop=True)\n\n# 2. Transformation des données en rangs (pour obtenir des valeurs uniformes dans [0, 1])\nrank_data = data_returns.rank() / (len(data_returns) + 1)\n\n# 3. Ajustement de la copule Gaussienne sur les rangs\ncopula_gaussian = GaussianCopula()  # 2 variables\ncopula_gaussian.fit(rank_data.values)\n\n# 4. Génération d'échantillons de la copule (valeurs sur [0, 1])\nsamples = copula_gaussian.random(len(data_returns))\n\n# 5. Transformation inverse des rangs vers l'échelle originale\n# Utilisation de np.percentile qui prend en argument les pourcentages (multipliez par 100)\nsimulated_BNP = np.percentile(data_returns['BNP'], samples[:, 0] * 100)\nsimulated_SG  = np.percentile(data_returns['SG'], samples[:, 1] * 100)\nsimulated_returns = pd.DataFrame({\n    'BNP': simulated_BNP,\n    'SG': simulated_SG\n})\n\n# 6. Visualisation\nplt.figure(figsize=(10, 5))\nplt.scatter(rank_data['BNP'], rank_data['SG'], alpha=0.5, label=\"Données réelles\", color=\"blue\")\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, label=\"Simulées (Copule Gaussienne)\", color=\"green\")\nplt.xlabel(\"BNP\")\nplt.ylabel(\"SG\")\nplt.title(\"Copule Gaussienne : Réelles vs simulées\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncopula_gaussian.summary()\n\n\n\n    Gaussian Copula Summary\n    Gaussian Copula with 2 dimensions\n    \n    \n\n    Parameters\n    Correlation Matrix\n\n\n\n1.000000\n0.860948\n\n\n0.860948\n1.000000"
  },
  {
    "objectID": "docs/risques/projets/Codes_copules.html#copules-elliptiques-gaussienne-student.",
    "href": "docs/risques/projets/Codes_copules.html#copules-elliptiques-gaussienne-student.",
    "title": "Gestion de risques multiples : implémentations",
    "section": "Copules elliptiques : gaussienne, Student.",
    "text": "Copules elliptiques : gaussienne, Student.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t as t_dist\nfrom scipy.optimize import minimize\nfrom math import log, gamma, pi, sqrt\n\n# =============================================================================\n# 1) Fonctions de densité t-Student (univariée & bivariée) en log\n# =============================================================================\n\ndef log_t_pdf_univariate(x, df):\n    \"\"\"\n    Retourne ln[f_nu(x)] pour la loi t univariée de df degrés de liberté.\n    Formule :\n      f_nu(x) = Gamma((df+1)/2) / [ sqrt(df*pi)*Gamma(df/2) ] * [1 + x^2/df]^(-(df+1)/2)\n    \"\"\"\n    c1 = np.log(gamma((df+1)/2)) - np.log(gamma(df/2)) - 0.5*np.log(df*pi)\n    c2 = -(df+1)/2 * np.log(1 + x**2/df)\n    return c1 + c2\n\ndef log_t_pdf_bivariate(x, y, df, r):\n    \"\"\"\n    Retourne ln[f_{df,r}(x,y)] pour la t-Student bivariée, corrélation r, df degrés de liberté.\n\n      f_{df,r}(x,y) =\n         Gamma((df+2)/2) / [ Gamma(df/2) * (pi*df) * sqrt(1-r^2) ]\n         * [1 + 1/df * (x^2 - 2rxy + y^2)/(1-r^2)]^(-(df+2)/2)\n\n    On renvoie le log de cette densité.\n    \"\"\"\n    # Partie constante\n    log_num_gamma = np.log(gamma((df+2)/2)) - np.log(gamma(df/2))\n    log_denom = np.log(pi*df) + 0.5*np.log(1 - r**2)\n    cst = log_num_gamma - log_denom\n\n    # Partie quadratique\n    quad = (x**2 - 2*r*x*y + y**2)/(1 - r**2)\n    log_noyau = - (df+2)/2 * np.log(1 + quad/df)\n\n    return cst + log_noyau\n\n# =============================================================================\n# 2) Log-densité de la copule t-Student (bivariée)\n# =============================================================================\n\ndef log_student_copula_2d(u1, u2, df, r):\n    \"\"\"\n    Calcule ln[c_{df,r}(u1,u2)] = ln f_{df,r}(x,y) - [ln f_df(x) + ln f_df(y)],\n    où x = t_{df}^{-1}(u1) et y = t_{df}^{-1}(u2).\n\n    - u1, u2 dans [0,1].\n    - df &gt; 0, r in (-1,1).\n    \"\"\"\n    # Inversion via la quantile function (ppf) univariée\n    x = t_dist.ppf(u1, df)\n    y = t_dist.ppf(u2, df)\n\n    # Log densité jointe\n    log_joint = log_t_pdf_bivariate(x, y, df, r)\n    # Log densités marginales (univariées)\n    log_marg_x = log_t_pdf_univariate(x, df)\n    log_marg_y = log_t_pdf_univariate(y, df)\n\n    return log_joint - (log_marg_x + log_marg_y)\n\n# =============================================================================\n# 3) Log-vraisemblance négative\n# =============================================================================\n\ndef negative_log_likelihood(params, data):\n    \"\"\"\n    Calcule - somme [ ln c_{df,r}(u1,u2) ] sur tout l'échantillon.\n    data : array shape (n,2) contenant les (u1,u2) dans [0,1].\n    params : (alpha, beta) --&gt; r, df via reparamétrage :\n\n      r  = tanh(alpha)   --&gt; r in (-1,1)\n      df = 2 + log(1 + exp(beta))  --&gt; df &gt; 2 (par exemple)\n    \"\"\"\n    alpha, beta = params\n    r = np.tanh(alpha)  # in (-1,1)\n    df = 2.0 + np.log1p(np.exp(beta))  # &gt; 2\n\n    ll = 0.0\n    for (u1, u2) in data:\n        ll += log_student_copula_2d(u1, u2, df, r)\n\n    return -ll  # On renvoie la -log-vraisemblance\n\n# =============================================================================\n# 4) Fonction d'ajustement (fit) par MLE\n# =============================================================================\n\ndef fit_student_copula_2d(data, alpha0=0.0, beta0=1.0):\n    \"\"\"\n    Ajuste la copule t-Student bivariée à un échantillon de pseudo-observations 'data' (n,2).\n    Minimisation de la -log-vraisemblance w.r.t. (alpha,beta).\n      alpha0, beta0 : valeurs initiales pour (alpha, beta).\n\n    Retourne un dict contenant r, df, logLik, etc.\n    \"\"\"\n    res = minimize(\n        fun=negative_log_likelihood,\n        x0=np.array([alpha0, beta0]),\n        args=(data,),\n        method='Nelder-Mead'\n    )\n    alpha_hat, beta_hat = res.x\n    r_hat = np.tanh(alpha_hat)\n    df_hat = 2.0 + np.log1p(np.exp(beta_hat))\n\n    ll_hat = -res.fun  # log-vraisemblance au point optimum\n\n    return {\n        'r': r_hat,\n        'df': df_hat,\n        'logLik': ll_hat,\n        'success': res.success,\n        'message': res.message\n    }\n\n# =============================================================================\n# 5) Utilisation sur données data_returns\n# =============================================================================\n\n# 1) Transformer vos rendements en pseudo-observations dans [0,1]\nrank_data = data_returns.rank(method='average') / (len(data_returns) + 1)\nu_data = rank_data.values  # numpy array de taille (n,2)\n\n# 2) Ajustement de la copule t-Student bivariée par MLE\nresult = fit_student_copula_2d(u_data, alpha0=0.0, beta0=1.0)\n\nprint(\"=== Estimation Copule t-Student Bivariée ===\")\nprint(f\"Paramètre r estimé = {result['r']:.4f}\")\nprint(f\"Paramètre df estimé = {result['df']:.4f}\")\nprint(f\"Log-vraisemblance  = {result['logLik']:.4f}\")\nprint(\"Convergence :\", result['success'], \"|\", result['message'])\n\n\n=== Estimation Copule t-Student Bivariée ===\nParamètre r estimé = 0.8496\nParamètre df estimé = 2.0000\nLog-vraisemblance  = 719.7436\nConvergence : True | Optimization terminated successfully.\n\n\n\n\nCode\nfrom copulae import GaussianCopula, StudentCopula, GumbelCopula, FrankCopula, ClaytonCopula\nfrom scipy.stats import kendalltau, spearmanr\n\n# Chargement des données\ndata_returns = data_returns.reset_index(drop=True)\n\n# Transformation en pseudo-observations (rangs normalisés)\nrank_data = data_returns.rank() / (len(data_returns) + 1)\n\n# Définition des copules\ncopules = {\n    \"Gaussian\": GaussianCopula(dim=2),\n    \"Student-t\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Ajustement des copules aux données empiriques\nfit_results = {}\nfor name, cop in copules.items():\n    if name == \"Student-t\":\n        cop.fit(rank_data.values, fix_df=True)\n    else:\n        cop.fit(rank_data.values)\n    fit_results[name] = cop\n\n# Comparaison des mesures de dépendance\nempirical_tau, _ = kendalltau(data_returns[\"BNP\"], data_returns[\"SG\"])\nempirical_rho, _ = spearmanr(data_returns[\"BNP\"], data_returns[\"SG\"])\n\nresults = []\nfor name, cop in fit_results.items():\n    simulated_data = cop.random(len(data_returns))\n    sim_tau, _ = kendalltau(simulated_data[:, 0], simulated_data[:, 1])\n    sim_rho, _ = spearmanr(simulated_data[:, 0], simulated_data[:, 1])\n    \n    # Nouveau calcul de la log-vraisemblance via la densité jointe\n    log_likelihood = np.sum(np.log(cop.pdf(rank_data.values)))\n\n    results.append([name, log_likelihood, abs(sim_tau - empirical_tau), abs(sim_rho - empirical_rho)])\n\n# Affichage des résultats\nresults_df = pd.DataFrame(results, columns=[\"Copule\", \"Log-Vraisemblance\", \"Erreur Kendall Tau\", \"Erreur Spearman Rho\"])\nprint(results_df.sort_values(by=\"Log-Vraisemblance\", ascending=False))\n\n\n      Copule  Log-Vraisemblance  Erreur Kendall Tau  Erreur Spearman Rho\n1  Student-t         720.838328            0.003462             0.000855\n2     Gumbel         703.146279            0.007835             0.004465\n0   Gaussian         670.141798            0.000534             0.016872\n3      Frank         630.219249            0.001741             0.023079\n4    Clayton         517.723513            0.091739             0.073704\n\n\n\n\nCode\n# Visualisation des distributions simulées\nplt.figure(figsize=(12, 8))\nfor i, (name, cop) in enumerate(fit_results.items(), 1):\n    simulated_data = cop.random(len(data_returns))\n    plt.subplot(3, 2, i)\n    plt.scatter(rank_data[\"BNP\"], rank_data[\"SG\"], alpha=0.5, label=\"Données réelles\", color=\"blue\")\n    plt.scatter(simulated_data[:, 0], simulated_data[:, 1], alpha=0.5, label=f\"Simulées ({name})\", color=\"green\")\n    plt.title(f\"Copule {name}\")\n    plt.xlabel(\"BNP\")\n    plt.ylabel(\"SG\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import gaussian_kde\n\nrank_data = data_returns.rank() / (len(data_returns) + 1)\nu = rank_data.iloc[:, 0].values\nv = rank_data.iloc[:, 1].values\nn = len(u)\n\n# Calcul de la statistique empirique V_i (distribution de Kendall)\nmask = (u[:, np.newaxis] &lt;= u) & (v[:, np.newaxis] &lt;= v)\ncounts = mask.sum(axis=1)\nV = (counts - 1) / (n - 1)  # Ajustement pour l'auto-comparaison\nV_trié = np.sort(V)\n\n# Définition des copules à tester\ncopules = {\n    \"Gaussienne\": GaussianCopula(dim=2),\n    \"Student-t\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Paramètres de simulation\nnb_simulations = 100  # Augmenter pour un résultat plus lisse\n\n# Génération des graphiques de Kendall\nplt.figure(figsize=(10, 10))\n\nfor i, (nom, copule) in enumerate(copules.items(), 1):\n    # Ajustement de la copule aux données\n    if nom == \"Student-t\":\n        copule.fit(rank_data.values, fix_df=True)\n    else:\n        copule.fit(rank_data.values)\n    \n    # Simulation de plusieurs jeux de données et calcul des statistiques V\n    V_simulé = np.zeros((nb_simulations, n))\n    \n    for s in range(nb_simulations):\n        # Génération de données synthétiques à partir de la copule ajustée\n        données_simulées = copule.random(n)\n        \n        # Calcul de V pour les données simulées\n        u_sim = données_simulées[:, 0]\n        v_sim = données_simulées[:, 1]\n        mask_sim = (u_sim[:, np.newaxis] &lt;= u_sim) & (v_sim[:, np.newaxis] &lt;= v_sim)\n        counts_sim = mask_sim.sum(axis=1)\n        V_sim = (counts_sim - 1) / (n - 1)\n        V_simulé[s] = np.sort(V_sim)  # Tri des valeurs simulées\n    \n    # Calcul des valeurs moyennes attendues à partir des simulations\n    V_attendu = V_simulé.mean(axis=0)\n    \n    # Tracé du graphique de Kendall avec une ligne fine pour mieux voir l'ajustement\n    plt.subplot(3, 2, i)\n    plt.plot(V_attendu, V_trié, lw=1, color='blue', label='Données vs Copule')\n    plt.plot([0, 1], [0, 1], '--r', lw=1, label='Ajustement parfait')\n    plt.xlabel(f'Valeurs attendues ({nom})')\n    plt.ylabel('Valeurs empiriques')\n    plt.title(f'Copule {nom}')\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import gaussian_kde\n\n# Configuration\ngrid_size = 100        # Résolution de la grille\ncontour_levels = 15    # Nombre de niveaux de contour\n\n# Préparation des données : transformation en marges uniformes par rang\nrank_data = data_returns.rank() / (len(data_returns) + 1)\nu = rank_data.iloc[:, 0]\nv = rank_data.iloc[:, 1]\n\n# Création de la grille d'évaluation\nx = np.linspace(0, 1, grid_size)\ny = np.linspace(0, 1, grid_size)\nX, Y = np.meshgrid(x, y)\ngrid_points = np.vstack([X.ravel(), Y.ravel()]).T\n\n# Calcul de la KDE empirique sur les données observées\nempirical_kernel = gaussian_kde(rank_data.values.T)\nZ_empirical = empirical_kernel(grid_points.T).reshape(X.shape)\n\n# Définition des copules à comparer\ncopulas = {\n    \"Gaussian\": GaussianCopula(dim=2),\n    \"Student\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Nombre d'observations simulées pour estimer la KDE théorique\nn_sim = 5000\n\n# Création de la figure avec une ligne par copule et 2 colonnes (empirique vs théorique)\nfig, axs = plt.subplots(nrows=len(copulas), ncols=2, figsize=(15, 25))\nplt.subplots_adjust(hspace=0.4)\n\nfor idx, (name, copula) in enumerate(copulas.items()):\n    # Ajustement de la copule aux données\n    if name == \"Student\":\n        copula.fit(rank_data.values, fix_df=True)\n    else:\n        copula.fit(rank_data.values)\n    \n    # Simulation d'un échantillon théorique à partir de la copule ajustée\n    simulated_data = copula.random(n_sim)\n    \n    # Calcul de la KDE théorique à partir de l'échantillon simulé\n    theoretical_kernel = gaussian_kde(simulated_data.T)\n    Z_theoretical = theoretical_kernel(grid_points.T).reshape(X.shape)\n    \n    # Affichage de la KDE empirique\n    ax_emp = axs[idx, 0]\n    cf_emp = ax_emp.contourf(X, Y, Z_empirical, levels=contour_levels, cmap='viridis', alpha=0.8)\n    ax_emp.scatter(u, v, s=10, color='white', edgecolors='black', alpha=0.7)\n    ax_emp.set_title(f'{name} Copule - KDE Empirique')\n    ax_emp.set_xlabel('u')\n    ax_emp.set_ylabel('v')\n    ax_emp.grid(True, linestyle='--', alpha=0.5)\n    \n    # Affichage de la KDE théorique\n    ax_theo = axs[idx, 1]\n    cf_theo = ax_theo.contourf(X, Y, Z_theoretical, levels=contour_levels, cmap='viridis', alpha=0.8)\n    ax_theo.scatter(u, v, s=10, color='white', edgecolors='black', alpha=0.7)\n    ax_theo.set_title(f'Copule {name} - KDE Théorique')\n    ax_theo.set_xlabel('u')\n    ax_theo.set_ylabel('v')\n    ax_theo.grid(True, linestyle='--', alpha=0.5)\n    \n    # Ajout d'une colorbar pour le graphique théorique\n    fig.colorbar(cf_theo, ax=ax_theo, orientation='vertical', shrink=0.9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(20, 15))\n\nfor idx, (name, copula) in enumerate(copulas.items(), 1):\n    # Calcul de la densité théorique\n    # Fit copula to data\n    if name == \"Student\":\n        copula.fit(rank_data.values, fix_df = True)\n    else:\n        copula.fit(rank_data.values)\n        \n    pdf = copula.pdf(grid_points).reshape(X.shape)\n    \n    # Graphique 3D\n    density_values = empirical_kernel(np.vstack([u, v]))\n\n    ax = fig.add_subplot(3, 2, idx, projection='3d')\n    ax.plot_surface(X, Y, pdf, cmap='viridis', alpha=0.7)\n    ax.scatter(u, v, density_values, c='red', s=10, label='Données')\n    ax.set_title(f'Densité {name} Copula\\n(Surface théorique vs Points empiriques)')\n    ax.set_xlabel('u')\n    ax.set_ylabel('v')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n⬅ Retour"
  },
  {
    "objectID": "risques/projets/Codes_copules.html",
    "href": "risques/projets/Codes_copules.html",
    "title": "Gestion de risques multiples : implémentations",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "risques/projets/Codes_copules.html#a.-effectuer-une-analyse-exploratoire-univariée-des-données-actions-de-ces-deux-entreprises",
    "href": "risques/projets/Codes_copules.html#a.-effectuer-une-analyse-exploratoire-univariée-des-données-actions-de-ces-deux-entreprises",
    "title": "Gestion de risques multiples : implémentations",
    "section": "3.a. Effectuer une analyse exploratoire univariée des données actions de ces deux entreprises",
    "text": "3.a. Effectuer une analyse exploratoire univariée des données actions de ces deux entreprises\nOn s’intéresse ici à l’historique des prix bruts des actions mais également à celui des log rendements car ils representent une mesure normalisée qui permet de mieux apprecier la volatilité des prix.\nLes log rendements étant définis comme suit :\n$$\n = ()\n$$\nOù :\n\n$ _t $ est le prix de l’action à l’instant t,\n$ _{t-1} $ est le prix de l’action à l’instant t-1.\n\n\n\nCode\n# Chargement des données\ndata = pd.read_csv(\"data.txt\", sep=\"\\s+\", header=0)\ndata.head()\n\n\n&lt;&gt;:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n&lt;&gt;:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_18236\\1367165320.py:2: SyntaxWarning: \"\\s\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\s\"? A raw string is also an option.\n  data = pd.read_csv(\"data.txt\", sep=\"\\s+\", header=0)\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n0\n42.36\n55.24\n\n\n1\n42.72\n55.59\n\n\n2\n43.20\n56.45\n\n\n3\n42.67\n55.55\n\n\n4\n41.81\n54.50\n\n\n\n\n\n\n\n\nStatistiques descriptives des rendements et des log rendements.\n\n\nCode\n## principales caractéristiques de tendances centrales et de dispersion  des prix des actions\ndata.describe()\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n1000.000000\n1000.000000\n\n\nmean\n31.906254\n43.532641\n\n\nstd\n9.630129\n10.007847\n\n\nmin\n14.056200\n21.667700\n\n\n25%\n23.450325\n36.065350\n\n\n50%\n34.353700\n46.824800\n\n\n75%\n40.229375\n51.291675\n\n\nmax\n48.330000\n60.680000\n\n\n\n\n\n\n\n\n\nCode\n## Calcul des log rendements\n\n\n\n\nCode\ndata_returns = np.log(data/data.shift(1)).dropna()\ndata_returns.head(3)\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n1\n0.008463\n0.006316\n\n\n2\n0.011173\n0.015352\n\n\n3\n-0.012344\n-0.016072\n\n\n\n\n\n\n\n\n\nCode\n## principales caractéristiques de tendances centrales et de dispersion log rendements des actions\ndata_returns.describe()\n\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n999.000000\n999.000000\n\n\nmean\n-0.000888\n-0.000687\n\n\nstd\n0.024441\n0.020731\n\n\nmin\n-0.123524\n-0.098292\n\n\n25%\n-0.013867\n-0.011830\n\n\n50%\n-0.000358\n-0.000547\n\n\n75%\n0.012526\n0.011223\n\n\nmax\n0.083225\n0.076478\n\n\n\n\n\n\n\nNous procédons ci après à des statistiques descriptives plus exhausives. Incluant notamment des test de stationnarité, l’évaluation de l’asymétrie et des niveaux d’aplatissement de nos distributions\n\n\nCode\nfrom scipy.stats import shapiro\nfrom statsmodels.tsa.stattools import adfuller\n\ndef analyze_data(df, name=\"Dataset\"):\n    \"\"\"\n    Analyse les données du DataFrame fourni et retourne un DataFrame avec :\n    - Statistiques descriptives\n    - Tests de stationnarité (Dickey-Fuller)\n\n    Paramètres :\n    df : DataFrame : Doit contenir les colonnes 'Close' et 'Rendement'\n    name : str : Nom du dataset (optionnel)\n\n    Retourne :\n    results : DataFrame avec toutes les analyses\n    \"\"\"\n    # Vérification de la présence des colonnes nécessaires\n    if not {'Close', 'Rendement'}.issubset(df.columns):\n        raise ValueError(\"Le DataFrame doit contenir les colonnes 'Close' et 'Rendement'.\")\n\n    # Statistiques descriptives\n    results = df[['Close', 'Rendement']].describe()\n    results['Indicateur'] = [\"Nombre\", \"Moyenne\", \"Ecart-Type\", \"Minimum\", \"Percentile 25%\", \"Médiane\", \"Percentile 75%\", \"Maximum\"]\n    results = results[['Indicateur', 'Close', 'Rendement']]\n\n    # Test de stationnarité (Dickey-Fuller)\n    _, p_value_rend = adfuller(df['Rendement'].dropna())[:2]\n    _, p_value_close = adfuller(df['Close'].dropna())[:2]\n\n    # Kurtosis \n    kurtosis_rend = df['Rendement'].dropna().kurt()\n    kurtosis_close = df['Close'].dropna().kurt()\n\n    # skewness\n    skew_rend = df['Rendement'].dropna().skew()\n    skew_close = df['Close'].dropna().skew()\n    results = pd.concat([results, pd.DataFrame([[\"Stationnarité (p)\", p_value_close, p_value_rend ]], columns=results.columns)])\n    results = pd.concat([results, pd.DataFrame([[\"Asymétrie\", skew_close , skew_rend ]], columns=results.columns)])\n    results = pd.concat([results, pd.DataFrame([[\"Excess Kurtosis\", kurtosis_close, kurtosis_rend ]], columns=results.columns)])\n  \n    return results\n\ndata_bnp = pd.concat([data[\"BNP\"], data_returns[\"BNP\"]], axis=1)\ndata_bnp.columns = ['Close', 'Rendement']  # Renommage des colonnes\n\nanalyze_data(data_bnp)\n\n\n\n\n\n\n\n\n\nIndicateur\nClose\nRendement\n\n\n\n\ncount\nNombre\n1000.000000\n999.000000\n\n\nmean\nMoyenne\n31.906254\n-0.000888\n\n\nstd\nEcart-Type\n9.630129\n0.024441\n\n\nmin\nMinimum\n14.056200\n-0.123524\n\n\n25%\nPercentile 25%\n23.450325\n-0.013867\n\n\n50%\nMédiane\n34.353700\n-0.000358\n\n\n75%\nPercentile 75%\n40.229375\n0.012526\n\n\nmax\nMaximum\n48.330000\n0.083225\n\n\n0\nStationnarité (p)\n0.794391\n0.000000\n\n\n0\nAsymétrie\n-0.310893\n-0.331426\n\n\n0\nExcess Kurtosis\n-1.233209\n2.016973\n\n\n\n\n\n\n\n\n\nCode\ndata_sg = pd.concat([data[\"SG\"], data_returns[\"SG\"]], axis=1)\ndata_sg.columns = ['Close', 'Rendement']  # Renommage des colonnes\n\nanalyze_data(data_sg)\n\n\n\n\n\n\n\n\n\nIndicateur\nClose\nRendement\n\n\n\n\ncount\nNombre\n1000.000000\n9.990000e+02\n\n\nmean\nMoyenne\n43.532641\n-6.874575e-04\n\n\nstd\nEcart-Type\n10.007847\n2.073120e-02\n\n\nmin\nMinimum\n21.667700\n-9.829201e-02\n\n\n25%\nPercentile 25%\n36.065350\n-1.182979e-02\n\n\n50%\nMédiane\n46.824800\n-5.468895e-04\n\n\n75%\nPercentile 75%\n51.291675\n1.122288e-02\n\n\nmax\nMaximum\n60.680000\n7.647830e-02\n\n\n0\nStationnarité (p)\n0.736159\n8.678531e-09\n\n\n0\nAsymétrie\n-0.489312\n-1.797332e-01\n\n\n0\nExcess Kurtosis\n-0.933549\n2.097393e+00\n\n\n\n\n\n\n\n\nCommentaire :\nLes cours des actions de BNP et de société générale sont tous deux assez variables sur la période d’étude. Les log rendements associés sont très volatiles, avec une dispersion allant jusqu’à l’ordre de 30 fois supérieur à la valeur absolue de la moyenne\n\n\n\nEvolution du cours des actions de BNP et SG ainsi que celle de leurs log rendements\nCette evolution permet d’illustrer de maniere visuelle la forte volatilité des rendements des actions de BNP et de SG\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\nax.plot(data[\"BNP\"], 'r-', lw=1, alpha=0.8, label='BNP')\nax.plot(data[\"SG\"], 'b-', lw=1, alpha=0.8, label='SG')\nax.set_title(\"Évolution du cours de l'action de BNP et SG\", fontsize=16)\nax.set_xlabel('Temps', fontsize=14)\nax.set_ylabel(\"Cours de l'action\", fontsize=14)\nax.tick_params(axis='both', which='major', labelsize=12)\nax.grid(True, linestyle='--', alpha=0.7)\nax.legend(loc='best', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n\n# Graphique pour BNP\nax1.plot(data_returns[\"BNP\"], 'r-', lw=1, alpha=0.8, label='BNP')\nax1.set_title(\"Évolution du cours des rendements de l'action BNP\", fontsize=16)\nax1.set_xlabel('Temps', fontsize=14)\nax1.set_ylabel(\"Rendements des actions\", fontsize=14)\nax1.tick_params(axis='both', which='major', labelsize=12)\nax1.grid(True, linestyle='--', alpha=0.7)\nax1.legend(loc='best', fontsize=12)\n\n# Graphique pour SG\nax2.plot(data_returns[\"SG\"], 'b-', lw=1, alpha=0.8, label='SG')\nax2.set_title(\"Évolution du cours des rendements de l'action SG\", fontsize=16)\nax2.set_xlabel('Temps', fontsize=14)\nax2.set_ylabel(\"Rendements des actions\", fontsize=14)\nax2.tick_params(axis='both', which='major', labelsize=12)\nax2.grid(True, linestyle='--', alpha=0.7)\nax2.legend(loc='best', fontsize=12)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "risques/projets/Codes_copules.html#b-modéliser-judicieusement-les-distributions-univariées-des-facteurs-de-risques",
    "href": "risques/projets/Codes_copules.html#b-modéliser-judicieusement-les-distributions-univariées-des-facteurs-de-risques",
    "title": "Gestion de risques multiples : implémentations",
    "section": "3.b Modéliser judicieusement les distributions univariées des facteurs de risques",
    "text": "3.b Modéliser judicieusement les distributions univariées des facteurs de risques\n\nHistogrammes et courbes de densité des log rendements des actions de BNP et SG\n\n\nCode\ndef HistogPlot(data, label):\n    sns.kdeplot(data, color = \"r\", lw=2, alpha=0.7, label=label)  # Density\n    n, bins, patches = plt.hist(\n        data, bins=20, density=True, histtype=\"stepfilled\", alpha=0.5\n    )  # Histogram\n    \n    \n    ymax = plt.ylim()[1]  # Get the max y value for positioning text\n\n    # Ajout de la moyenne\n    plt.axvline(x=np.mean(data))\n    plt.text(np.mean(data), ymax * 0.5, \"Moyenne\", rotation=90)\n\n    # Ajout de la médiane\n    plt.axvline(x=np.median(data))\n    plt.text(np.median(data), ymax * 0.7, \"Médiane\", rotation=90)\n\n    # Adjout du mode\n    plt.axvline(x=(bins[n.argmax()] + bins[n.argmax() + 1]) / 2)\n    plt.text((bins[n.argmax()] + bins[n.argmax() + 1]) / 2, ymax * 0.2, \"Mode\", rotation=90)\n    plt.legend()\n\n\n# Plot\nplt.figure(figsize=(18, 5))\nax1 = plt.subplot(131)\nHistogPlot(data_returns[\"BNP\"], \"Densité des rendements d'actions BNP\")\nax2 = plt.subplot(132)\nHistogPlot(data_returns[\"SG\"], \"Densité des rendements d'action SG\")\nplt.show()\n\n\n\n\n\n\n\n\n\nDans les deux cas on observe des courbes en forme de cloche, et sensiblement symétrique, ce qui laisse présager une loi normale. Mais les kurtosis excessifs sont supérieurs à 2 et donc des kurtosis supérieurs à 5. Nous allons donc modéliser en plus des lois gaussiennes des lois à queues lourdes, notamment une Student et une Skew Student.\n\n\nModélisation des lois marginales associées aux log rendements\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import norm, t, kstest, skew\n\nfrom skew_student import*  # Import de la Skew-Student\n\ndef modeliser_distribution(data):\n    \"\"\"\n    Modélise une distribution normale, Student et Skew-Student, compare les ajustements et sélectionne la meilleure.\n    \n    Paramètre :\n    - data : array NumPy contenant l'échantillon de données.\n    \n    Retour :\n    - Un dictionnaire contenant la distribution sélectionnée et les p-values du KS test.\n    \"\"\"\n    \n    #  1. Estimation des paramètres\n    mu_norm, sigma_norm = norm.fit(data)  # Loi normale\n    df_t, loc_t, scale_t = t.fit(data)  # Loi de Student\n    mu_skew, sigma_skew, gamma_skew, nu_skew = optimize_parameters(data)  # Loi Skew-Student\n    \n    print(\"--\"*10,\"PARAMETRES ESTIMES\", \"--\"*10)\n    print(f\"Loi Normale - Moyenne: {mu_norm:.3f}, Écart-type: {sigma_norm:.3f}\")\n    print(f\"Loi de Student - df: {df_t:.3f}, Moyenne: {loc_t:.3f}, Échelle: {scale_t:.3f}\")\n    print(f\"Loi Skew-Student - μ: {mu_skew:.3f}, σ: {sigma_skew:.3f}, γ: {gamma_skew:.3f}, ν: {nu_skew:.3f}\")\n    print(\"--\"*30)\n    \n    #  2. Affichage des Q-Q Plots juxtaposés\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 3 colonnes pour ajouter la Skew-Student\n\n    # Q-Q Plot pour la loi Normale\n    stats.probplot(data, dist=\"norm\", sparams=(mu_norm, sigma_norm), plot=axes[0])\n    axes[0].set_title(\"Q-Q Plot - Loi Normale\")\n    axes[0].grid()\n\n    # Q-Q Plot pour la loi de Student\n    stats.probplot(data, dist=\"t\", sparams=(df_t, loc_t, scale_t), plot=axes[1])\n    axes[1].set_title(\"Q-Q Plot - Loi de Student\")\n    axes[1].grid()\n\n    # Q-Q Plot pour la loi Skew-Student\n    sorted_data = np.sort(data)\n    n = len(data)\n    quantiles_skew = skew_student_ppf((np.arange(1, n + 1) - 0.5) / n, mu_skew, sigma_skew, gamma_skew, nu_skew)\n\n    axes[2].scatter(quantiles_skew, sorted_data, color=\"blue\", alpha=0.6)\n    axes[2].plot(quantiles_skew, quantiles_skew, 'r--')  # Ligne théorique (y=x)\n    axes[2].set_title(\"Q-Q Plot - Loi Skew-Student\")\n    axes[2].grid()\n\n    plt.tight_layout()\n    plt.show()\n    \n    #  3. Test KS (Kolmogorov-Smirnov)\n    ks_stat_norm, ks_pval_norm = kstest(data, \"norm\", args=(mu_norm, sigma_norm))\n    ks_stat_t, ks_pval_t = kstest(data, \"t\", args=(df_t, loc_t, scale_t))\n    \n    # KS-Test pour la Skew-Student\n    np.random.seed(42)\n    sim_data = skew_student_sim(mu_skew, sigma_skew, gamma_skew, nu_skew, size = 100_000)\n    ks_stat_skew, ks_pval_skew =  kstest(data,sim_data)\n\n    print(f\"KS Test - Loi Normale: Stat={ks_stat_norm:.3f}, p-value={ks_pval_norm:.3f}\")\n    print(f\"KS Test - Loi de Student: Stat={ks_stat_t:.3f}, p-value={ks_pval_t:.3f}\")\n    print(f\"KS Test - Loi Skew-Student: Stat={ks_stat_skew:.3f}, p-value={ks_pval_skew:.3f}\")\n\n    #  4. Sélection de la meilleure distribution\n    p_values = {\"Normale\": ks_pval_norm, \"Student\": ks_pval_t, \"Skew-Student\": ks_pval_skew}\n    best_dist = max(p_values, key=p_values.get)  # Sélectionne la distribution avec la plus grande p-value\n    print(f\" Distribution sélectionnée: {best_dist}\")\n    \n    return {\n        \"Distribution\": best_dist, \n        \"p-value Normale\": ks_pval_norm, \n        \"p-value Student\": ks_pval_t, \n        \"p-value Skew-Student\": ks_pval_skew\n    }\n\n\n\n\nCode\nfrom skew_student import *\nfrom scipy.stats import kstest, norm, t, ks_2samp\n\n# Modélisation des rendement \nprint(\"Modélisation des rendements des actions BNP\\n\")\nresultats_BNP = modeliser_distribution(data_returns[\"BNP\"])\n\nprint(\"--\"*30,\"\\n\")\n\nprint(\"Modélisation des rendements des actions SG\\n\")\nresultats_SG = modeliser_distribution(data_returns[\"SG\"])\n\n\nModélisation des rendements des actions BNP\n\n\n\nC:\\Users\\mokom\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:728: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n  self.H.update(delta_x, delta_g)\nC:\\Users\\mokom\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:376: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n\n\n-------------------- PARAMETRES ESTIMES --------------------\nLoi Normale - Moyenne: -0.001, Écart-type: 0.024\nLoi de Student - df: 4.533, Moyenne: -0.001, Échelle: 0.019\nLoi Skew-Student - μ: 0.002, σ: 0.019, γ: -0.152, ν: 4.587\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nKS Test - Loi Normale: Stat=0.055, p-value=0.004\nKS Test - Loi de Student: Stat=0.026, p-value=0.487\nKS Test - Loi Skew-Student: Stat=0.023, p-value=0.681\n Distribution sélectionnée: Skew-Student\n------------------------------------------------------------ \n\nModélisation des rendements des actions SG\n\n-------------------- PARAMETRES ESTIMES --------------------\nLoi Normale - Moyenne: -0.001, Écart-type: 0.021\nLoi de Student - df: 4.593, Moyenne: -0.001, Échelle: 0.016\nLoi Skew-Student - μ: -0.002, σ: 0.016, γ: 0.073, ν: 4.585\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nKS Test - Loi Normale: Stat=0.049, p-value=0.015\nKS Test - Loi de Student: Stat=0.025, p-value=0.559\nKS Test - Loi Skew-Student: Stat=0.029, p-value=0.373\n Distribution sélectionnée: Student\n\n\n\nCommentaire\nA la lecture des QQ_plot, et à l’issu des tests d’adéquation de Kolmogorov smirnov:\n\nles rendements de BNP s’ajustent mieux à l’aide d’une skew-student (confirmation par le test de Kolmogorov-Smirnov, p-value = 0.68). Toutefois les queues de distribution semblent ne pas être bien ajustées.\nles rendements de SG s’ajustent aussi bien à l’aide d’une skew-student qu’à l’aide d’une student. Nous privilégions donc un modèle simple en retenant une student (p-value = 0.559, ks-test)."
  },
  {
    "objectID": "risques/projets/Codes_copules.html#dépendogramme",
    "href": "risques/projets/Codes_copules.html#dépendogramme",
    "title": "Gestion de risques multiples : implémentations",
    "section": "Dépendogramme",
    "text": "Dépendogramme\n\n\nCode\ndata_uniform = data_returns.rank(method='average', pct=True)\n# Nuage de points\nplt.scatter(data_uniform['BNP'], data_uniform['SG'])\nplt.xlabel(\"BNP\")\nplt.ylabel(\"SG\")\nplt.title(\"Scatter plot BNP vs SG\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nL’analyse du dépendodogramme suggère une dependance de queue dans nos données principalement marquée à droite."
  },
  {
    "objectID": "risques/projets/Codes_copules.html#a.-copules-elliptiques-gaussienne-student.",
    "href": "risques/projets/Codes_copules.html#a.-copules-elliptiques-gaussienne-student.",
    "title": "Gestion de risques multiples : implémentations",
    "section": "5.a. Copules elliptiques : gaussienne, Student.",
    "text": "5.a. Copules elliptiques : gaussienne, Student.\n\n\nCode\nfrom copulae import GaussianCopula\n\n# 1. Correction des index dupliqués\ndata_returns = data_returns.reset_index(drop=True)\n\n# 2. Transformation des données en rangs (pour obtenir des valeurs uniformes dans [0, 1])\nrank_data = data_returns.rank() / (len(data_returns) + 1)\n\n# 3. Ajustement de la copule Gaussienne sur les rangs\ncopula_gaussian = GaussianCopula()  # 2 variables\ncopula_gaussian.fit(rank_data.values)\n\n# 4. Génération d'échantillons de la copule (valeurs sur [0, 1])\nsamples = copula_gaussian.random(len(data_returns))\n\n# 5. Transformation inverse des rangs vers l'échelle originale\n# Utilisation de np.percentile qui prend en argument les pourcentages (multipliez par 100)\nsimulated_BNP = np.percentile(data_returns['BNP'], samples[:, 0] * 100)\nsimulated_SG  = np.percentile(data_returns['SG'], samples[:, 1] * 100)\nsimulated_returns = pd.DataFrame({\n    'BNP': simulated_BNP,\n    'SG': simulated_SG\n})\n\n# 6. Visualisation\nplt.figure(figsize=(10, 5))\nplt.scatter(rank_data['BNP'], rank_data['SG'], alpha=0.5, label=\"Données réelles\", color=\"blue\")\nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, label=\"Simulées (Copule Gaussienne)\", color=\"green\")\nplt.xlabel(\"BNP\")\nplt.ylabel(\"SG\")\nplt.title(\"Copule Gaussienne : Réelles vs simulées\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncopula_gaussian.summary()\n\n\n\n    Gaussian Copula Summary\n    Gaussian Copula with 2 dimensions\n    \n    \n\n    Parameters\n    Correlation Matrix\n\n\n\n1.000000\n0.860948\n\n\n0.860948\n1.000000"
  },
  {
    "objectID": "risques/projets/Codes_copules.html#copules-elliptiques-gaussienne-student.",
    "href": "risques/projets/Codes_copules.html#copules-elliptiques-gaussienne-student.",
    "title": "Gestion de risques multiples : implémentations",
    "section": "Copules elliptiques : gaussienne, Student.",
    "text": "Copules elliptiques : gaussienne, Student.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t as t_dist\nfrom scipy.optimize import minimize\nfrom math import log, gamma, pi, sqrt\n\n# =============================================================================\n# 1) Fonctions de densité t-Student (univariée & bivariée) en log\n# =============================================================================\n\ndef log_t_pdf_univariate(x, df):\n    \"\"\"\n    Retourne ln[f_nu(x)] pour la loi t univariée de df degrés de liberté.\n    Formule :\n      f_nu(x) = Gamma((df+1)/2) / [ sqrt(df*pi)*Gamma(df/2) ] * [1 + x^2/df]^(-(df+1)/2)\n    \"\"\"\n    c1 = np.log(gamma((df+1)/2)) - np.log(gamma(df/2)) - 0.5*np.log(df*pi)\n    c2 = -(df+1)/2 * np.log(1 + x**2/df)\n    return c1 + c2\n\ndef log_t_pdf_bivariate(x, y, df, r):\n    \"\"\"\n    Retourne ln[f_{df,r}(x,y)] pour la t-Student bivariée, corrélation r, df degrés de liberté.\n\n      f_{df,r}(x,y) =\n         Gamma((df+2)/2) / [ Gamma(df/2) * (pi*df) * sqrt(1-r^2) ]\n         * [1 + 1/df * (x^2 - 2rxy + y^2)/(1-r^2)]^(-(df+2)/2)\n\n    On renvoie le log de cette densité.\n    \"\"\"\n    # Partie constante\n    log_num_gamma = np.log(gamma((df+2)/2)) - np.log(gamma(df/2))\n    log_denom = np.log(pi*df) + 0.5*np.log(1 - r**2)\n    cst = log_num_gamma - log_denom\n\n    # Partie quadratique\n    quad = (x**2 - 2*r*x*y + y**2)/(1 - r**2)\n    log_noyau = - (df+2)/2 * np.log(1 + quad/df)\n\n    return cst + log_noyau\n\n# =============================================================================\n# 2) Log-densité de la copule t-Student (bivariée)\n# =============================================================================\n\ndef log_student_copula_2d(u1, u2, df, r):\n    \"\"\"\n    Calcule ln[c_{df,r}(u1,u2)] = ln f_{df,r}(x,y) - [ln f_df(x) + ln f_df(y)],\n    où x = t_{df}^{-1}(u1) et y = t_{df}^{-1}(u2).\n\n    - u1, u2 dans [0,1].\n    - df &gt; 0, r in (-1,1).\n    \"\"\"\n    # Inversion via la quantile function (ppf) univariée\n    x = t_dist.ppf(u1, df)\n    y = t_dist.ppf(u2, df)\n\n    # Log densité jointe\n    log_joint = log_t_pdf_bivariate(x, y, df, r)\n    # Log densités marginales (univariées)\n    log_marg_x = log_t_pdf_univariate(x, df)\n    log_marg_y = log_t_pdf_univariate(y, df)\n\n    return log_joint - (log_marg_x + log_marg_y)\n\n# =============================================================================\n# 3) Log-vraisemblance négative\n# =============================================================================\n\ndef negative_log_likelihood(params, data):\n    \"\"\"\n    Calcule - somme [ ln c_{df,r}(u1,u2) ] sur tout l'échantillon.\n    data : array shape (n,2) contenant les (u1,u2) dans [0,1].\n    params : (alpha, beta) --&gt; r, df via reparamétrage :\n\n      r  = tanh(alpha)   --&gt; r in (-1,1)\n      df = 2 + log(1 + exp(beta))  --&gt; df &gt; 2 (par exemple)\n    \"\"\"\n    alpha, beta = params\n    r = np.tanh(alpha)  # in (-1,1)\n    df = 2.0 + np.log1p(np.exp(beta))  # &gt; 2\n\n    ll = 0.0\n    for (u1, u2) in data:\n        ll += log_student_copula_2d(u1, u2, df, r)\n\n    return -ll  # On renvoie la -log-vraisemblance\n\n# =============================================================================\n# 4) Fonction d'ajustement (fit) par MLE\n# =============================================================================\n\ndef fit_student_copula_2d(data, alpha0=0.0, beta0=1.0):\n    \"\"\"\n    Ajuste la copule t-Student bivariée à un échantillon de pseudo-observations 'data' (n,2).\n    Minimisation de la -log-vraisemblance w.r.t. (alpha,beta).\n      alpha0, beta0 : valeurs initiales pour (alpha, beta).\n\n    Retourne un dict contenant r, df, logLik, etc.\n    \"\"\"\n    res = minimize(\n        fun=negative_log_likelihood,\n        x0=np.array([alpha0, beta0]),\n        args=(data,),\n        method='Nelder-Mead'\n    )\n    alpha_hat, beta_hat = res.x\n    r_hat = np.tanh(alpha_hat)\n    df_hat = 2.0 + np.log1p(np.exp(beta_hat))\n\n    ll_hat = -res.fun  # log-vraisemblance au point optimum\n\n    return {\n        'r': r_hat,\n        'df': df_hat,\n        'logLik': ll_hat,\n        'success': res.success,\n        'message': res.message\n    }\n\n# =============================================================================\n# 5) Utilisation sur données data_returns\n# =============================================================================\n\n# 1) Transformer vos rendements en pseudo-observations dans [0,1]\nrank_data = data_returns.rank(method='average') / (len(data_returns) + 1)\nu_data = rank_data.values  # numpy array de taille (n,2)\n\n# 2) Ajustement de la copule t-Student bivariée par MLE\nresult = fit_student_copula_2d(u_data, alpha0=0.0, beta0=1.0)\n\nprint(\"=== Estimation Copule t-Student Bivariée ===\")\nprint(f\"Paramètre r estimé = {result['r']:.4f}\")\nprint(f\"Paramètre df estimé = {result['df']:.4f}\")\nprint(f\"Log-vraisemblance  = {result['logLik']:.4f}\")\nprint(\"Convergence :\", result['success'], \"|\", result['message'])\n\n\n=== Estimation Copule t-Student Bivariée ===\nParamètre r estimé = 0.8496\nParamètre df estimé = 2.0000\nLog-vraisemblance  = 719.7436\nConvergence : True | Optimization terminated successfully.\n\n\n\n\nCode\nfrom copulae import GaussianCopula, StudentCopula, GumbelCopula, FrankCopula, ClaytonCopula\nfrom scipy.stats import kendalltau, spearmanr\n\n# Chargement des données\ndata_returns = data_returns.reset_index(drop=True)\n\n# Transformation en pseudo-observations (rangs normalisés)\nrank_data = data_returns.rank() / (len(data_returns) + 1)\n\n# Définition des copules\ncopules = {\n    \"Gaussian\": GaussianCopula(dim=2),\n    \"Student-t\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Ajustement des copules aux données empiriques\nfit_results = {}\nfor name, cop in copules.items():\n    if name == \"Student-t\":\n        cop.fit(rank_data.values, fix_df=True)\n    else:\n        cop.fit(rank_data.values)\n    fit_results[name] = cop\n\n# Comparaison des mesures de dépendance\nempirical_tau, _ = kendalltau(data_returns[\"BNP\"], data_returns[\"SG\"])\nempirical_rho, _ = spearmanr(data_returns[\"BNP\"], data_returns[\"SG\"])\n\nresults = []\nfor name, cop in fit_results.items():\n    simulated_data = cop.random(len(data_returns))\n    sim_tau, _ = kendalltau(simulated_data[:, 0], simulated_data[:, 1])\n    sim_rho, _ = spearmanr(simulated_data[:, 0], simulated_data[:, 1])\n    \n    # Nouveau calcul de la log-vraisemblance via la densité jointe\n    log_likelihood = np.sum(np.log(cop.pdf(rank_data.values)))\n\n    results.append([name, log_likelihood, abs(sim_tau - empirical_tau), abs(sim_rho - empirical_rho)])\n\n# Affichage des résultats\nresults_df = pd.DataFrame(results, columns=[\"Copule\", \"Log-Vraisemblance\", \"Erreur Kendall Tau\", \"Erreur Spearman Rho\"])\nprint(results_df.sort_values(by=\"Log-Vraisemblance\", ascending=False))\n\n\n      Copule  Log-Vraisemblance  Erreur Kendall Tau  Erreur Spearman Rho\n1  Student-t         720.838328            0.003462             0.000855\n2     Gumbel         703.146279            0.007835             0.004465\n0   Gaussian         670.141798            0.000534             0.016872\n3      Frank         630.219249            0.001741             0.023079\n4    Clayton         517.723513            0.091739             0.073704\n\n\n\n\nCode\n# Visualisation des distributions simulées\nplt.figure(figsize=(12, 8))\nfor i, (name, cop) in enumerate(fit_results.items(), 1):\n    simulated_data = cop.random(len(data_returns))\n    plt.subplot(3, 2, i)\n    plt.scatter(rank_data[\"BNP\"], rank_data[\"SG\"], alpha=0.5, label=\"Données réelles\", color=\"blue\")\n    plt.scatter(simulated_data[:, 0], simulated_data[:, 1], alpha=0.5, label=f\"Simulées ({name})\", color=\"green\")\n    plt.title(f\"Copule {name}\")\n    plt.xlabel(\"BNP\")\n    plt.ylabel(\"SG\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import gaussian_kde\n\nrank_data = data_returns.rank() / (len(data_returns) + 1)\nu = rank_data.iloc[:, 0].values\nv = rank_data.iloc[:, 1].values\nn = len(u)\n\n# Calcul de la statistique empirique V_i (distribution de Kendall)\nmask = (u[:, np.newaxis] &lt;= u) & (v[:, np.newaxis] &lt;= v)\ncounts = mask.sum(axis=1)\nV = (counts - 1) / (n - 1)  # Ajustement pour l'auto-comparaison\nV_trié = np.sort(V)\n\n# Définition des copules à tester\ncopules = {\n    \"Gaussienne\": GaussianCopula(dim=2),\n    \"Student-t\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Paramètres de simulation\nnb_simulations = 100  # Augmenter pour un résultat plus lisse\n\n# Génération des graphiques de Kendall\nplt.figure(figsize=(10, 10))\n\nfor i, (nom, copule) in enumerate(copules.items(), 1):\n    # Ajustement de la copule aux données\n    if nom == \"Student-t\":\n        copule.fit(rank_data.values, fix_df=True)\n    else:\n        copule.fit(rank_data.values)\n    \n    # Simulation de plusieurs jeux de données et calcul des statistiques V\n    V_simulé = np.zeros((nb_simulations, n))\n    \n    for s in range(nb_simulations):\n        # Génération de données synthétiques à partir de la copule ajustée\n        données_simulées = copule.random(n)\n        \n        # Calcul de V pour les données simulées\n        u_sim = données_simulées[:, 0]\n        v_sim = données_simulées[:, 1]\n        mask_sim = (u_sim[:, np.newaxis] &lt;= u_sim) & (v_sim[:, np.newaxis] &lt;= v_sim)\n        counts_sim = mask_sim.sum(axis=1)\n        V_sim = (counts_sim - 1) / (n - 1)\n        V_simulé[s] = np.sort(V_sim)  # Tri des valeurs simulées\n    \n    # Calcul des valeurs moyennes attendues à partir des simulations\n    V_attendu = V_simulé.mean(axis=0)\n    \n    # Tracé du graphique de Kendall avec une ligne fine pour mieux voir l'ajustement\n    plt.subplot(3, 2, i)\n    plt.plot(V_attendu, V_trié, lw=1, color='blue', label='Données vs Copule')\n    plt.plot([0, 1], [0, 1], '--r', lw=1, label='Ajustement parfait')\n    plt.xlabel(f'Valeurs attendues ({nom})')\n    plt.ylabel('Valeurs empiriques')\n    plt.title(f'Copule {nom}')\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import gaussian_kde\n\n# Configuration\ngrid_size = 100        # Résolution de la grille\ncontour_levels = 15    # Nombre de niveaux de contour\n\n# Préparation des données : transformation en marges uniformes par rang\nrank_data = data_returns.rank() / (len(data_returns) + 1)\nu = rank_data.iloc[:, 0]\nv = rank_data.iloc[:, 1]\n\n# Création de la grille d'évaluation\nx = np.linspace(0, 1, grid_size)\ny = np.linspace(0, 1, grid_size)\nX, Y = np.meshgrid(x, y)\ngrid_points = np.vstack([X.ravel(), Y.ravel()]).T\n\n# Calcul de la KDE empirique sur les données observées\nempirical_kernel = gaussian_kde(rank_data.values.T)\nZ_empirical = empirical_kernel(grid_points.T).reshape(X.shape)\n\n# Définition des copules à comparer\ncopulas = {\n    \"Gaussian\": GaussianCopula(dim=2),\n    \"Student\": StudentCopula(dim=2, df=2),\n    \"Gumbel\": GumbelCopula(dim=2),\n    \"Frank\": FrankCopula(dim=2),\n    \"Clayton\": ClaytonCopula(dim=2)\n}\n\n# Nombre d'observations simulées pour estimer la KDE théorique\nn_sim = 5000\n\n# Création de la figure avec une ligne par copule et 2 colonnes (empirique vs théorique)\nfig, axs = plt.subplots(nrows=len(copulas), ncols=2, figsize=(15, 25))\nplt.subplots_adjust(hspace=0.4)\n\nfor idx, (name, copula) in enumerate(copulas.items()):\n    # Ajustement de la copule aux données\n    if name == \"Student\":\n        copula.fit(rank_data.values, fix_df=True)\n    else:\n        copula.fit(rank_data.values)\n    \n    # Simulation d'un échantillon théorique à partir de la copule ajustée\n    simulated_data = copula.random(n_sim)\n    \n    # Calcul de la KDE théorique à partir de l'échantillon simulé\n    theoretical_kernel = gaussian_kde(simulated_data.T)\n    Z_theoretical = theoretical_kernel(grid_points.T).reshape(X.shape)\n    \n    # Affichage de la KDE empirique\n    ax_emp = axs[idx, 0]\n    cf_emp = ax_emp.contourf(X, Y, Z_empirical, levels=contour_levels, cmap='viridis', alpha=0.8)\n    ax_emp.scatter(u, v, s=10, color='white', edgecolors='black', alpha=0.7)\n    ax_emp.set_title(f'{name} Copule - KDE Empirique')\n    ax_emp.set_xlabel('u')\n    ax_emp.set_ylabel('v')\n    ax_emp.grid(True, linestyle='--', alpha=0.5)\n    \n    # Affichage de la KDE théorique\n    ax_theo = axs[idx, 1]\n    cf_theo = ax_theo.contourf(X, Y, Z_theoretical, levels=contour_levels, cmap='viridis', alpha=0.8)\n    ax_theo.scatter(u, v, s=10, color='white', edgecolors='black', alpha=0.7)\n    ax_theo.set_title(f'Copule {name} - KDE Théorique')\n    ax_theo.set_xlabel('u')\n    ax_theo.set_ylabel('v')\n    ax_theo.grid(True, linestyle='--', alpha=0.5)\n    \n    # Ajout d'une colorbar pour le graphique théorique\n    fig.colorbar(cf_theo, ax=ax_theo, orientation='vertical', shrink=0.9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(20, 15))\n\nfor idx, (name, copula) in enumerate(copulas.items(), 1):\n    # Calcul de la densité théorique\n    # Fit copula to data\n    if name == \"Student\":\n        copula.fit(rank_data.values, fix_df = True)\n    else:\n        copula.fit(rank_data.values)\n        \n    pdf = copula.pdf(grid_points).reshape(X.shape)\n    \n    # Graphique 3D\n    density_values = empirical_kernel(np.vstack([u, v]))\n\n    ax = fig.add_subplot(3, 2, idx, projection='3d')\n    ax.plot_surface(X, Y, pdf, cmap='viridis', alpha=0.7)\n    ax.scatter(u, v, density_values, c='red', s=10, label='Données')\n    ax.set_title(f'Densité {name} Copula\\n(Surface théorique vs Points empiriques)')\n    ax.set_xlabel('u')\n    ax.set_ylabel('v')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n⬅ Retour"
  },
  {
    "objectID": "docs/risques/projets/CreditVar.html",
    "href": "docs/risques/projets/CreditVar.html",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "",
    "text": "⬅ Retour\n\nCONTEXTE\n\nL’objectif de la présent étude est d’évaluer, au moyen d’une CVAR à 99%, le risque de crédit sur un portefeuille fictif composé de deux créances, issues du secteur bancaire, de même notionnel 1000 EUR et de même maturité 4 ans.\nLa première est une obligation BNP senior de taux de recouvrement de moyenne 60% et de volatilité 15%, et la seconde est une obligation Société Générale junior (ou subordonnée) de taux de recouvrement de moyenne 30% et de volatilité 25%. On suppose qu’il n’y a pas de dépendance entre les taux de recouvrement.\nLa perte de crédit attendue pour ce portefeuille peut s’écrire sous la forme mathématique suivante\n\\(L= EAD_{BNP} (1 - R_{BNP}) \\mathbf{1}_{\\tau_{BNP} \\leq 4} + EAD_{SG} (1 - R_{SG}) \\mathbf{1}_{\\tau_{SG} \\leq 4}\\) où\n\n\\(EAD_i\\) est la perte en cas de défaut. Dans notre cas elle au notionnel;\n\\(R_i\\) est le taux de recouvrement;\n\\(\\tau_i\\) est le moment de survenu du défaut.\n\nNous souhaitons retrouver une CVAR à 99% qui s’écrit \\(CVAR_{99\\%} = inf\\{x, \\mathbf{P}(L \\leq x) \\geq 99\\%\\}\\).S\nDans l’expression donnée de la perte, on note la présence de variables aléatoires (le taux de recouvrement et le moment de survenu du défaut). La caractérisation des pertes est donc conditionnée par la connaissance des distributions de ces variables. Nous allons nous evertuer tout le long de ce projet à modéliser cette VaR et pour ce faire, nous procédons suivant une méthodologie s’appuyant sur la théorie des copules que nous déroulons de manière détaillée dans le rapport suivant: ➡ Rapport\n\nRetrouvez l’essentiel des codes implémentés dans le cadre de projet ici ➡ Codes"
  },
  {
    "objectID": "finance_quant/index.html#vous-trouverez-des-ressources-bibliographiques-que-je-vous-suggère-ici-codes",
    "href": "finance_quant/index.html#vous-trouverez-des-ressources-bibliographiques-que-je-vous-suggère-ici-codes",
    "title": "Finance Quantitative",
    "section": "Vous trouverez des ressources bibliographiques que je vous suggère ici ➡ Codes",
    "text": "Vous trouverez des ressources bibliographiques que je vous suggère ici ➡ Codes\n\n📘 Chapitres disponibles\n\nChapitre 1 : Introduction aux Instruments Financiers\n\n\n\n\nPartie II — Travaux, applications et études"
  },
  {
    "objectID": "finance_quant/index.html#section",
    "href": "finance_quant/index.html#section",
    "title": "Finance Quantitative",
    "section": " ",
    "text": "Courbe de taux & modèle de Hull–White\n\n\nBootstrapping, valorisation caplets/swap­tion, calibration Hull–White\n\nLire"
  },
  {
    "objectID": "finance_quant/chapter_one.html",
    "href": "finance_quant/chapter_one.html",
    "title": "Chapitre 1 : Introduction aux Instruments Financiers",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "finance_quant/chapter_one.html#comprendre-les-instruments-financiers",
    "href": "finance_quant/chapter_one.html#comprendre-les-instruments-financiers",
    "title": "Chapitre 1 : Introduction aux Instruments Financiers",
    "section": "Comprendre les Instruments Financiers",
    "text": "Comprendre les Instruments Financiers\nLes instruments financiers constituent les briques fondamentales des marchés. Ils représentent soit un droit de propriété (action), soit une créance (obligation), soit un contrat dérivé dont la valeur dépend d’un actif sous-jacent.\nIls jouent un rôle central dans l’orientation du capital, la gestion des risques et le financement de l’économie.\nOn distingue traditionnellement deux grandes familles :\n\nLes instruments de base : actions, obligations, devises, instruments monétaires.\n\nLes produits dérivés : contrats dont la valeur dépend d’un actif sous-jacent (taux, actions, indices, matières premières…).\n\nL’essor de ces produits, en particulier pour la couverture des risques, a conduit à une forte technicisation des salles de marché et à l’arrivée de profils spécialisés (ingénieurs, mathématiciens, informaticiens)."
  },
  {
    "objectID": "finance_quant/chapter_one.html#les-marchés-financiers",
    "href": "finance_quant/chapter_one.html#les-marchés-financiers",
    "title": "Chapitre 1 : Introduction aux Instruments Financiers",
    "section": "Les Marchés Financiers",
    "text": "Les Marchés Financiers\n\nMarchés OTC (de gré à gré)\nLes marchés OTC regroupent l’ensemble des transactions négociées directement entre deux contreparties. Ils permettent une flexibilité maximale : chaque contrat peut être adapté aux besoins précis des institutions (montants, dates, maturités, devises, structure des flux…).\nAtouts : - forte personnalisation des contrats,\n- maturités et montants ajustables,\n- possibilité de concevoir des produits complexes ou sur mesure.\nLimites : - absence de garantie du meilleur prix,\n- présence d’un risque de contrepartie : la qualité de crédit du partenaire est essentielle.\n\n\n\nMarchés Listés\nÀ l’inverse, les marchés listés organisent l’échange de produits standardisés à travers un carnet d’ordres centralisé. Les transactions sont sécurisées par une chambre de compensation, qui garantit leur bonne exécution.\nAvantages : - transparence et visibilité du prix,\n- liquidité souvent élevée,\n- risque de contrepartie fortement réduit.\nInconvénients : - manque de flexibilité,\n- standardisation parfois incompatible avec des besoins spécifiques."
  },
  {
    "objectID": "finance_quant/chapter_one.html#les-produits-financiers-de-base",
    "href": "finance_quant/chapter_one.html#les-produits-financiers-de-base",
    "title": "Chapitre 1 : Introduction aux Instruments Financiers",
    "section": "Les Produits Financiers de Base",
    "text": "Les Produits Financiers de Base\n\nLes Obligations\nUne obligation est un titre de dette émis par un État, une entreprise ou une institution financière. Le détenteur perçoit des coupons réguliers puis le remboursement du principal à l’échéance.\nLes obligations se distinguent notamment par :\n\nleur séniorité en cas de défaut,\n\nleur risque de crédit,\n\nleur structure de flux (coupon fixe, variable, zéro-coupon…).\n\nElles constituent un pilier des marchés financiers, particulièrement utilisé pour le financement des États et des grandes entreprises.\n\n\n\nLes Actions\nUne action représente une part de propriété dans une entreprise. Elle donne droit : - à des dividendes, - à un droit de vote, - à une participation à la croissance future de la firme.\nLa valorisation d’une action repose sur l’anticipation des flux futurs et sur les conditions de marché.\nLes indices boursiers (CAC 40, S&P 500…) permettent de suivre l’évolution d’un ensemble représentatif de titres."
  },
  {
    "objectID": "finance_quant/chapter_one.html#les-premiers-dérivés-forwards-futures-swaps",
    "href": "finance_quant/chapter_one.html#les-premiers-dérivés-forwards-futures-swaps",
    "title": "Chapitre 1 : Introduction aux Instruments Financiers",
    "section": "Les Premiers Dérivés : Forwards, Futures, Swaps",
    "text": "Les Premiers Dérivés : Forwards, Futures, Swaps\n\nContrats Forward\nUn forward est un contrat dans lequel deux parties s’engagent à acheter ou vendre un actif à une date future et à un prix fixé dès aujourd’hui.\nIl permet : - de fixer un prix à l’avance,\n- de couvrir un risque (par exemple, un risque de change sur un paiement futur).\nCes contrats sont généralement négociés OTC et peuvent être entièrement personnalisés.\n\n\n\nContrats Futures\nLes futures sont la version standardisée des forwards, négociée sur un marché listé. Ils présentent plusieurs avantages :\n\nune forte transparence des prix,\n\nune grande liquidité,\n\nune élimination quasi totale du risque de contrepartie grâce à la chambre de compensation.\n\nIls constituent l’un des outils les plus utilisés pour couvrir des risques de taux, de change ou de matières premières.\n\n\n\nSwaps de Taux\nUn swap de taux d’intérêt est un contrat dans lequel deux parties s’échangent des flux financiers, typiquement :\n\nun flux à taux fixe,\n\ncontre un flux à taux variable.\n\nLes swaps permettent de transformer la nature d’un financement ou d’une position, par exemple passer d’un taux variable à un taux fixe, ou inversement.\nIls sont devenus l’un des instruments les plus utilisés au monde pour la gestion du risque de taux."
  },
  {
    "objectID": "finance_quant/index.html",
    "href": "finance_quant/index.html",
    "title": "Finance Quantitative",
    "section": "",
    "text": "Vous trouverez ci après des ressources bibliographiques que je vous suggère : ➡ Codes"
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html",
    "href": "finance_quant/projets/projet_courbe_taux.html",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "",
    "text": "Ce projet vise à explorer les concepts de base des modèles de courbe de taux, en abordant les sujets suivants :\n\nReconstituer une courbe de taux zéro-coupon\n\nValoriser des instruments de taux dérivés (caplets, swaptions)\n\nExplorer le modèle de Hull & White pour la calibration et le pricing d’options de taux\n\n\n\n\n\n\n\n\nUtilisation de la méthode du bootstrapping et de l’interpolation spline pour reconstruire la courbe des taux zéro-coupon à partir des données de marché.\n\n\n\n\n\nValorisation des dérivés de taux comme les caplets et les swaptions.\n\nConstruction des courbes de taux forward.\n\n\n\n\n\nCalcul des prix de marché à partir de la courbe zéro-coupon.\n\nComparaison des interpolations linéaire vs spline pour déterminer la méthode la plus adaptée à la valorisation.\n\n\n\n\n\nTransition depuis le modèle HJM (Heath–Jarrow–Morton) vers le modèle Hull & White.\n\nFormulation mathématique du modèle et calibration à partir des données de marché.\n\nSimulation Monte-Carlo pour la valorisation d’options structurées basées sur ce modèle.\n\n\n\n\nCode\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt \n\nimport seaborn as sns\n\n\n\nfrom scipy.interpolate import interp1d, CubicSpline\n\nfrom tabulate import tabulate\n\nfrom scipy.stats import norm"
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#objectif-du-projet",
    "href": "finance_quant/projets/projet_courbe_taux.html#objectif-du-projet",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "",
    "text": "Ce projet vise à explorer les concepts de base des modèles de courbe de taux, en abordant les sujets suivants :\n\nReconstituer une courbe de taux zéro-coupon\n\nValoriser des instruments de taux dérivés (caplets, swaptions)\n\nExplorer le modèle de Hull & White pour la calibration et le pricing d’options de taux"
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#contenu",
    "href": "finance_quant/projets/projet_courbe_taux.html#contenu",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "",
    "text": "Utilisation de la méthode du bootstrapping et de l’interpolation spline pour reconstruire la courbe des taux zéro-coupon à partir des données de marché.\n\n\n\n\n\nValorisation des dérivés de taux comme les caplets et les swaptions.\n\nConstruction des courbes de taux forward.\n\n\n\n\n\nCalcul des prix de marché à partir de la courbe zéro-coupon.\n\nComparaison des interpolations linéaire vs spline pour déterminer la méthode la plus adaptée à la valorisation.\n\n\n\n\n\nTransition depuis le modèle HJM (Heath–Jarrow–Morton) vers le modèle Hull & White.\n\nFormulation mathématique du modèle et calibration à partir des données de marché.\n\nSimulation Monte-Carlo pour la valorisation d’options structurées basées sur ce modèle.\n\n\n\n\nCode\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt \n\nimport seaborn as sns\n\n\n\nfrom scipy.interpolate import interp1d, CubicSpline\n\nfrom tabulate import tabulate\n\nfrom scipy.stats import norm"
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#formules-de-valorisation-des-taux-de-marché",
    "href": "finance_quant/projets/projet_courbe_taux.html#formules-de-valorisation-des-taux-de-marché",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Formules de valorisation des taux de marché",
    "text": "Formules de valorisation des taux de marché\n\n\nLe tableau de l’énoncé (courbe de taux de marché) décrit une courbe de taux interbancaires.\nEn effet, les instruments qu’il contient, et desquels on derivera nos taux zero coupons pour differentes maturités, sont essentiellement des taux de marché monétaire (money market), des contrats futures et des swaps.\nCe tableau contient trois types d’instruments financiers:\n\nDes Money market : Il s’agit des taux (monétaires) à court terme sur le marché interbancaire. Dans le contexte de notre exercice, il s’agit de taux Libor pour les maturités allant de 3 mois à 1 an.\nDes contrats futures de taux d’intérêt :Ce sont des instruments financiers négociés sur un marché réglementé, permettant aux acteurs du marché\nde s’engager dès aujourd’hui sur l’évolution d’un taux d’intérêt futur à un niveau prédéfini. Ils sont utilisés aussi bien à des fins de couverture contre le risque de taux que pour la spéculation. Afin de limiter le risque de contrepartie, ces contrats sont sujets à des appels de marge quotidiens. La courbe de taux de marché présente des futures de taux d’intérêt avec des maturités comprises entre 1.25 et 2.75 ans.\nDes swaps de taux d’intérêts: Un swap de taux d’intérêt est un contrat entre deux parties visant à échanger des flux financiers basés sur un montant notionnel déterminé. Généralement, l’une des parties verse des paiements à taux fixe, tandis que l’autre paie des intérêts à taux variable, indexés sur un indice de référence (dans notre cas, le Libor). Cet échange suit un échéancier prédéfini et permet aux acteurs du marché de se couvrir contre le risque de taux d’intérêt ou d’optimiser leurs coûts de financement.\nLa courbe de taux de marché contient des Swaps dont les tenors vont de 3 à 30 ans\n\n\\end{itemize}\nSous l’hypothèse d’absence d’opportunuité d’arbitrage ces instruments sont valorisés par les formules suivantes:\n\nMoney Market (Taux libor): Etant donné une période d’intérêt \\([T, T+\\delta]\\), la valeur du tau libor vu aujourd’hui est donnée par\n$$\n\\[\\begin{equation}\n\nL_0(T,T+\\delta) = \\frac{1}{\\delta} \\left( \\frac{B(0,T)}{B(0,T + \\delta)} - 1 \\right)\n\n\\end{equation}\\]\n$$\nLa valeur vue d’aujourd’hui du future de taux d’intérêt pour la date de départ \\(T\\) et de tenor \\(T+\\delta\\) est donnée par:\n$$\n\\[\\begin{equation}\n\nF(0,T, T+\\delta) = 1 - L_0(T,T+\\delta)\n\n\\end{equation}\\]\n$$\nEtant donné les échéanciers $(t_i)_{i=0,…,m} $(pour la jambe fixe) et \\((t_j)_{j=0,...,n}\\) (pour la jambe flottante), on définit le swap à départ \\(T_0 = t_0 = \\tilde{t}_0\\) et de maturité \\(T_n = t_m = \\tilde{t}_n\\). On introduit les nominaux respectifs de chaque jambe \\((N_i)_{i=1,...,m}\\) et \\(( \\tilde{N})_{j=1,...,n}\\) ainsi que les taux fixes \\((K_i)_{i=1,...,m}\\) et les spreads \\((s_i)_{i=1,...,m}\\).\nA la date de pricing t = 0, le prix du swap est donnée par:\n$$\n\\[\\begin{equation}\n\nS_0(T_0, T) = \\frac{\\sum_j^n \\tilde{N}_j(\\tilde{t}_j - \\tilde{t}_{j-1})(L(\\tilde{t}_{j-1}, \\tilde{t}_j) + s_j)B(0, \\tilde{t}_j)}{\\sum_i^m N_i(t_i-t_{i-1})B(0,t_i)}\n\n\\end{equation}\\]\n$$\n\nRappelons le principe du bootstrapping et son importance.\nLe principe de base du bootstrapping est de restituer - à partir de l’information disponible sur le marché - une courbe de taux zéro-coupon\ndiscrète à l’aide d’une approche récursive.\nCette méthode est essentielle car il n’existe pas sur le marché un continuum de cotations d’obligations zéro-coupon; ceci rend impossible l’obtention directe des taux zéro-coupon pour toutes les maturités. Le bootstraping constitue donc un outil indispensable pour la construction de la courbe des taux zéro-coupon par terme, qui est la brique fondamentale pour la valorisation des instruments financiers.\nRappelons la différence entre une opération forward et future.\nLa différence entre un contrat forward et un contrat future réside principalement dans leur mode de négociation et de règlement. Un forward est un contrat de gré à gré, réglé en un unique paiement à l’échéance, correspondant à la différence entre le taux négocié et le taux de marché réalisé. À l’inverse, un future est standardisé et négocié sur un marché réglementé, avec un ajustement quotidien de sa valeur via des appels de marge (mark-to-market), répartissant ainsi les gains et pertes tout au long de la durée du contrat."
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#construction-de-la-courbe-des-taux-zéro-coupon",
    "href": "finance_quant/projets/projet_courbe_taux.html#construction-de-la-courbe-des-taux-zéro-coupon",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Construction de la courbe des taux zéro-coupon",
    "text": "Construction de la courbe des taux zéro-coupon\n\n\nL’interpolation des taux de swap pour obtenir des cotations annuelles simplifie la méthode du bootstrap en fournissant une courbe de taux continue pour toutes les maturités. Cela aligne les échéances des instruments de marché avec celles utilisées dans la construction de la courbe zéro-coupon, évitant ainsi de gérer des échéances intermédiaires ou irrégulières. Avec des intervalles de temps constants égaux à 1 an, les calculs deviennent plus simples et plus directs, permettant une extraction précise des taux zéro-coupon à chaque étape.\nConstruisons sur le segment des SWAP une nouvelle courbe de taux de marché avec des cotations annuelles obtenues à l’aide d’une méthode d’interpolation par spline.\n\n\n\nCode\n# Import data\n\ndf = pd.read_excel(\"Data.xlsx\")\n\ndf.columns = [\"Market\", \"MAT\", \"MKT\"]\n\ndf.head()\n\n\n\n\n\n\n\n\n\nMarket\nMAT\nMKT\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n\n\n\n\n\n\n\nCode\ndef interpolation_spline_swap(df):\n\n    df_swap = df[df[\"Market\"] == 'SWAP']\n\n    # Appliquons une interpolation spline cubique\n\n    cs = CubicSpline(df_swap['MAT'].values, df_swap['MKT'].values)\n\n\n\n    # générons les points pour l'affichage de la courbe \n\n    maturities_gen = np.linspace(min(df_swap['MAT'].values), max(df_swap['MAT'].values), 100)\n\n\n\n    swap_rate_gen = cs(maturities_gen)\n\n    data_gen = {\n\n        \"Market\": ['SWAP'] * len(maturities_gen),\n\n        'MAT': maturities_gen.tolist(),\n\n        'MKT': swap_rate_gen.tolist()\n\n    }\n\n\n\n    data_gen = pd.DataFrame(data_gen)\n\n    return data_gen\n\n\n\n\nCode\ndata_swap =  interpolation_spline_swap(df)\n\nplt.plot(data_swap['MAT'].values,data_swap['MKT'].values, label =\"Coutbe de taux swap interpolés\")\n\nplt.scatter(df[df[\"Market\"]== \"SWAP\"]['MAT'].values,df[df[\"Market\"]== \"SWAP\"]['MKT'].values,color = \"red\", label = \"Taux swap\", zorder =3)\n\n\n\nplt.title(\"Courbe des taux swap de marché avec interpolation cubique\")\n\nplt.xlabel(\"Maturité (années)\")  # Axe des abscisses\n\nplt.ylabel(\"Taux (%)\")\n\n\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDans la suite, on supposera que pour ce segment, cette nouvelle courbe est la courbe de marché de référence\n\n\nCalculons à présent les taux zéro-coupon continus associés à toutes les maturités de marché.\nEn appliquant le principe de bootstrap, les expressions énoncées en (1), (2), (3) , nous permettent d’obtenir de manière recursive les prix d’obligations zero coupons pour differentes maturités à partir des taux de marché.\nDes prix de ces obligations zéro-coupons, on déduit le taux zéro-coupon par le calcul suivant :\n$$\nB(0, T) = (-R(0,T) T) R(0,T) = -\n$$\n\n\n\nCode\ndef bootstrap_yield_curve(df):\n\n    \"\"\"\n\n    Bootstrap a yield curve from market data using cubic spline interpolation.\n\n\n\n    Parameters:\n\n        df (pd.DataFrame): Market data with columns [\"Market\", \"MAT\", \"MKT\"].\n\n\n\n    Returns:\n\n        pd.DataFrame: Yield curve with zero-coupon rates.\n\n    \"\"\"\n\n    # Convert DataFrame into dictionary for faster lookup\n\n    data = {market: df[df[\"Market\"] == market].reset_index(drop=True) for market in [\"MM\", \"FUT\", \"SWAP\"]}\n\n\n\n    # Step 1: Interpolate Swap Rates\n\n    swap_data = data[\"SWAP\"]\n\n    spline = CubicSpline(swap_data[\"MAT\"], swap_data[\"MKT\"])\n\n    maturities_annual = np.arange(3, 31, 1)\n\n    interpolated_rates = spline(maturities_annual)\n\n\n\n    # Combine interpolated data\n\n    SWAP_interpolated = pd.DataFrame({\"Market\": \"SWAP\", \"MAT\": maturities_annual, \"MKT\": interpolated_rates})\n\n    df_new = pd.concat([df[df[\"Market\"] != \"SWAP\"], SWAP_interpolated])\n\n\n\n    # Update data dictionary\n\n    data[\"MM\"] = df_new[df_new[\"Market\"] == \"MM\"].reset_index(drop=True)\n\n    data[\"FUT\"] = df_new[df_new[\"Market\"] == \"FUT\"].reset_index(drop=True)\n\n    data[\"SWAP\"] = df_new[df_new[\"Market\"] == \"SWAP\"].reset_index(drop=True)\n\n\n\n    # Step 2: Compute Zero-Coupon Rates for MM\n\n    data[\"MM\"][\"B(0,MAT)\"] = 1 / (1 + data[\"MM\"][\"MAT\"] * data[\"MM\"][\"MKT\"])\n\n    data[\"MM\"][\"tx ZC\"] = -np.log(data[\"MM\"][\"B(0,MAT)\"]) / data[\"MM\"][\"MAT\"]\n\n\n\n    # Step 3: Compute Zero-Coupon Rates for Futures\n\n    data[\"FUT\"][\"B(0,MAT)\"] = data[\"MM\"][\"B(0,MAT)\"].iloc[-1]\n\n    B_tau_i = data[\"FUT\"].iloc[0][\"B(0,MAT)\"]\n\n    tau_i = 1\n\n\n\n    for i, row in data[\"FUT\"].iterrows():\n\n        T_i, f_i = row[\"MAT\"], row[\"MKT\"]\n\n        B_tau_i /= (1 + (T_i - tau_i) * (1 - f_i))\n\n        data[\"FUT\"].at[i, \"B(0,MAT)\"] = B_tau_i\n\n        tau_i = T_i\n\n\n\n    data[\"FUT\"][\"tx ZC\"] = -np.log(data[\"FUT\"][\"B(0,MAT)\"]) / data[\"FUT\"][\"MAT\"]\n\n\n\n    # Step 4: Compute Zero-Coupon Rates for Swaps\n\n    B_T_1 = data[\"MM\"].loc[data[\"MM\"][\"MAT\"] == 1, \"B(0,MAT)\"].values[0]\n\n    B_T_2 = data[\"FUT\"].loc[data[\"FUT\"][\"MAT\"] == 2, \"B(0,MAT)\"].values[0]\n\n    A = B_T_1 + B_T_2\n\n\n\n    data[\"SWAP\"][\"B(0,MAT)\"] = (1 - data[\"SWAP\"][\"MKT\"].iloc[0] * A) / (1 + data[\"SWAP\"][\"MKT\"].iloc[0])\n\n\n\n    for i in range(1, len(data[\"SWAP\"])):\n\n        A += data[\"SWAP\"].iloc[i - 1][\"B(0,MAT)\"]\n\n        data[\"SWAP\"].at[i, \"B(0,MAT)\"] = (1 - data[\"SWAP\"].iloc[i][\"MKT\"] * A) / (1 + data[\"SWAP\"].iloc[i][\"MKT\"])\n\n\n\n    data[\"SWAP\"][\"tx ZC\"] = -np.log(data[\"SWAP\"][\"B(0,MAT)\"]) / data[\"SWAP\"][\"MAT\"]\n\n\n\n    # Combine results\n\n    yield_curve = pd.concat(data.values())\n\n\n\n    return yield_curve\n\n\n\nL’application de la méthode de Bootstrapping à la courbe de marché permet d’obtenir les taux zéro-coupons ci-après présentés.\n\n\nCode\n# Application du bootstraping\n\n\n\nyield_zc = bootstrap_yield_curve(df)\n\n\n\nyield_zc\n\n\n\n\n\n\n\n\n\nMarket\nMAT\nMKT\nB(0,MAT)\ntx ZC\n\n\n\n\n0\nMM\n0.25\n0.030698\n0.992384\n0.030581\n\n\n1\nMM\n0.50\n0.026191\n0.987074\n0.026021\n\n\n2\nMM\n0.75\n0.023958\n0.982349\n0.023745\n\n\n3\nMM\n1.00\n0.022979\n0.977537\n0.022719\n\n\n0\nFUT\n1.25\n0.978691\n0.972357\n0.022426\n\n\n1\nFUT\n1.50\n0.977094\n0.966821\n0.022495\n\n\n2\nFUT\n1.75\n0.974981\n0.960811\n0.022844\n\n\n3\nFUT\n2.00\n0.972911\n0.954348\n0.023363\n\n\n4\nFUT\n2.25\n0.970984\n0.947475\n0.023980\n\n\n5\nFUT\n2.50\n0.969711\n0.940355\n0.024599\n\n\n6\nFUT\n2.75\n0.968436\n0.932992\n0.025221\n\n\n0\nSWAP\n3.00\n0.026112\n0.925392\n0.025846\n\n\n1\nSWAP\n4.00\n0.028117\n0.894510\n0.027870\n\n\n2\nSWAP\n5.00\n0.029680\n0.863031\n0.029461\n\n\n3\nSWAP\n6.00\n0.031107\n0.830611\n0.030932\n\n\n4\nSWAP\n7.00\n0.032313\n0.798250\n0.032190\n\n\n5\nSWAP\n8.00\n0.033382\n0.766005\n0.033321\n\n\n6\nSWAP\n9.00\n0.034385\n0.733744\n0.034400\n\n\n7\nSWAP\n10.00\n0.035312\n0.701779\n0.035414\n\n\n8\nSWAP\n11.00\n0.036197\n0.670056\n0.036399\n\n\n9\nSWAP\n12.00\n0.037003\n0.639058\n0.037313\n\n\n10\nSWAP\n13.00\n0.037668\n0.609615\n0.038071\n\n\n11\nSWAP\n14.00\n0.038201\n0.581859\n0.038681\n\n\n12\nSWAP\n15.00\n0.038624\n0.555759\n0.039161\n\n\n13\nSWAP\n16.00\n0.038950\n0.531317\n0.039525\n\n\n14\nSWAP\n17.00\n0.039184\n0.508571\n0.039774\n\n\n15\nSWAP\n18.00\n0.039331\n0.487555\n0.039908\n\n\n16\nSWAP\n19.00\n0.039395\n0.468277\n0.039931\n\n\n17\nSWAP\n20.00\n0.039380\n0.450732\n0.039844\n\n\n18\nSWAP\n21.00\n0.039292\n0.434869\n0.039653\n\n\n19\nSWAP\n22.00\n0.039145\n0.420521\n0.039376\n\n\n20\nSWAP\n23.00\n0.038954\n0.407474\n0.039034\n\n\n21\nSWAP\n24.00\n0.038734\n0.395495\n0.038651\n\n\n22\nSWAP\n25.00\n0.038501\n0.384336\n0.038250\n\n\n23\nSWAP\n26.00\n0.038270\n0.373731\n0.037855\n\n\n24\nSWAP\n27.00\n0.038056\n0.363408\n0.037490\n\n\n25\nSWAP\n28.00\n0.037874\n0.353080\n0.037181\n\n\n26\nSWAP\n29.00\n0.037739\n0.342452\n0.036952\n\n\n27\nSWAP\n30.00\n0.037668\n0.331225\n0.036832\n\n\n\n\n\n\n\n\n\nCode\n# Fonction pour appliquer le stripping\n\n\n\ndef zc_interpolation(yield_curve):\n\n    \"\"\"\n\n    Calcule les taux forward 3M à partir des taux zéro-coupon.\n\n    \n\n    Parameters:\n\n    yield_curve (pd.DataFrame): Contient 'MAT' (maturité) et 'B(0,MAT)' (Prix ZC).\n\n    \n\n    Returns:\n\n    pd.DataFrame: Contenant 'MAT' et 'Forward 3M'.\n\n    \"\"\"\n\n    \n\n    # Convertir les maturités en mois\n\n    yield_curve[\"MAT_months\"] = yield_curve[\"MAT\"] * 12\n\n    \n\n    # Sélection des maturités tous les 3 mois\n\n    maturities = np.arange(3, max(yield_curve[\"MAT_months\"]), 3) / 12  # en années\n\n\n\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    # Interpolation log-linéaire sur B(0,T) pour estimer tx ZC pour ces maturités\n\n    temp = interp1d(yield_curve[\"MAT\"], np.log(yield_curve[\"B(0,MAT)\"]), kind=\"linear\", fill_value=\"extrapolate\")\n\n    log_interp_zc = -np.log(temp(maturities)) / maturities\n\n    \n\n    \n\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    # Interpolation linéaire sur les tx ZC \n\n    temp = interp1d(yield_curve[\"MAT\"], yield_curve[\"tx ZC\"], kind=\"linear\", fill_value=\"extrapolate\")\n\n    lin_interp_zc = temp(maturities)\n\n    \n\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    # Interpolation cubic des tx ZC pour estimer B(0,T) pour ces maturités\n\n    temp  = CubicSpline(yield_curve[\"MAT\"], yield_curve[\"tx ZC\"])\n\n    spline_interp_zc = temp(maturities)\n\n    \n\n    return {\"MAT\": maturities,\n\n                        \"Interpolation Linéaire\": lin_interp_zc,\n\n                        \"Interpolation log-linéaire\": log_interp_zc,\n\n                        \"Interpolation Spline\": spline_interp_zc}\n\n    \n\n\n\ndef plot_curve(forward_rates, columns = [\"Interpolation Spline\", \"Interpolation Linéaire\"], title= \"Courbe des Taux Forwards 3M\"):\n\n    \"\"\"\n\n    Trace la courbe des taux forwards en utilisant l'interpolation linéaire et spline.\n\n    \n\n    Parameters:\n\n    forward_rates (pd.DataFrame): Contenant 'MAT' et 'Forward 3M'.\n\n    \"\"\"\n\n    # Tracé des courbes\n\n    plt.figure(figsize=(10, 6))\n\n    for col in columns:\n\n        plt.plot(forward_rates[\"MAT\"], forward_rates[col],  label=col)\n\n    \n\n\n\n    plt.xlabel(\"Maturité (années)\")\n\n    plt.ylabel(\"Taux d'intérêt\")\n\n    plt.title(f\"{title}\")\n\n    plt.legend()\n\n    plt.grid()\n\n    plt.show()\n\n\n\nStripping\nNous utilisons trois méthodes pour réaliser l’interpolation de la\ncourbe discrète des taux zéro-coupon obtenue via le bootstrapping précédent.\n\nUne interpolation linéaire par morceaux (taux zéro-coupon):\nl’hypothèse est que la fonction \\(R(0,T)\\) est linéaire entre deux maturités successives \\(T_{i-1}\\) et $T_i $ :\n$$\nR(0,T) = _i(T) R(0,T_i) + (1 - i(T)) R(0,T{i-1}), T \n$$\navec :\n$$\n_i(T) = \n$$\nUne interpolation log-linéaire par morceaux (prix des obligations zéro-coupon):\non suppose que la fonction ( B(0,T) ) est log-linéaire par morceaux\n$$\n  R(0,T) \\cdot T = \\lambda_i(T) R(0,T_i) T_i + (1 - \\lambda_i(T)) R(0,T_{i-1}) T_{i-1}, \\quad \\text{pour } T \\in [T_{i-1}, T_i]\n$$\noù \\(\\lambda_i(T)\\) est défini comme précédemment.\nUne interpolation par spline cubique\n\nPour ces trois méthodes, la courbe de taux interpolée a le même allure comme illustrée ci-après.\n\n\nCode\n# Application du stripping\n\ncourbe_zc = pd.DataFrame(zc_interpolation(yield_zc))\n\n\n\n# Tracé des courbes interpolées\n\nplot_curve(courbe_zc,\n\n           columns = [\"Interpolation Spline\", \"Interpolation Linéaire\",\"Interpolation log-linéaire\"],\n\n           title= \"Courbe de taux Zero-coupon\")\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\679077462.py:45: RuntimeWarning: invalid value encountered in log\n  log_interp_zc = -np.log(temp(maturities)) / maturities\n\n\n\n\n\n\n\n\n\n\nLe mode d’interpolation des taux zéro-coupon a un impact direct sur le calcul des taux de marché, car il conditionne la forme et la régularité de la courbe des taux. En particulier, les méthodes d’interpolation linéaires et log-linéaires génèrent de fortes irrégularités sur la courbe des taux forward implicite.\nCeci peut s’expliquer par le fait que l’information de marché est très dispersée sur la partie swap. Ceci étant principalement dû à l’espacement important entre les maturités des taux swap consécutifs observés, qui s’étend sur une année dans notre cas. Ainsi, ces deux méthodes d’interpolation standards sont\nsouvent inadaptées en terme de lissage des courbes implicites."
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#construction-de-la-courbe-des-taux-forward",
    "href": "finance_quant/projets/projet_courbe_taux.html#construction-de-la-courbe-des-taux-forward",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Construction de la courbe des taux forward",
    "text": "Construction de la courbe des taux forward\n\n\nÀ partir de la courbe des taux zéro-coupon obtenue par bootstrapping, traçons la courbe des taux forwards 3Mois en fonction de la maturité en utilisant les differentes méthodes d’interpolation présentées précédemment.\n\nLe graphique ci après présente les courbes de taux forward 3 mois obtenue en fonction des différentes méthodes d’interpolation.\n\n\nCode\n# Courbe des taux forward de tenor 3 mois\n\n\n\ndef compute_forward_rates(yield_curve):\n\n    \"\"\"\n\n    Calcule les taux forward 3M à partir des taux zéro-coupon.\n\n    \n\n    Parameters:\n\n    yield_curve (pd.DataFrame): Contient 'MAT' (maturité) et 'B(0,MAT)' (Prix ZC).\n\n    \n\n    Returns:\n\n    pd.DataFrame: Contenant 'MAT' et 'Forward 3M'.\n\n    \"\"\"\n\n    \n\n    # Convertir les maturités en mois\n\n    yield_curve[\"MAT_months\"] = yield_curve[\"MAT\"] * 12\n\n    \n\n    # Sélection des maturités tous les 3 mois\n\n    maturities = np.arange(3, max(yield_curve[\"MAT_months\"]), 3) / 12  # en années\n\n\n\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    # Interpolation log-linéaire sur B(0,T) pour estimer B(0,T) pour ces maturités\n\n    log_interp_bond_prices = interp1d(yield_curve[\"MAT\"], np.log(yield_curve[\"B(0,MAT)\"]), kind=\"linear\", fill_value=\"extrapolate\")\n\n    \n\n    # Calcul des prix interpolés\n\n    B_t = np.exp(log_interp_bond_prices(maturities))\n\n    B_t_3M = np.exp(log_interp_bond_prices(maturities + 0.25))  # 3 mois plus tard\n\n\n\n    # Calcul du taux forward 3M\n\n    forward_rates_log = ((B_t / B_t_3M) - 1)/0.25\n\n    \n\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    # Interpolation linéaire sur les tx ZC pour estimer B(0,T) pour ces maturités\n\n    lin_interp_bond_prices = interp1d(yield_curve[\"MAT\"], yield_curve[\"tx ZC\"], kind=\"linear\", fill_value=\"extrapolate\")\n\n    \n\n    # Calcul des prix interpolés\n\n    B_t = np.exp(-lin_interp_bond_prices(maturities)*maturities)\n\n    B_t_3M = np.exp(-lin_interp_bond_prices(maturities + 0.25)*(maturities + 0.25))  # 3 mois plus tard\n\n\n\n    # Calcul du taux forward 3M\n\n    forward_rates_lin = ((B_t / B_t_3M) - 1)/0.25\n\n    \n\n    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    # Interpolation cubic des tx ZC pour estimer B(0,T) pour ces maturités\n\n    spline_interp_bond_prices = CubicSpline(yield_curve[\"MAT\"], yield_curve[\"tx ZC\"])\n\n    \n\n    # Calcul des prix interpolés\n\n    B_t = np.exp(-spline_interp_bond_prices(maturities)*maturities)\n\n    B_t_3M = np.exp(-spline_interp_bond_prices(maturities + 0.25)*(maturities + 0.25))  # 3 mois plus tard\n\n\n\n    # Calcul du taux forward 3M\n\n    forward_rates_spline = ((B_t / B_t_3M) - 1)/0.25\n\n    \n\n    return {\"MAT\": maturities,\n\n                        \"Interpolation Linéaire\": forward_rates_lin,\n\n                        \"Interpolation log-linéaire\": forward_rates_log,\n\n                        \"Interpolation Spline\": forward_rates_spline}\n\n\n\n\nCode\n# Calcul de la courbe des taux forwards\n\nforward_rates = pd.DataFrame(compute_forward_rates(yield_zc))\n\n\n\n# Tracé des courbes interpolées\n\nplot_curve(forward_rates, columns = [\"Interpolation Spline\", \"Interpolation Linéaire\",\"Interpolation log-linéaire\"])\n\n\n\n\n\n\n\n\n\n\nOn constate des irrégularités pour les interpolations linéaires et log-linéaires, contrairement à la courbe obtenue via une interpolation spline qui est plus lisse.\nLes irrégularités observées pour les interpolations linéaires et log-linéaires sont observés à partir de la maturité 3 ans. Ceci est cohérent avec l’analyse faite plus haut. En effet, à partir de cette maturité, les taux zero coupons ont été extraits à partir de taux swap de marché.\n\nShiftons le taux de swap 5Y de 10 bps, puis expliquons l’impact sur les courbes des taux zéro-coupon et forward.\n\n\n\nCode\n# 1bp = 0.01% donc 10bps = 0.10%=0.0001\n\ndf_bumped = yield_zc.copy()\n\ndf_bumped.loc[df_bumped[\"MAT\"]==5,\"MKT\"] = df_bumped.loc[df_bumped[\"MAT\"]==5,\"MKT\"] + 0.001\n\nyield_bumped = bootstrap_yield_curve(df_bumped) # On refait le bootstraping\n\n\n\n# Application du stripping\n\ncourbe_zc_bump = pd.DataFrame(zc_interpolation(yield_bumped))\n\n\n\n# Tracé des courbes interpolées\n\nplot_curve(courbe_zc_bump,\n\n           columns = [\"Interpolation Spline\", \"Interpolation Linéaire\",\"Interpolation log-linéaire\"],\n\n           title= \"Courbe de taux Zero-coupon shiftée\")\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\679077462.py:45: RuntimeWarning: invalid value encountered in log\n  log_interp_zc = -np.log(temp(maturities)) / maturities\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calcul de la courbe des taux forwards\n\nforward_rates_bumped = pd.DataFrame(compute_forward_rates(yield_bumped))\n\nforward_rates_bumped.rename(columns={\"Interpolation Linéaire\": \"Interpolation Linéaire shift\",\n\n                                    \"Interpolation log-linéaire\": \"Interpolation log-linéaire shift\",\n\n                                    \"Interpolation Spline\": \"Interpolation Spline shift\"}, inplace=True)\n\n\n\nforward_rates_bumped = forward_rates_bumped.merge(forward_rates, on=\"MAT\", how=\"inner\")\n\n\n\n\n\n# Tracé des courbes interpolées\n\nplot_curve(forward_rates_bumped,\n\n           columns=[\"Interpolation Spline\", \"Interpolation Spline shift\"],\n\n           title=\"Courbe des Taux Forwards 3M shiftée\")  # On passe l'axe correspondant\n\n\n\nplot_curve(forward_rates_bumped,\n\n           columns=[\"Interpolation Linéaire\", \"Interpolation Linéaire shift\"],\n\n           title=\"Courbe des Taux Forwards 3M shiftée\")  \n\n\n\nplot_curve(forward_rates_bumped,\n\n           columns=[\"Interpolation log-linéaire\", \"Interpolation log-linéaire shift\"],\n\n           title=\"Courbe des Taux Forwards 3M shiftée\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn observe que le shift de 10 bps sur le taux swap 5Y entraîne une modification des courbes de taux forward 3 mois, avec des effets qui varient selon la méthode d’interpolation utilisée.\nCette différence de comportement découle des propriétés des méthodes d’interpolation utilisées. L’interpolation linéaire et log-linéaire segmente l’évolution des taux entre deux maturités successives \\(T_{i-1}\\) et $ T_i$ , en supposant une variation linéaire des taux zero coupons sur chaque intervalle. Ainsi, lorsqu’un bump est appliqué, son impact reste localisé, et les taux forward reviennent rapidement à leur trajectoire initiale.\nÀ l’inverse, l’interpolation spline assure une transition plus fluide en imposant des contraintes de régularité sur la dérivée seconde. Cette régularité propage l’effet du bump de manière plus progressive, entraînant une déviation plus large et plus persistante des taux forward."
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#du-modèle-hjm-vers-le-modèle-hullwhite",
    "href": "finance_quant/projets/projet_courbe_taux.html#du-modèle-hjm-vers-le-modèle-hullwhite",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Du modèle HJM vers le modèle Hull&White",
    "text": "Du modèle HJM vers le modèle Hull&White\n\nOn souhaite retrouver la dynamique du taux forward instantané. Le prix du bond zéro coupon évoluant sous la probabilité risque neutre selon la dynamique:\n\\[\\frac{dB(t,T)}{B(t,T)} = r_t dt + \\Gamma(t,T)dW_t^Q\\]\nqui est une EDS géométrique, on vérifie par le lemme d’Îto que la solution à cette EDS est\n$$\nB(t,T) = B(0,T) (_0^tr_udu - _0^t ^2(u,T)du + _0^t (u,T)dW_u^Q)\n$$ .\nEn appliquant le logarithme à cette égalité, on obtient\n$$\n(B(t,T)) = (B(0,T)) + _0^tr_udu - _0^t ^2(u,T)du + _0^t (u,T)dW_u^Q\n$$\net\n\\[\\begin{align*}\n\n    f(t,T) &= -\\partial_T \\ln(B(t,T))\\\\\n\n           &= -\\partial_T \\ln(B(0,T)) +\\frac{1}{2} \\int_0^t 2\\Gamma(u,T)\\partial_T\\Gamma(u,T)du - \\int_0^t \\partial_T\\Gamma(u,T)dW_u^Q\n\n\\end{align*}\\]\nPar suite\n\\[\\begin{align*}\n\n    df(t,T) &=   \\Gamma(t,T)\\partial_T\\Gamma(t,T) -\\partial_T\\Gamma(t,T)dW_t^Q\\\\\n\n            &= \\gamma(t,T) \\int_t^T \\gamma(t,u)du + \\gamma(t,T)dW_t^Q\n\n\\end{align*}\\]\noù \\[\\gamma(t,T) = -\\partial_T\\Gamma(t,T).\\]\nLe lemme dÎto permet alors de retrouver\n$$\nf(t,T) = f(o,T) + _0^t (s,T) _s^T (s,u)duds + _0^t (s,T)dW_s^Q\n$$."
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#hypothèses-du-modèle-hullwhite",
    "href": "finance_quant/projets/projet_courbe_taux.html#hypothèses-du-modèle-hullwhite",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Hypothèses du modèle Hull&White",
    "text": "Hypothèses du modèle Hull&White\n\nOn suppose que le modèle HJM est gaussien, linéaire et calibrable.Ces trois hypothèses nous permettent d’écrire:\n$$\n\\begin{cases}\n\n    \\gamma(t,T) = \\sigma(t)e^{-\\lambda(T-t)} \\\\\n\n    \\Gamma(t,T) = \\frac{\\sigma(t)}{\\lambda}(e^{-\\lambda(T-t)}-1)\n\n\\end{cases}\n$$\noù la fonction de volatilité instantanée \\(\\sigma(t)\\) est constante par morceaux."
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#construction-de-la-formule-zéro-coupon",
    "href": "finance_quant/projets/projet_courbe_taux.html#construction-de-la-formule-zéro-coupon",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Construction de la formule zéro-coupon",
    "text": "Construction de la formule zéro-coupon\n\nOn considère le modèle de Hull&White\n$$\ndX_t = ((t)-X_t) + (t)dW_t^Q.\n$$\n\nLe modèle de Hull-White fait partie de la famille des EDS à coefficients affines.\nOn veut déterminer la loi du processus \\(X_t|X_s\\). On commence par résoudre l’EDS à coefficients affines\n$$\ndX_t = ((t) - X_t)dt + (t) dW_t^Q\n$$.\nPosons \\(Y_t = X_t e^{\\lambda t}\\). Alors,\n\\[\\begin{align*}\n\n    dY_t &= \\lambda e^{\\lambda t} X_t dt + e^{\\lambda t}dX_t\\\\\n\n            &= \\lambda e^{\\lambda t} X_t dt + e^{\\lambda t} \\left(\n\n(\\phi(t) - \\lambda X_t)dt + \\sigma(t) dW_t^Q \\right)\\\\\n\n            &= \\lambda e^{\\lambda t} X_t dt + e^{\\lambda t} \\phi(t)dt - e^{\\lambda t} \\lambda X_tdt +  e^{\\lambda t} \\sigma(t) dW_t^Q\\\\\n\n            &=  e^{\\lambda t} \\phi(t)dt +  e^{\\lambda t} \\sigma(t) dW_t^Q\\\\\n\n\\end{align*}\\]\nAinsi, par le lemme d’Îto,\n\\[\\begin{align*}\n\n    Y_t &= Y_0 + \\int_0^t e^{\\lambda u} \\phi(u)du +  \\int_0^t e^{\\lambda u} \\sigma(u) dW_u^Q\\\\\n\n    X_t e^{\\lambda t} &=  Y_0 +  \\int_0^t e^{\\lambda u} \\phi(u)du +  \\int_0^t e^{\\lambda u} \\sigma(u) dW_u^Q\\\\\n\n    X_t  &= X_0 + e^{-\\lambda t} \\int_0^t e^{\\lambda u} \\phi(u)du + e^{-\\lambda t} \\int_0^t e^{\\lambda u} \\sigma(u) dW_u^Q\n\n\\end{align*}\\]\nPosons\n$$\nM_t = _0^t e^{u} (u) dW_u^Q,\n$$\npour tout \\(t \\geq 0\\).\nLe processus \\((M_t)_{t \\geq 0}\\) est une martingale locale comme intégralle stochastique d’une martingale locale et\n$$\nE([M]_t) = _0^t e^{2u} ^2(u)du &lt; +,\n$$\ndonc \\(M\\) est une vraie martingale.\nFinalement, \\(X_t\\) suit une distribution gaussienne, pour tout \\(t \\geq 0\\).\nOn en déduit que \\(X_t|X_s\\) suit une loi gaussienne d’espérance \\(E(X_t |X_s)\\) et de variance \\(V(X_t |X_s)\\).\nSoit \\(t&gt;s \\geq 0\\),\n\\[\\begin{align*}\n\n    X_t e^{\\lambda t} &=  X_0 +  \\int_0^s e^{\\lambda u} \\phi(u)du +  \\int_0^s e^{\\lambda u} \\sigma(u) dW_u^Q + \\int_s^t e^{\\lambda u} \\phi(u)du + \\notag \\\\\n\n  &\\quad \\int_s^t e^{\\lambda u} \\sigma(u) dW_u^Q\\\\\n\n    X_t e^{\\lambda (t-s)} &=  e^{-\\lambda s} \\left(X_0 +  \\int_0^s e^{\\lambda u} \\phi(u)du +  \\int_0^s e^{\\lambda u} \\sigma(u) dW_u^Q \\right)+ \\notag \\\\\n\n  &\\quad e^{-\\lambda s} \\left(\\int_s^t e^{\\lambda u} \\phi(u)du +  \\int_s^t e^{\\lambda u} \\sigma(u) dW_u^Q\\right)\\\\\n\n    X_t e^{\\lambda (t-s)} &=  X_s + e^{-\\lambda s} \\left(\\int_s^t e^{\\lambda u} \\phi(u)du +  \\int_s^t e^{\\lambda u} \\sigma(u) dW_u^Q\\right)\\\\\n\n    X_t  &=  X_se^{-\\lambda (t-s)} + e^{-\\lambda t} \\int_s^t e^{\\lambda u} \\phi(u)du + e^{-\\lambda t} (M_t-M_s)\\\\\n\n\\end{align*}\\]\nIl vient que\n\\[\\begin{align*}\n\n    E(X_t|X_s)  &=  E(X_se^{-\\lambda (t-s)} + e^{-\\lambda t} \\int_s^t e^{\\lambda u} \\phi(u)du + e^{-\\lambda t} (M_t-M_s)|\\mathcal{F}_s)\\\\\n\n    &=  X_se^{-\\lambda (t-s)} + e^{-\\lambda t} \\int_s^t e^{\\lambda u} \\phi(u)du + e^{-\\lambda t} E(M_t-M_s |\\mathcal{F}_s)\\\\\n\n    &= X_se^{-\\lambda (t-s)} + e^{-\\lambda t} \\int_s^t e^{\\lambda u} \\phi(u)du \\text{,  car $M$ est une martingale}\\\\\n\n\\end{align*}\\]\n\\[\\begin{align*}\n\n    V(X_t|X_s) &= V(X_se^{-\\lambda (t-s)} + e^{-\\lambda t} \\int_s^t e^{\\lambda u} \\phi(u)du + e^{-\\lambda t} (M_t-M_s)|\\mathcal{F}_s)\\\\\n\n                &= e^{-2\\lambda t}V(M_t-M_s|\\mathcal{F}_s)\\\\\n\n                &= e^{-2\\lambda t}E((M_t-M_s)^2|\\mathcal{F}_s)\\\\\n\n                &= e^{-2\\lambda t}E((M_t-M_s)^2) \\text{,  car $M$  est a accroissements indépendants}\\\\\n\n                &= e^{-2\\lambda t}\\int_s^t e^{\\lambda 2u}\\sigma^2(u) du\\\\\n\n                &= \\int_s^t e^{-\\lambda 2(t-u)}\\sigma^2(u) du\\\\\n\n\\end{align*}\\]\nFinalement,\n$$\nX_t|X_s ( X_se^{-(t-s)} + e^{-t} _s^t e^{u} (u)du,_s^t e{-(t-u)}2(u) du)\n$$"
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#dynamique-des-taux-forwards",
    "href": "finance_quant/projets/projet_courbe_taux.html#dynamique-des-taux-forwards",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Dynamique des taux forwards",
    "text": "Dynamique des taux forwards\n\n\nOn a\n$$\nZ_t = ,\n$$\navec\n$$\n = r_t dt + (t,T)dW_t^Q.\n$$\n\\[\\begin{align*}\n\n     \\frac{dZ_t}{Z_t} &= \\frac{dB(t,T_{i})}{B(t,T_{i})} - \\frac{dB(t,T_{i+1})}{B(t,T_{i+1})} - \\left&lt; \\frac{dB(t,T_{i})}{B(t,T_{i})} - \\frac{dB(t,T_{i+1})}{B(t,T_{i+1})} | \\frac{dB(t,T_{i+1})}{B(t,T_{i+1})}\\right&gt;\\\\\n\n     &= r_t dt + \\Gamma(t,T_{i})dW_t^Q - r_t dt - \\Gamma(t,T_{i+1})dW_t^Q - \\notag \\\\\n\n   &\\quad \\left&lt; r_t dt + \\Gamma(t,T_{i})dW_t^Q - r_t dt - \\Gamma(t,T_{i+1})dW_t^Q | r_t dt + \\Gamma(t,T_{i+1})dW_t^Q\\right&gt;\\\\\n\n     &= \\Gamma(t,T_{i})dW_t^Q - \\Gamma(t,T_{i+1})dW_t^Q - \\notag \\\\\n\n   &\\quad \\left&lt; \\Gamma(t,T_{i})dW_t^Q - \\Gamma(t,T_{i+1})dW_t^Q | r_t dt + \\Gamma(t,T_{i+1})dW_t^Q\\right&gt;\\\\\n\n     &= \\Gamma(t,T_{i})dW_t^Q - \\Gamma(t,T_{i+1})dW_t^Q -\\Gamma(t,T_{i}) \\Gamma(t,T_{i+1}) d&lt;W^Q&gt;_t + \\notag \\\\\n\n   &\\quad \\Gamma^2(t,T_{i+1}) d&lt;W^Q&gt;_t\\\\\n\n     &= (\\Gamma(t,T_{i})- \\Gamma(t,T_{i+1}))(dW_t^Q - \\Gamma(t,T_{i+1}) dt) \\text{,  car $d&lt;W^Q&gt;_t = dt$}\\\\\n\n     &= (\\Gamma(t,T_{i})- \\Gamma(t,T_{i+1}))d\\tilde{W}^Q_t \\text{,   avec $d\\tilde{W}^Q_t =dW_t^Q - \\Gamma(t,T_{i+1}) dt$}\\\\\n\n\\end{align*}\\]\nD’où\n$\nZ_t = Z_0 .\n$\nEn notant \\(L_i(t)\\) le taux libor forward à la date \\(t\\) qui fixe en \\(T_i\\) et qui paye en \\(T_{i+1}\\), on a\n$$\nL_i(t) = ( - 1) = (Z_t - 1).\n$$\nOn en déduit que\n$\nL_i(t) = - + (1 + _i L_i(0)) \n$\n$\n\n$\nEn appliquant le lemme d’Îto à cette dernière égalité on obtient la dyanamique du taux libor forward donnée par:\n$$\ndL_i(t) = (L_i(t) + )((t,T_{i})- (t,T_{i+1}))d^Q_t.\n$$"
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#valorisation-des-instruments-de-calibration",
    "href": "finance_quant/projets/projet_courbe_taux.html#valorisation-des-instruments-de-calibration",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Valorisation des instruments de calibration",
    "text": "Valorisation des instruments de calibration\n\n\nLe payoff d’un caplet vanille sur taux libor \\(L_i(T_i)\\), de maturité \\(T_i\\), de date de paiement \\(T_{i+1}\\) et de strike \\(K\\) s’écrit\n$$\nPayoff(T_i, T_{i+1}, K) = _i(L_i(T_i) - K)^+.\n$$\nLa formule de valorisation de ce caplet dans le cadre du modèle d’Hull and White\n\\[\\begin{align*}\n\n     B(0,T_{i+1})E(Payoff(T_i, T_{i+1}, K)) = B(0,T_{i+1})E(\\delta_i(L_i(T_i) - K)^+)\n\n\\end{align*}\\]\nor\n\\[\\begin{align*}\n\n     Payoff(T_i, T_{i+1}, K) &= \\delta_i(L_i(T_i) - K)^+\\\\\n\n     &= \\delta_i(\\frac{1}{\\delta_i} (Z_{T_i} - 1) - K)^+ \\\\\n\n     &= (Z_{T_i} - (\\delta_iK + 1))^+ \\\\\n\n     &= (Z_{T_i} - \\tilde{K})^+, \\text{  avec $\\tilde{K} = \\delta_iK + 1$}\n\n\\end{align*}\\]\nPuisque la dynamique du processus \\(Z\\) est décrite par le modèle de Black, on déduit de la remaque ci-dessus que le prix d’un caplet vanille sur taux libor \\(L_i(T_i)\\), de maturité \\(T_i\\), de date de paiement \\(T_{i+1}\\) et de strike \\(K\\) peut s’écrire sous la forme\n\\[\\begin{align*}\n\n     Caplet(T_i, T_{i+1}, K) &= C(Z_{T_i},\\tilde{K}, T_i, \\sigma^*_i, B(0,T_{i+1}))\n\n\\end{align*}\\]\n$$\n^*i = t^{T_i} ((u,T{i})- (u,T{i+1}))du\n\n$$"
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#calibration-du-modèle",
    "href": "finance_quant/projets/projet_courbe_taux.html#calibration-du-modèle",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Calibration du modèle",
    "text": "Calibration du modèle\n\n\nOn considère un caplet sur l’Euribor 12M de maturité \\(5\\) ans, avec un strike at-the-money. Nous cherchons la volatilité implicite \\(\\sigma\\) telle que le prix modèle (Hull&White) soit égal au prix observé sur le marché.\nPour ce faire, nous avons utilisé une méthode de type dichotomie pour estimer la volatilité implicite associée au processus \\(Z_t\\), et avons obtenu une valeur de \\(\\mathbf{0.9271} \\%.\\)\nDans un second temps, nous en avons déduit analytiquement le paramètre de volatilité instantanée du modèle, estimé à \\(\\mathbf{1.071}\\%\\).\nNous en déduisons les prix “modèles” des caplets, obtenus à partir de la volatilité calibrée précédemment, pour les strikes suivants : FWD \\(\\pm\\) 25 bps, 50 bps et 100 bps.\n\n\n\nCode\n# Volatilité instantanée\n\n\n\ndef sigma(T_i_1, T_i, lambd, vol):\n\n    beta = (1-np.exp(-lambd*(T_i - T_i_1)))/lambd\n\n    sigma = np.sqrt((2*lambd*T_i_1)/(1-np.exp(-2*lambd*T_i_1))) * vol / beta\n\n    return sigma\n\n\n\n# Pricer du caplet HW\n\n\n\ndef P_HW(t,Z, K, T_i_1, T_i, B, vol):\n\n    delta = T_i - T_i_1\n\n    d1 = (np.log(Z/K) + 0.5 * (vol**2)*(T_i_1 - t))/(vol * np.sqrt(T_i_1 - t))\n\n    d2 = d1 - vol*np.sqrt(T_i_1 - t)\n\n    caplet = delta * B[1] * (Z * norm.cdf(d1) - K * norm.cdf(d2))\n\n    return caplet\n\n\n\n\nCode\n# Initialisation des paramètres\n\nt = 0\n\nT_i_1 = 5\n\nT_i = 5 + 1\n\nlambd = 0.05\n\n\n\nP_MKT = 0.7137 / 100 # Prix du marché\n\nB = np.array([yield_zc[yield_zc.MAT==T_i_1][\"B(0,MAT)\"],\n\nyield_zc[yield_zc.MAT==T_i][\"B(0,MAT)\"] ])\n\nZ = B[0] / B[1]\n\nK = (Z-1)/(T_i-T_i_1)\n\nK_tilde = 1 + (T_i-T_i_1)*K\n\nL_0 = (Z-1)/(T_i-T_i_1) # Taux libor\n\n\n\n\nCode\n# Recherche de la vol implicite par dichotomie\n\n\n\ndef dichot(t,Z, K_tilde, T_i_1, T_i, B, P_MKT):\n\n    sig_inf = 1e-8\n\n    sig_sup = 1e1\n\n    epsi = 1e-8\n\n    sig_moy = (sig_inf + sig_sup)/2\n\n    error = sig_sup - sig_inf\n\n\n\n    while error&gt;epsi:\n\n        p_hw = P_HW(t,Z, K_tilde, T_i_1, T_i, B, vol=sig_moy)\n\n        if p_hw &gt; P_MKT:\n\n            sig_sup = sig_moy\n\n        elif p_hw &lt; P_MKT:\n\n            sig_inf = sig_moy\n\n        sig_moy = (sig_inf + sig_sup)/2\n\n        error = np.abs(sig_sup - sig_inf)\n\n\n\n    return sig_moy\n\n\n\n\nCode\n# Vol implicte optimale\n\nvol_imp = dichot(t,Z, K_tilde, T_i_1, T_i, B, P_MKT)\n\nvol_imp\n\n\n0.009270474060870853\n\n\n\n\nCode\n# Vol instantanée\n\nvol_inst = sigma(T_i_1, T_i, lambd, vol=vol_imp)\n\nvol_inst\n\n\nnp.float64(0.01071380211500768)\n\n\n\n\nCode\n# Vérification dui matching du prix du marché par le pricer\n\nP_HW(t,Z, K_tilde, T_i_1, T_i, B, vol_imp)\n\n\narray([0.007137])\n\n\n\n\nCode\nstrikes = np.array([-100, -50, -25, 0, 25, 50, 100])/10_000\n\ncaplets = np.array([P_HW(t,Z, 1 + (T_i-T_i_1)*(K+strike), T_i_1, T_i, B, vol_imp) for strike in strikes])\n\n\n\n# Préparation des données pour le tableau\n\ntable_data = []\n\nfor i in range(len(caplets)):\n\n    table_data.append([strikes[i]*10_000, np.round(caplets[i]*100,2)])\n\n\n\n# Affichage des résultats dans un tableau\n\nheaders = [\"Strike (bps)\", \"Prix Modèle du Caplet (%)\"]\n\nprint(tabulate(table_data, headers=headers, tablefmt=\"pretty\", floatfmt=\".4f\"))\n\n\n+--------------+---------------------------+\n| Strike (bps) | Prix Modèle du Caplet (%) |\n+--------------+---------------------------+\n|    -100.0    |           [1.2]           |\n|    -50.0     |          [0.94]           |\n|    -25.0     |          [0.82]           |\n|     0.0      |          [0.71]           |\n|     25.0     |          [0.62]           |\n|     50.0     |          [0.53]           |\n|    100.0     |          [0.38]           |\n+--------------+---------------------------+\n\n\n\n\nCode\nplt.plot(strikes*10_000, caplets*100)\n\n\n\nplt.xlabel(\"Strikes (bps)\")\n\nplt.ylabel(\"Prix modèle (%)\")\n\nplt.title(\"Prix modèle en fonction des strikes\")\n\nplt.legend()\n\nplt.grid()\n\nplt.show()\n\n\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2575563213.py:11: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  plt.legend()\n\n\n\n\n\n\n\n\n\n\n\nInversons ces prix modèle à l’aide de la fonction de Black pour calculer les volatilités implicites.\n\nLes volatilités implicites retrouvés pour ces prix modèles sont resumés dans le tableau ci après:\n\n\nCode\n# Inversion des prix pour retrouver la vol implicite\n\n\n\nL_i = (Z-1)/(T_i-T_i_1)\n\nvols = np.array([dichot(t,L_i, strike + K, T_i_1, T_i, B, caplet) for strike,caplet in zip(strikes,caplets)])\n\n#vols_inst = np.array([sigma(T_i_1, T_i, lambd, vol=vol) for vol in vols])\n\n\n\n\n\n\n\n# Préparation des données pour le tableau\n\ntable_data = []\n\nfor i in range(len(caplets)):\n\n    table_data.append([strikes[i]*10_000, np.round(vols[i]*100,2), np.round(caplets[i]*100,2)])\n\n\n\n# Affichage des résultats dans un tableau\n\nheaders = [\"Strike (bps)\", \"Vol implicite (%)\", \"Prix Modèle du Caplet (%)\"]\n\nprint(tabulate(table_data, headers=headers, tablefmt=\"pretty\", floatfmt=\".4f\"))\n\n\n+--------------+-------------------+---------------------------+\n| Strike (bps) | Vol implicite (%) | Prix Modèle du Caplet (%) |\n+--------------+-------------------+---------------------------+\n|    -100.0    |       28.87       |           [1.2]           |\n|    -50.0     |       26.74       |          [0.94]           |\n|    -25.0     |       25.83       |          [0.82]           |\n|     0.0      |       25.0        |          [0.71]           |\n|     25.0     |       24.24       |          [0.62]           |\n|     50.0     |       23.55       |          [0.53]           |\n|    100.0     |       22.3        |          [0.38]           |\n+--------------+-------------------+---------------------------+\n\n\n\nComparons le smile modèle avec le smile de marché\n\n\nCode\nvol_mkt = np.array([31.2, 28.4, 26.6, 25.0, 24.4, 25.0, 27.2])\n\n\n\nplt.plot(strikes*10_000, vols*100, label=\"Vol implicite\", marker='o')\n\nplt.plot(strikes*10_000, vol_mkt, label=\"Vol de marché\",  marker='o')\n\n\n\nplt.xlabel(\"Strikes (bps)\")\n\nplt.ylabel(\"Volatilité (%)\")\n\nplt.title(\"Volatilités en fonction des strikes\")\n\nplt.legend()\n\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOn observe pour ce modèle un Skew-Smile. Le modèle de Hull & white n’arrive pas à reproduire le smile de marché. ."
  },
  {
    "objectID": "finance_quant/projets/projet_courbe_taux.html#valorisation-dun-produit-structuré",
    "href": "finance_quant/projets/projet_courbe_taux.html#valorisation-dun-produit-structuré",
    "title": "Gestion des risques multiples: Théorie des copules",
    "section": "Valorisation d’un produit structuré",
    "text": "Valorisation d’un produit structuré\nNous souhaitons valoriser un caplet\n\nde strike K,\nde dates de fixing \\(T_i\\) et de paiement \\(T_{i+1}\\),\navec une barrière désactivante \\(B\\) telle que \\(B &gt; K\\).\n\n\nÉcrivons le payoff de l’option décrite et traçons la fonction de payoff en fonction de \\(L_i(T_i)\\).\nLe payoff l’option s’écrit :\n$$\n{T{i+1}} = i (L_i(T_i) - K)^+ {{ L_i(T_i) &lt; B }}\n$$\navec \\(\\delta_i = T_{i+1}-T_i\\).\nIl correspond à celui d’un caplet classique, mais est nul lorsque le taux Libor au fixing dépasse la barrière.\nAinsi, fonction de payoff est donc nulle pour \\(L_i(T_i) &lt; K\\) ou \\(L_i(T_i) \\geq B\\), et croît linéairement entre \\(K\\) et $ B$.\nUne representation de la fonction de payoff est donnée la figure ci-dessous.\n\n\n\nCode\n#3.7. \n\n#1. Payoff et courbe\n\n# Définition des paramètres\n\ndelta = 1\n\nZ_T = 1.05  # Exemple de valeur pour Z_T\n\nB = 0.4  # Barrière (B)\n\n\n\n# Définition des valeurs possibles pour Li(Ti)\n\nLi_Ti = np.linspace(0.001, 0.5, 500)\n\n\n\n# Calcul du payoff du caplet désactivant\n\npayoff = np.maximum(0, Li_Ti - K) * (Li_Ti &lt; B)\n\n\n\n# Tracé de la fonction de payoff\n\nplt.figure(figsize=(8, 6))\n\nplt.plot(Li_Ti, payoff, label='Payoff du Caplet Désactivant', color='b')\n\nplt.axvline(x=K, color='r', linestyle='--', label='Strike (K)')\n\nplt.axvline(x=B, color='g', linestyle='--', label='Barrière (B)')\n\nplt.title('Fonction de Payoff d\\'un Caplet Désactivant')\n\nplt.xlabel('Li(Ti)')\n\nplt.ylabel('Payoff')\n\nplt.legend()\n\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCette option est moins chère qu’un caplet classique de strike \\(K\\) car elle comporte une barrière désactivante qui limite le risque pour le vendeur. En effet, dès que le taux dépasse $B $, le caplet est désactivé, ce qui protège le vendeur contre les scénarios de forte hausse de taux.\nDu point de vue de l’acheteur, cette protection a un coût : l’option devient moins attractive, car elle ne couvre plus les situations les plus défavorables. La probabilité de recevoir un payoff est donc réduite, ce qui justifie un prix plus bas par rapport à un caplet standard.\n\nLe payoff de cet option est semblable à celui de:\n\nl’achat d’un caplet de stkire K: \\((L_i(T_i) - K)^+\\) ,\nde la vente d’un caplet de strike B: \\(-(L_i(T_i) - B)^+\\),\net d’un paiement fixe déclenché par le franchissement de la barrière: \\(-(B-K)\\mathbb{1}_{\\{ L_i(T_i) &gt; B \\}}\\)\n\n\nLe payoff peut alors se réécrire comme suit :\n$$\n\\text{Payoff}_{T_{i+1}} = \\delta_i \\cdot( (L_i(T_i) - K)^+ -(L_i(T_i) - B)^+ -(B-K)\\mathbb{1}_{\\{ L_i(T_i) &gt; B \\}})\n\n$$\nLa prise en compte du smile de volatilité est indispensable pour deux raisons principales. Premièrement, elle permet de modéliser les variations du sous-jacent (ici, le taux Libor), qui influencent directement la valeur de l’option. Deuxièmement, elle est cruciale pour évaluer l’effet de la barrière : une volatilité élevée accroît la probabilité que le taux Libor dépasse la barrière, désactivant ainsi l’option, tandis qu’une faible volatilité réduit cette probabilité, maintenant l’option active.\n\nLa méthode de Monte-Carlo repose sur des simulations aléatoires pour estimer la valeur attendue d’un payoff futur, actualisé à aujourd’hui. Cette méthode repose sur la loi des grands nombres : si l’on simule un grand nombre de scénarios possibles pour l’évolution du sous-jacent (les taux d’intérêt pour ce qui nous concerne), la moyenne des payoffs actualisés converge vers le prix théorique de l’instrument\nOn peut générer une variable gaussienne standard \\(Z \\sim \\mathcal{N}(0,1)\\) à partir d’une variable uniforme \\(U \\sim \\mathcal{U}(0,1)\\) en utilisant la réciproque de la fonction de répartition:\n$$\nZ = ^{-1}(U)\n$$\nOn considère un caplet sur euribor12M à barrière désactivante\n\nde strike \\(ATM-100\\) bps,\nde barrière \\(ATM+100\\) 100 bps et\nde maturité 5Y.\n\nAlors, sa valeur calculée à l’aide de la méthode de Monte-Carlo est de 0.29%.\n\n\n\n\nCode\ndef B(t,T, x, lambd, sig):\n\n    \n\n    def beta(t,T):\n\n        return (1- np.exp(-lambd*(T-t)))/lambd\n\n\n\n    def phi(t):\n\n        return (sig**2)*(1-np.exp(-2*lambd*t))/(2*lambd)\n\n    \n\n    B_t = float(yield_zc[yield_zc.MAT==t][\"B(0,MAT)\"].iloc[0])\n\n    B_T =  float(yield_zc[yield_zc.MAT==T][\"B(0,MAT)\"].iloc[0])\n\n    \n\n    bet = beta(t,T)\n\n    B_t_T = (B_T/B_t) * np.exp(-0.5*phi(t)*(bet**2)-bet*x)\n\n    \n\n    return B_t_T\n\n\n\n\n\ndef option_vanille(T, delta, K, sig, lambd, barrier, M=10_000):\n\n\n\n    # Number of time steps (daily steps for T years)\n\n    n = round(T * 252)\n\n    Delta = T / n  # Time step size\n\n    t_vec = np.arange(1, n + 1) * Delta  # Time steps\n\n\n\n    # Simulate paths of X_t\n\n    X = np.zeros((M, n))\n\n    for t in range(n - 1):\n\n        phi_t = (sig**2 / (2 * lambd)) * (1 - np.exp(-2 * lambd * t_vec[t]))\n\n        gamma_t_T = (sig / lambd) * (np.exp(-lambd * (T - t_vec[t])) - 1)\n\n        \n\n        # Euler-Maruyama discretization\n\n        X[:, t + 1] = X[:, t] + (phi_t + sig * gamma_t_T - lambd * X[:, t]) * Delta + sig * np.sqrt(Delta) * np.random.normal(0, 1, size=M)\n\n\n\n    # Calculate Libor rates for each path\n\n    L_i = np.array([((1 / B(T, T + delta, X[:, t], lambd, sig)) - 1) / delta for t in range(n)])\n\n\n\n    # Barrier condition\n\n    L_i_B = (L_i[-1,:] &lt; barrier).astype(int)\n\n\n\n    # Payoff calculation\n\n    payoff = np.maximum(L_i[-1,:] - K, 0) * L_i_B\n\n\n\n    # Discounting\n\n    B_T_delta = float(yield_zc[yield_zc.MAT == T + delta][\"B(0,MAT)\"].iloc[0])\n\n    option_price = B_T_delta * np.mean(payoff)\n\n    option_std = np.std(B_T_delta*payoff)\n\n    #plt.plot(L_i[:,:1000])\n\n\n\n    return np.array([option_price, option_std]) # prix et ecart-type de l'option\n\n\nLa valeur de l’option calculée à l’aide de la méthode de Monte-Carlo est de 0.29% pour 10 000 simulations.\n\n\n\nCode\nnp.random.seed(90) # Pour la reproductibilité\n\n\n\nbarrier = L_0 + 100/ 10_000 \n\nK = L_0 - 100/ 10_000\n\n\n\nL = option_vanille(T = 5, delta = 1, K = K, sig = vol_inst, lambd = lambd, barrier = barrier, M=10_000)\n\nprint(\"Prix du caplet: \\t\", L[0])\n\nprint(\"Standard error du caplet: \\t\", L[1])\n\n\nPrix du caplet:      0.002952664054116537\nStandard error du caplet:    0.0048520375617754595\n\n\n\n\nCode\nM =  np.arange(100, 10_000, 500)\n\ncap_prices = np.array([option_vanille(T = 5, delta = 1, K = K, sig = vol_inst, lambd = lambd, barrier = barrier, M=m) for m in M])\n\nplt.plot(M, cap_prices[:,0], label=\"Caplet MC price avec intervalle de confiance à 95%\", color='blue')\n\nplt.plot(M, cap_prices[:,0] + 1.96*cap_prices[:,1], label=\"Upper bound\", linestyle=\"dashed\", color='red')\n\nplt.plot(M, cap_prices[:,0] - 1.96*cap_prices[:,1], label=\"Lower bound\", linestyle=\"dashed\", color='red')\n\n\n\n\n\n\n\n\n\n\nLa dégénération du produit en faisant tendre la barrière vers 0 et \\(+\\infty\\) est donnée par la figure ci-dessous.\n\nOn contate que le prix de l’option tend à s’annuler à mesure que la barrière tends vers 0 et il tend à être constant à mesure que la barrière tend vers \\(+\\infty\\).\nCeci traduit le fait que l’option devient de moins en moins attractifs pour des barrières très basses et s’apprécie pour des barrières élevées (il devient moins probable de la franchir).\n\n\n\n\n\nCode\nnp.random.seed(90)\n\n\n\nbarriers = np.linspace(0,1, num=50) # Différentes barrières\n\nK = L_0 - 100/ 10_000\n\n\n\ncaplets_barrier = np.array([option_vanille(T = 5, delta = 1, K = K, sig = vol_inst, lambd = lambd, barrier = barrier) for barrier in barriers])\n\n\n\nplt.plot(barriers, caplets_barrier, label =\"Prix de l'option\")\n\nplt.xlabel(\"Barrières\")\n\nplt.ylabel(\"Prix de l'option\")\n\nplt.legend()\n\nplt.title(\"Prix de l'option en fonction du niveau de la barrière\")\n\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.random.seed(90)\n\n\n\nbarrier = 10 # Barrière à 1000%\n\nK = L_0 \n\nL = option_vanille(T = 5, delta = 1, K = L_0, sig = vol_inst, lambd = lambd, barrier = barrier, M=10_000)\n\nprint(L)\n\n\n[0.0074075  0.01072443]\n\n\n\nLa barrière est rendue “bermudéenne” en étendant la condition de désactivation aux dates 1Y, 2Y, 3Y, 4Y et 5Y.\nLa nouvelle fonction de payoff s’écrit:\n\\[\\begin{align*}\n\n     \\text{Payoff}_{T_{i+1}} &= \\begin{cases}\n\n         \\delta_i \\cdot (L_i(T_i) - K)^+ \\text{ si aucune la barrière n'est franchie en aucune date } T_j \\\\\n\n         0 \\text{ si la barrière est franchie au moins une fois}\n\n     \\end{cases} \\\\\n\n     &=\\delta_i \\cdot (L_i(T_i) - K)^+ \\cdot \\mathbb{1}_{\\{max_{1\\leq j \\leq 5} L_i(T_j) &lt; B \\}}\\\\  \n\n\\end{align*}\\]\n\n\n\nCode\n# Pricer d'une option bermudéenne\n\n\n\ndef option_vanille_bermude(T, delta, K, sig, lambd, barrier, M=10_000):\n\n    \"\"\"\n\n    Valorisation d'un caplet avec barrière bermudéenne via Monte Carlo.\n\n\n\n    Paramètres :\n\n    - T : Dates d'exercice bermudéennes (ex. [1, 2, 3, 4, 5] ans)\n\n    - delta : Intervalle de temps entre la date de fixing et la date de paiement\n\n    - K : Strike du caplet\n\n    - sig : Volatilité du taux d'intérêt\n\n    - lambd : Paramètre de régression vers la moyenne du taux court\n\n    - barrier : Barrière désactivante\n\n    - yield_zc : Courbe des taux zéro-coupon (DataFrame avec colonnes 'MAT' et 'B(0,MAT)')\n\n    - M : Nombre de simulations Monte Carlo\n\n    \"\"\"\n\n\n\n    # Discrétisation temporelle : pas de temps quotidien\n\n    n = round(T[-1] * 252)  # Nombre total de pas de temps\n\n    Delta = T[-1] / n  # Taille du pas de temps\n\n    t_vec = np.arange(1, n + 1) * Delta  # Vecteur des dates simulées\n\n\n\n    # Simulation des trajectoires du taux court via un processus de Vasicek/Hull-White\n\n    X = np.zeros((M, n))\n\n    for t in range(n - 1):\n\n        phi_t = (sig**2 / (2 * lambd)) * (1 - np.exp(-2 * lambd * t_vec[t]))\n\n        gamma_t_T = (sig / lambd) * (np.exp(-lambd * (T[-1] - t_vec[t])) - 1)\n\n        \n\n        # Euler\n\n        X[:, t + 1] = X[:, t] + (phi_t + sig * gamma_t_T - lambd * X[:, t]) * Delta + sig * np.sqrt(Delta) * np.random.normal(0, 1, size=M)\n\n\n\n    # Vérification de la désactivation bermudéenne aux dates spécifiques\n\n    T_surveillance = np.array(T)  # Dates de surveillance de la barrière\n\n\n\n    # Calcul des L_i aux dates de surveillance\n\n    L_i_surveillance = np.zeros((M, len(T_surveillance)))\n\n    for i, T_i in enumerate(T_surveillance):\n\n        idx = round(T_i * 252) - 1  # Indice temporel à la date de surveillance\n\n        L_i_surveillance[:, i] = ((1 / B(T_i, T_i + delta, X[:, idx], lambd, sig)) - 1) / delta  # Libor forward\n\n\n\n    # Désactivation si la barrière est franchie à une date de surveillance\n\n    barrier_crossed = np.any(L_i_surveillance &gt;= barrier, axis=1)\n\n    \n\n\n\n    # Calcul des payoffs à la date de maturité\n\n    idx = round(T[-1] * 252) - 1  # Indice temporel à la maturité\n\n    L_i_maturity = ((1 / B(T[-1], T[-1] + delta, X[:, idx], lambd, sig)) - 1) / delta  # Libor forward\n\n\n\n    # Payoff du caplet\n\n    payoff = np.maximum(0, L_i_maturity - K)  # Payoff à la maturité\n\n    payoff[barrier_crossed] = 0  # Désactivation complète\n\n\n\n    # Actualisation et valorisation\n\n    B_T_delta = float(yield_zc[yield_zc.MAT == T[-1] + delta][\"B(0,MAT)\"].iloc[0])\n\n    option_price = B_T_delta * np.mean(payoff)\n\n\n\n    return np.array([option_price, np.mean(barrier_crossed)]) # retourn le prix de l'option ainsi que \n\n                                                            # la proba de franchissement de la barrière\n\n\n\n\nCode\nnp.random.seed(90)\n\n\n\nbarrier = L_0 + 100/ 10_000\n\nK = L_0 - 100/ 10_000\n\n\n\ncap = option_vanille_bermude(T = np.array([1,2,3,4,5]), delta = 1, K = K, sig = vol_inst, lambd = lambd, barrier = barrier)\n\n\n\nprint(\"Prix de l'option bermudéenne: \\t\", round(cap[0]*100,2), \"%\")\n\nprint(\"Probabilité d'atteinte de la barrière: \\t\", round(cap[1]*100, 2), \"%\")\n\n\nPrix de l'option bermudéenne:    0.22 %\nProbabilité d'atteinte de la barrière:   40.78 %\n\n\nAvec un strike \\(ATM-100\\) et une barrière \\(ATM+100\\), le prix de cette option est de \\(0.22\\%\\).\n\nOn constate que cette option est moins chère que la précédente.\nCe résultat est conforme à nos attentes. En effet, avec plus de conditions de désactivation, l’option bermudéenne a une valeur intrinsèque plus faible, car elle est plus susceptible d’être désactivée avant la date de maturité.\nCela reflète le fait que le porteur de l’option est exposé à un risque plus élevé de perdre son payoff, tandis que le vendeur se protège contre des variations défavorables des taux à plusieurs dates.\n\n\n\nL’impact de la mean reversion sur la valorisation de l’option est illustré par la figure ci-après.\nNB: Le modèle est recalibré sur le même caplet ATM pour chaque valeur de mean-reversion\n\n\n\nCode\nbarrier = L_0 + 100/ 10_000\n\nK = L_0 - 100/ 10_000\n\n\n\nlambdas = np.linspace(0,1, num=100)\n\ncap_mean_rev = np.array([option_vanille_bermude(T = np.array([1,2,3,4,5]), delta = 1, K = K,\n\n                                                sig = sigma(5, 6, lambd, vol=vol_imp), # On recalibre la vol pour chaque lambda\n\n                                                lambd = lambd,\n\n                                                barrier = barrier) for lambd in lambdas] )\n\n\n\nplt.plot(lambdas, cap_mean_rev[:,0], label =\"Prix de l'option\")\n\nplt.xlabel(\"Mean reversion ($\\lambda$)\")\n\nplt.ylabel(\"Prix de l'option\")\n\nplt.legend()\n\nplt.title(\"Prix de l'option en fonction de la mean reversion\")\n\nplt.grid(True)\n\nplt.show()\n\n\n&lt;&gt;:21: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:21: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\1195450163.py:21: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.xlabel(\"Mean reversion ($\\lambda$)\")\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2512216972.py:7: RuntimeWarning: invalid value encountered in scalar divide\n  beta = (1-np.exp(-lambd*(T_i - T_i_1)))/lambd\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2512216972.py:9: RuntimeWarning: invalid value encountered in scalar divide\n  sigma = np.sqrt((2*lambd*T_i_1)/(1-np.exp(-2*lambd*T_i_1))) * vol / beta\n\n\n\n\n\n\n\n\n\n\n\nOn observe qu’une mean reversion faible augmente la valeur de l’option. En effet, le taux se stabilise autour de sa moyenne de long terme, réduisant la probabilité de franchissement de la barrière.\nÀ l’inverse, une mean reversion forte diminue la valeur de l’option, car elle rend le taux plus volatile et moins prévisible.\n\n\n\n\nCode\nnp.random.seed(90)\n\n\n\nbarrier = L_0 + 100/ 10_000\n\nK = L_0 - 100/ 10_000\n\n\n\nlambdas = np.linspace(0,1, num=100)\n\ncap_mean_rev = np.array([option_vanille_bermude(T = np.array([1,2,3,4,5]), delta = 1, K = K,\n\n                                                sig = sigma(5, 6, lambd, vol=vol_imp), # On recalibre la vol pour chaque lambda\n\n                                                lambd = lambd,\n\n                                                barrier = barrier) for lambd in lambdas] )\n\n\n\nplt.plot(lambdas, cap_mean_rev[:,0], label =\"Prix de l'option\")\n\nplt.xlabel(\"Mean reversion ($\\lambda$)\")\n\nplt.ylabel(\"Prix de l'option\")\n\nplt.legend()\n\nplt.title(\"Prix de l'option en fonction de la mean reversion\")\n\nplt.grid(True)\n\nplt.show()\n\n\n&lt;&gt;:25: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:25: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2646903226.py:25: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.xlabel(\"Mean reversion ($\\lambda$)\")\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2512216972.py:7: RuntimeWarning: invalid value encountered in scalar divide\n  beta = (1-np.exp(-lambd*(T_i - T_i_1)))/lambd\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2512216972.py:9: RuntimeWarning: invalid value encountered in scalar divide\n  sigma = np.sqrt((2*lambd*T_i_1)/(1-np.exp(-2*lambd*T_i_1))) * vol / beta\n\n\n\n\n\n\n\n\n\n\n\nCode\nlambdas = np.linspace(0,1, num=100)\n\nvol_mean_rev = np.array([sigma(5, 6, lambd, vol=vol_imp) for lambd in lambdas] )\n\n\n\nplt.plot(lambdas, vol_mean_rev, label =\"Volatilité instantanée\")\n\nplt.xlabel(\"Mean reversion ($\\lambda$)\")\n\nplt.ylabel(\"volatilité\")\n\nplt.legend()\n\nplt.title(\"Volatilité instantanée en fonction de la mean reversion\")\n\nplt.grid(True)\n\nplt.show()\n\n\n&lt;&gt;:9: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:9: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\1203548000.py:9: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.xlabel(\"Mean reversion ($\\lambda$)\")\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2512216972.py:7: RuntimeWarning: invalid value encountered in scalar divide\n  beta = (1-np.exp(-lambd*(T_i - T_i_1)))/lambd\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2512216972.py:9: RuntimeWarning: invalid value encountered in scalar divide\n  sigma = np.sqrt((2*lambd*T_i_1)/(1-np.exp(-2*lambd*T_i_1))) * vol / beta\n\n\n\n\n\n\n\n\n\n\nProbabilité d’atteinte de la barrière en fonction de la mean reversion\n\nOn peut déduire de ce qui précède que la probabilité de toucher la barrière croît lorsque la mean reversion décroît.\nUne estimation faite en valorisant l’option illutrer d’ailleurs cette analyse (voir figure i-dessous). Il apparaît clairement qu’il devient de plus en plus probable de franchir la barrière pour des mean reversion fortes.\n\n\nIl est à noter que ces observations faite pour la mean reversion et la probabilité d’atteinte de la barrière est inversée lorsque le modèle n’est pas recalibré pour chaque valeur de \\(\\lambda\\).\n\n\nCode\nplt.plot(lambdas, cap_mean_rev[:,1],  linestyle='-', color='r', label=\"Probabilité de toucher la barrière\")\n\nplt.xlabel(\"Mean reversion ($\\lambda$)\")\n\nplt.ylabel(\"Probabilité d'atteindre la barrière\")\n\nplt.title(\"Impact de la Mean Reversion sur l'Atteinte de la Barrière\")\n\nplt.grid(True)\n\nplt.legend()\n\nplt.show()\n\n\n&lt;&gt;:3: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n&lt;&gt;:3: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\nC:\\Users\\mokom\\AppData\\Local\\Temp\\ipykernel_22344\\2478896390.py:3: SyntaxWarning: \"\\l\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\l\"? A raw string is also an option.\n  plt.xlabel(\"Mean reversion ($\\lambda$)\")"
  },
  {
    "objectID": "risques/projets/stat_extreme.html",
    "href": "risques/projets/stat_extreme.html",
    "title": "Statistiques des risques extrêmes",
    "section": "",
    "text": "Ce projet a pour objectif d’étudier la mesure du risque de pertes extrêmes à partir d’une série historique de log-rendements boursiers, en combinant :\n\ndes approches classiques de VaR non paramétrique et gaussienne,\ndes méthodes plus avancées basées sur la pondération EWMA,\ndes lois de probabilité à queues épaisses (Skew-Student),\nla théorie des valeurs extrêmes (TVE : GEV, GPD, POT),\net enfin des modèles dynamiques de volatilité (AR(1)-GARCH(1,1)) pour construire une VaR dynamique et backtestée.\n\nPlus spécifiquement, il s’agira de :\n\ncomparer les différentes approches de VaR en termes de réalisme et de couverture du risque ;\nmettre en place un protocole de backtesting adaptatif permettant de recalibrer les modèles lorsque ceux-ci ne décrivent plus correctement le risque observé.\n\nLien vers l’application Dash (GitHub)"
  },
  {
    "objectID": "risques/projets/stat_extreme.html#introduction",
    "href": "risques/projets/stat_extreme.html#introduction",
    "title": "Statistiques des risques extrêmes",
    "section": "",
    "text": "Ce projet a pour objectif d’étudier la mesure du risque de pertes extrêmes à partir d’une série historique de log-rendements boursiers, en combinant :\n\ndes approches classiques de VaR non paramétrique et gaussienne,\ndes méthodes plus avancées basées sur la pondération EWMA,\ndes lois de probabilité à queues épaisses (Skew-Student),\nla théorie des valeurs extrêmes (TVE : GEV, GPD, POT),\net enfin des modèles dynamiques de volatilité (AR(1)-GARCH(1,1)) pour construire une VaR dynamique et backtestée.\n\nPlus spécifiquement, il s’agira de :\n\ncomparer les différentes approches de VaR en termes de réalisme et de couverture du risque ;\nmettre en place un protocole de backtesting adaptatif permettant de recalibrer les modèles lorsque ceux-ci ne décrivent plus correctement le risque observé.\n\nLien vers l’application Dash (GitHub)"
  },
  {
    "objectID": "risques/projets/stat_extreme.html#récupération-des-données-et-analyse",
    "href": "risques/projets/stat_extreme.html#récupération-des-données-et-analyse",
    "title": "Statistiques des risques extrêmes",
    "section": "Récupération des données et analyse",
    "text": "Récupération des données et analyse\n\n\nCode\nimport functions\nimport importlib\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport yfinance as yf\n\nwarnings.filterwarnings(\"ignore\")\n\nticker_symbol = \"^FCHI\"\ndf_close = yf.download(ticker_symbol, start=\"2008-10-15\", end=\"2024-06-11\")[[\"Close\"]]\ndf_close.head(2)\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nClose\n\n\nTicker\n^FCHI\n\n\nDate\n\n\n\n\n\n2008-10-15\n3381.070068\n\n\n2008-10-16\n3181.000000\n\n\n\n\n\n\n\n\nVérification des valeurs manquantes\n\n\nCode\nmissing_values = df_close.isnull().sum().iloc[0]\nprint(f\"Nombre de valeurs manquantes dans 'Close': {missing_values}\")\n\n\nNombre de valeurs manquantes dans 'Close': 0\n\n\n\n\nRésumé descriptif\n\n\nCode\nsummary = df_close['Close'].describe()\nsummary\n\n\n\n\n\n\n\n\nTicker\n^FCHI\n\n\n\n\ncount\n4002.000000\n\n\nmean\n4909.823427\n\n\nstd\n1289.022374\n\n\nmin\n2519.290039\n\n\n25%\n3871.482483\n\n\n50%\n4755.440186\n\n\n75%\n5590.390015\n\n\nmax\n8239.990234\n\n\n\n\n\n\n\n\n\nCalcul des log-rendements\n\n\nCode\ndf_close[\"Log Return\"] = np.log(df_close[\"Close\"] / df_close[\"Close\"].shift(1))\ndf_close.dropna(inplace=True)\ndf_close.head(2)\n\n\n\n\n\n\n\n\nPrice\nClose\nLog Return\n\n\nTicker\n^FCHI\n\n\n\nDate\n\n\n\n\n\n\n2008-10-16\n3181.000000\n-0.060997\n\n\n2008-10-17\n3329.919922\n0.045753\n\n\n\n\n\n\n\n\n\nVisualisation\n\n\nCode\nfig, ax1 = plt.subplots(figsize=(12, 6))\nax1.plot(df_close.index, df_close[\"Close\"], label=\"Close\", alpha=0.7)\nax2 = ax1.twinx()\nax2.plot(df_close.index, df_close[\"Log Return\"], color=\"red\", label=\"Log Return\", alpha=0.7)\nplt.title(\"Évolution des prix de clôture et des log-rendements\")\nplt.show()\n\n\n\n\n\n\n\n\n\nOn observe une tendance haussière nette sur les prix, signe de non-stationnarité, tandis que les rendements semblent plus réguliers."
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html",
    "href": "risques/projets/stat_extreme_corrige.html",
    "title": "Statistiques des risques extrêmes",
    "section": "",
    "text": "⬅ Retour"
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#recuperation-des-données-et-analyse-des-données",
    "href": "risques/projets/stat_extreme_corrige.html#recuperation-des-données-et-analyse-des-données",
    "title": "Statistiques des risques extrêmes",
    "section": "Recuperation des données et analyse des données",
    "text": "Recuperation des données et analyse des données\n\n\nCode\nimport yfinance as yf\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom arch import arch_model\nfrom scipy import stats\nfrom scipy.stats import jarque_bera\nfrom scipy.stats import shapiro\n\nimport functions\nimport importlib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nticker_symbol = \"^FCHI\"\ndf_close = yf.download(ticker_symbol, start=\"2008-10-15\", end = \"2024-06-11\")[\"Close\"]\ndf_close.columns = ['Close']\ndf_close.head(2)\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nClose\n\n\nDate\n\n\n\n\n\n2008-10-15\n3381.070068\n\n\n2008-10-16\n3181.000000\n\n\n\n\n\n\n\n\nExploration des données à notre disposition\n\nVérification des valeurs manquantes\n\n\nCode\nmissing_values = df_close.isnull().sum()\nprint(f\"Nombre de valeurs manquantes dans la colonne 'Close': {missing_values[0]}\")\n\n\nNombre de valeurs manquantes dans la colonne 'Close': 0\n\n\nRésumé descriptif des données\n\n\nCode\nsummary = df_close['Close'].describe()\nprint(\"\\nRésumé descriptif des prix de clôture :\")\nprint(summary)\n\n\n\nRésumé descriptif des prix de clôture :\ncount    4002.000000\nmean     4909.823427\nstd      1289.022374\nmin      2519.290039\n25%      3871.482483\n50%      4755.440186\n75%      5590.390015\nmax      8239.990234\nName: Close, dtype: float64\n\n\nCalcul des log return\n\n\nCode\ndf_close['Log Return'] = np.log(df_close['Close'] / df_close['Close'].shift(1))\ndf_close = df_close.dropna()\ndf_close.head(2)\n\n\n\n\n\n\n\n\n\nClose\nLog Return\n\n\nDate\n\n\n\n\n\n\n2008-10-16\n3181.000000\n-0.060997\n\n\n2008-10-17\n3329.919922\n0.045753\n\n\n\n\n\n\n\nVisualisation de l’évolution des prix de clôture ainsi que celle des log_return sur la période d’étude\n\n\nCode\n# Création du graphique\nfig, ax1 = plt.subplots(figsize=(12, 6))\n\n# Evolution de \"Close\" sur l'axe primaire\nax1.plot(df_close.index, df_close['Close'], color='blue', label='Close', alpha=0.7)\nax1.set_xlabel('Date')\nax1.set_ylabel('Prix de Clôture', color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\n\n# Création d'un deuxième axe y pour les log returns\nax2 = ax1.twinx()\nax2.plot(df_close.index, df_close['Log Return'], color='red', label='Log Return', alpha=0.7)\nax2.set_ylabel('Log Return', color='red')\nax2.tick_params(axis='y', labelcolor='red')\n\n# Ajout des titres et légendes\nplt.title(\"Évolution des prix de clôture et des Log Returns (Eurostocks50)\")\nfig.tight_layout()  \nplt.show()\n\n\n\n\n\n\n\n\n\nOn observe dejà une tendance très marquée et haussiere au niveau des prix de clôture, signe que cette variable n’est pas stationnaire.\nLes rendements en revanche semblent présenter une certaine regularité autour de la moyenne\n\nDivision de nos données en echantillon de train et de test\n\nApprentissage : 15 octobre 2008 - 26 juillet 2022\nTest : 27 juillet 2022 - 11 juin 2024\n\n\n\nCode\ntrain_start, train_end = \"2008-10-15\", \"2022-07-26\"  # la BCE a commencé à baisser ses taux directeurs\ntest_start, test_end = \"2022-07-27\", \"2024-06-11\"   # la BCE a commencé à relever progressivement ses taux directeurs\n\n\ntrain_data = df_close.loc[train_start:train_end]\ntest_data = df_close.loc[test_start:test_end]\n\n# Afficher les tailles des ensembles\nprint(f\"Taille de l'ensemble d'entraînement: {train_data.shape[0]} jours\")\nprint(f\"Taille de l'ensemble de test: {test_data.shape[0]} jours\")\n\n\nTaille de l'ensemble d'entraînement: 3522 jours\nTaille de l'ensemble de test: 479 jours\n\n\n\n\n\nStatistiques descriptives sur les 2 jeux de données\nNous nous intéressons aux caractéristiques de tendance centrale et de dispersion usuelles et, du fait du constat fait sur le graphique de l’évolution des prix de cloture et des log-rendements sur la période d’étude, nous réalisons également des test de stationnarité à savoir : le test de Dick et Fuller augmenté et le test de KPSS.\n\n\nEchantillon train\n\n\nCode\nimportlib.reload(functions)\nresultat = functions.descriptive_statistics(train_data, train_data.columns)\nresultat\n\n\n\n\n\n\n\n\n\nStatistic\nClose\nLog Return\n\n\n\n\n0\nMean\n4601.646103\n0.000173\n\n\n1\nMedian\n4463.850098\n0.000582\n\n\n2\nVariance\n1044911.013882\n0.000193\n\n\n3\nStandard Deviation\n1022.20889\n0.013904\n\n\n4\nSkewness\n0.402481\n-0.268165\n\n\n5\nKurtosis\n-0.483977\n7.302393\n\n\n6\nADF p-value\n0.461665\n0.0\n\n\n7\nKPSS p-value\n0.01\n0.1\n\n\n8\nADF Stationarity\nNon stationnaire\nStationnaire\n\n\n9\nKPSS Stationarity\nNon stationnaire\nStationnaire\n\n\n\n\n\n\n\n\n\nCode\nfunctions.plot_histograms(train_data, train_data.columns)\n\n\n\n\n\n\n\n\n\nCommentaire\nSur l’échantillon de train, les prix sont non stationnaires, et assez variables, avec un écart-type de 1022 pour une moyenne de 4601, soit près de 1/4 de la valeur moyenne.\nLes log rendements en revanche présentent une certaine régularité autour de leur moyenne (0.000173) comme l’attestent les tests de stationnarité de Dick et Fuller augmenté et de KPSS. On note toute fois une asymétrie marquée (-0.26) et des queues lourdes (avec un excess kurtosis de 7.30) contrairement aux prix de clôture\n\n\n\nEchantillon test\n\n\nCode\nstat_test = functions.descriptive_statistics(test_data, test_data.columns)\nstat_test\n\n\n\n\n\n\n\n\n\nStatistic\nClose\nLog Return\n\n\n\n\n0\nMean\n7178.986868\n0.0005\n\n\n1\nMedian\n7260.72998\n0.000767\n\n\n2\nVariance\n346425.733475\n0.000078\n\n\n3\nStandard Deviation\n588.57942\n0.008812\n\n\n4\nSkewness\n-0.369065\n-0.126194\n\n\n5\nKurtosis\n-0.2724\n1.960086\n\n\n6\nADF p-value\n0.715622\n0.0\n\n\n7\nKPSS p-value\n0.01\n0.1\n\n\n8\nADF Stationarity\nNon stationnaire\nStationnaire\n\n\n9\nKPSS Stationarity\nNon stationnaire\nStationnaire\n\n\n\n\n\n\n\n\n\nCode\nfunctions.plot_histograms(test_data, test_data.columns)\n\n\n\n\n\n\n\n\n\nCommentaire\nLes caractéristiques de stationnarité sont similaires entre l’échantillon de test et l’échantillon d’entraînement.\nCependant, l’échantillon de test présente en moyenne des prix et des rendements logarithmiques plus élevés, tout en étant moins dispersés.\nEn ce qui concerne les distributions des rendements logarithmiques, les queues de l’échantillon de test sont moins lourdes que celles de l’échantillon d’entraînement. Toutefois, elles restent plus épaisses que celles d’une loi normale, comme l’indique un excès de kurtosis de 1,96."
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#var-non-paramétrique",
    "href": "risques/projets/stat_extreme_corrige.html#var-non-paramétrique",
    "title": "Statistiques des risques extrêmes",
    "section": "VaR non paramétrique",
    "text": "VaR non paramétrique\n\nVaR Historique\nEcrire une fonction calculant la VaR historique d’un ensemble de log-rendements : VaR_Hist(x, alpha)\nCalcul de la VaR historique à horizon 1 jour sur base d’apprentissage pour alpha = 99%.\n\n\nCode\nimportlib.reload(functions)\nvar_99_hist = functions.VaR_Hist(train_data['Log Return'])\nprint(f\"VaR historique à 99% sur un  horizon d'un jour : {var_99_hist:.4%}\")\n\n\nVaR historique à 99% sur un  horizon d'un jour : -4.0517%\n\n\n\n\nVaR Bootstrap\nEcrire une fonction calculant la VaR historique bootstrap d’un ensemble de log-rendements et donnant un IC de niveau alpha_IC de cette VaR\nNous choisissons de prendre la médiane des VaR des differents echantillons bootstrapés car elle est plus robuste que la moyenne. Nous choississons un nombre d’itération suffisamment grand pour nous assurer de la pertinence de la VaR estimée. On choisit également N tel que Nα soit divisible par 2 afin de pouvoir extraire des indices exacts pour les quantiles d’ordre Nα/2 et 1−Nα/2.\n\nCalcul de la VaR historique bootstrap et l’IC associé à 90% sur base d’apprentissage pour alpha = 99%.\n\n\nCode\nimportlib.reload(functions)\nvar_bootstrap_99, var_ci = functions.VaR_Hist_Bootstrap(train_data['Log Return'],confidence_level=0.99, n_iterations=10000, ci_level=0.90)\n\n# Affichage des résultats\nprint(f\"VaR Bootstrap à 99% sur un jour : {var_bootstrap_99:.4%}\")\nprint(f\"Intervalle de confiance à 95% : [{var_ci[0]:.4%}, {var_ci[1]:.4%}]\")\n\n\nVaR Bootstrap à 99% sur un jour : -4.0517%\nIntervalle de confiance à 95% : [-4.3588%, -3.8138%]\n\n\nCalcul du nombre d’exceptions sur base de test associées à la VaR historique calculée précédemment\n\n\nCode\nimportlib.reload(functions)\nfunctions.var_exceptions(test_data, var_99_hist)\n\n\nNombre d'exceptions (log-returns inférieurs à la VaR à 99%) : 0\nPourcentage d'exceptions : 0.00%\n\n\nComparer statistiquement ce pourcentage d’exceptions avec le niveau de risque attendu\nNous mettons ici en place le test statistique d’unconditional coverage.\n\n\nTest d’unconditionnal coverage : Présentation du principe\nPrésentation du principe du test d’unconditional coverage\nLe test d’Unconditional Coverage évalue si la proportion d’excès observés (c’est-à-dire les rendements inférieurs à la VaR) correspond à la probabilité théorique de dépassement, \\(\\alpha\\) (1% dans notre cas).\n\nHypothèse nulle (H₀) : La proportion d’excès observée est égale à \\(\\alpha\\).\nStatistique de test : Le test est basé sur le rapport de vraisemblance (Likelihood Ratio, LR) :\n\\[\nLR_{\\text{uc}} = -2 \\cdot \\log\\left( (1 - \\alpha)^{n - n_{\\text{excès}}} \\cdot \\alpha^{n_{\\text{excès}}} \\right) + 2 \\cdot \\log\\left( (1 - \\hat{p}_{\\text{emp}})^{n - n_{\\text{excès}}} \\cdot \\hat{p}_{\\text{emp}}^{n_{\\text{excès}}} \\right)\n\\]\noù :\n\n\\(n\\) est le nombre total d’observations,\n\\(n_{\\text{excès}}\\) est le nombre d’excès (jours où le rendement est inférieur à la VaR),\n\\(\\hat{p}_{\\text{emp}}\\) est la proportion empirique d’excès observée.\n\np-value : La statistique suit une loi du chi-deux à 1 degré de liberté. La p-value est obtenue comme suit :\n\\[\np_{\\text{uc}} = 1 - \\chi^2_{\\text{cdf}}(LR_{\\text{uc}}, df=1)\n\\]\nConclusion : Si \\(p_{\\text{uc}} &gt; \\text{seuil de significativité}\\) (nous avons choisi 0.05), on ne rejette pas l’hypothèse nulle et la proportion d’excès observée est cohérente avec la probabilité théorique \\(\\alpha\\). Sinon, on rejette l’hypothèse nulle.\n\n\nResultat du test\n\n\nCode\nimportlib.reload(functions)\nLR_uc, p_value = functions.unconditional_coverage_test(test_data, var_99_hist, significance_level=0.01)\n\n\nNombre d'exceptions observées : 0\n--------------------------------------------------\nNombre total d'observations : 479\n--------------------------------------------------\nProbabilité de dépassement empirique : 0.0000\n--------------------------------------------------\nStatistique LR pour le test d'unconditional coverage : 9.6282\n--------------------------------------------------\nP-value du test d'unconditional coverage : 0.0019\n--------------------------------------------------\nL'hypothèse nulle est rejetée : Le modèle de VaR ne couvre pas correctement les exceptions."
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#var-gaussienne",
    "href": "risques/projets/stat_extreme_corrige.html#var-gaussienne",
    "title": "Statistiques des risques extrêmes",
    "section": "VaR Gaussienne",
    "text": "VaR Gaussienne\n\n\nEcrire une fonction calculant la VaR gaussienne d’un ensemble de log-rendements\nCalcul de la VaR gaussienne sur base d’apprentissage pour alpha = 99%.\n\n\nCode\nimportlib.reload(functions)\n\nvar_gauss = functions.VaR_Gauss(train_data, alpha=0.99)\nprint(f\"VaR Gaussienne à 99% sur un jour : {var_gauss:.4%}\")\n\n\nVaR Gaussienne à 99% sur un jour : -3.2172%\n\n\n\n\nFaire une validation ex-ante de cette VaR Gaussienne (analyses graphiques, QQ-plot, etc.)\n\nComparaison de la courbe densité empirique des log return et de la courbe de densité théorique supposée\n\n\nCode\nimportlib.reload(functions)\nfunctions.plot_kde_vs_gauss(train_data)\n\n\n\n\n\n\n\n\n\nOn observe que la distribution réelle est plus concentrée au centre et présente des queues plus épaisses que la loi normale supposée. Ceci indique une fréquence plus élevée d’événements extrêmes que ne le prévoit la loi normale. Ainsi, A priori, notre VaR gaussienne à 99% sous-estime le risque réel.\nNous utilisons un QQ-plot afin de conforter cette observation\n\nQQ-plot\n\n\nCode\nimportlib.reload(functions)\nfunctions.plot_qq(train_data['Log Return'])\n\n\n\n\n\n\n\n\n\nLe QQ-plot compare les quantiles empiriques des log-returns à ceux d’une distribution normale. On observe que :\n\nAu centre de la distribution, les points suivent approximativement la droite théorique, ce qui indique une légère conformité à la normalité.\nAux extrémités, les points s’écartent fortement de la droite, révélant des queues épaisses\n\nIl semblerait donc bien que la loi normale ne soit pas celle adaptée à nos données.\nNous confortons ces resultats visuels à l’aide de tests statistiques. Nous utilisons un test statistique d’adéquation (Kolmogorov-smirnov)\nTest de Kolmogorov Smirnov\n\n\nCode\nimportlib.reload(functions)\nfunctions.ks_test(train_data['Log Return'])\n\n\nStatistique KS: 0.0803\nP-valeur: 0.0000\nResultat du test : Rejet de l'hypothèse de normalité au seuil de 5% \n\n\nLe test d’adéquation corrobore les observations faites précédemment. On s’attend donc à ce que la Var ici calibrée ne passe pas le test d’unconditional coverage.\nTest d’unconditional coverage\n\n\nCode\nfunctions.unconditional_coverage_test(train_data, var_gauss, significance_level=0.01)\n\n\nNombre d'exceptions observées : 78\n--------------------------------------------------\nNombre total d'observations : 3522\n--------------------------------------------------\nProbabilité de dépassement empirique : 0.0221\n--------------------------------------------------\nStatistique LR pour le test d'unconditional coverage : 39.0018\n--------------------------------------------------\nP-value du test d'unconditional coverage : 0.0000\n--------------------------------------------------\nL'hypothèse nulle est rejetée : Le modèle de VaR ne couvre pas correctement les exceptions.\n\n\n(np.float64(39.00181311712197), np.float64(4.234120831725363e-10))\n\n\nEffectivemment , cette VaR ne couvre pas correctement les exceptions: Elle sous-estime le risque de perte\n\n\n\nCalcul de la VaR gaussienne à 10j par la méthode de scaling\n\nLa VaR gaussienne à 10 jours est estimée en multipliant la VaR quotidienne par la racine carrée du nombre de jours (10 jours dans ce cas)\n\\[\n\\text{VaR}_{10j} = \\text{VaR}_{1j} \\times \\sqrt{10}\n\\]\noù : - \\(\\text{VaR}_{1j}\\) est la VaR calculée sur une période de 1 jour, - \\(\\sqrt{10}\\) est le facteur de scaling basé sur le nombre de jours (10 jours ici).\n\n\nCode\nimportlib.reload(functions)\nvar_10j = functions.var_gauss_horizon(var_gauss, horizon=10)\n\n\nVaR à 10 jours : -10.17%\n\n\n\n\n\nCalcul de la VaR Gaussienne à 10 jours par la méthode de diffusion d’un actif\n\nLa VaR gaussienne à 10 jours peut être calculée en utilisant la méthode de diffusion d’un actif, basée sur le modèle suivant pour l’évolution du prix de l’actif :\n\\[\ndS = S \\cdot \\mu \\cdot dt + S \\cdot \\sigma \\cdot Z \\cdot \\sqrt{dt}\n\\]\noù : - \\(Z \\sim N(0, 1)\\) (bruit blanc suivant une loi normale standard), - \\(S_0\\) est la valeur du cours de clôture de l’actif à la dernière date de l’échantillon d’apprentissage, - \\(dt = 1 \\, \\text{jour}\\), - \\(\\mu\\) et \\(\\sigma\\) sont les paramètres estimés dans la section 2.b (moyenne et écart-type des rendements log-transformés).\nCette méthode permet d’estimer la VaR en tenant compte de la dynamique de diffusion du prix de l’actif.\n\nLa formule utilisée pour simuler les trajectoires du prix est obtenue en resolvant l’EDS présentée plus haut:\n\\(S_t = S_{t-1} \\cdot \\exp\\left(\\left(\\mu - 0.5 \\cdot \\sigma^2\\right) \\cdot \\Delta t + \\sigma \\cdot Z\\right)\\)\nIci, $ t = 1 $ car la simulation est effectuée sur des étapes d’une unité de temps (1 jour).\n\n\nCode\nimportlib.reload(functions)\n\n## Paramètres\nS0 = float(train_data[['Close']].iloc[-1])\nmu = np.mean(train_data['Log Return'])\nsigma = np.std(train_data['Log Return'])\nt = 10\nnum_simulations = 1000\n\n\nSt = functions.simulate_price_paths(t, S0, mu, sigma, num_simulations)\nlog_returns = functions.calculate_log_returns(St, S0)\n\n# Calcul de la VaR  à 99%\nvar_99 = functions.calculate_var(log_returns)\n\n\nVaR diffusée à 99.0% pour un horizon de 10 jours : -0.1035\n\n\n\n\nAperçu de quelques trajectoires de prix\n\n\n\nCode\nimportlib.reload(functions)\nnum_trajectoires =10\nfunctions.plot_simulations(St, num_trajectoires, t)\n\n\n\n\n\n\n\n\n\n\n\n\nCalcul de la VaR Gaussienne à 1 jour avec EWMA\n\nLa VaR Gaussienne à 1 jour est calculée en surpondérant les observations les plus récentes avec la méthode EWMA (Exponential Weighted Moving Average).\nLes poids sont définis par : - $ _i () = ^i (1 - ) $, où $ $ est le paramètre de lissage et $ i $ l’indice de l’observation.\nLes poids normalisés sont : - $ _i () = $.\nLa moyenne et la variance sont calculées comme suit : - $ () = {i=0}^{T} i () r{T-i} $, - $ ^2 () = {i=0}^{T} i () (r{T-i} - ())^2 $.\nCela permet de calculer la VaR en utilisant les rendements pondérés, avec une importance plus grande pour les rendements récents.\n\nCalcul de mu et sigma ewma pour differentes valeurs de lambda\n\n\nCode\nimportlib.reload(functions)\nlambdas = [ 0.9 , 0.95 , 0.99  ]\nfor lambda_ in lambdas:\n    result = functions.calculate_mu_sigma_ewma(train_data['Log Return'], lambda_)\n\nprint(\"L'ecart-type non pondéré des log rendements dans l'echantillon train est : \", train_data['Log Return'].std())\nprint('-'*50)\nprint(\"La moyenne non pondérée des log rendements dans l'echantillon train est : \", train_data['Log Return'].mean())\nprint('-'*50)\n\n\nPour λ = 0.9:\n  - μ̂(λ) = -0.0019\n  - σ̂(λ) = 0.0409\n----------------------------------------\nPour λ = 0.95:\n  - μ̂(λ) = -0.0015\n  - σ̂(λ) = 0.0380\n----------------------------------------\nPour λ = 0.99:\n  - μ̂(λ) = -0.0005\n  - σ̂(λ) = 0.0273\n----------------------------------------\nL'ecart-type non pondéré des log rendements dans l'echantillon train est :  0.013903729926502252\n--------------------------------------------------\nLa moyenne non pondérée des log rendements dans l'echantillon train est :  0.00017268658342823863\n--------------------------------------------------\n\n\nCommentaire\n\nPlus λ est grand, plus on lisse, et plus on se concentre sur les données récentes.\nEt des résultats précédents, en parcourant ces valeurs de λ, on observe que les log-rendements récents semblent moins négatifs et moins volatils lorsque lambda grandit, ce qui se reflète dans une moyenne μ̂ qui remonte légèrement (de -0.0019 à -0.0005) et une volatilité σ̂ qui diminue (de 0.0409 à 0.0273).\nCependant, ces écarts-types pondérés restent nettement supérieurs à l’écart-type non pondéré de l’échantillon (≈ 0.0139).\nDe plus, alors que la moyenne non pondérée est positive (≈ 0.00017), les moyennes pondérées sont toutes négatives, ce qui indique une tendance à la perte dans les observations récentes .\nOn s’attend donc à une VaR plus sévère que celle issue d’une approche non pondérée, car les récentes performances sont à la fois plus volatiles et plus négatives. Cela dit, à mesure que λ augmente, la VaR devrait décroître, en cohérence avec une baisse de la volatilité estimée.\n\nCalcul de la VaR gaussienne selon la méthode EWMA\n\nPour calculer la VaR gaussienne à 99% selon la méthode EWMA, on utilise les paramètres estimés $ () $ et $ ^2() $. La VaR est ensuite calculée à l’aide de la formule suivante :\n$ {} = () - {1-} () $\n\n\nCode\nimportlib.reload(functions)\nfor lambda_ in lambdas:\n    var_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], lambda_, alpha=0.99)\n\n\nPour λ = 0.9:\n  - μ̂(λ) = -0.0019\n  - σ̂(λ) = 0.0409\n----------------------------------------\nVaR gaussienne EWMA à 1 jour (α = 99.0%): -0.0971\n----------------------------------------\nPour λ = 0.95:\n  - μ̂(λ) = -0.0015\n  - σ̂(λ) = 0.0380\n----------------------------------------\nVaR gaussienne EWMA à 1 jour (α = 99.0%): -0.0898\n----------------------------------------\nPour λ = 0.99:\n  - μ̂(λ) = -0.0005\n  - σ̂(λ) = 0.0273\n----------------------------------------\nVaR gaussienne EWMA à 1 jour (α = 99.0%): -0.0640\n----------------------------------------\n\n\nLes resultats observés corroborent bien notre analyse faite précédemment. Les pertes decroissent à mesure que lambda grandit. Mais, elles demeurent bien plus sévères que celle observée sans pondération.\n\nOn constate par ailleurs que ces VaR pondérées sont toutes plus sévères que la VaR historique calculée précédemment, on s’attend donc à ce qu’il n’y ait pas d’exception et que le test d’unconditionnal coverage ne soit pas satisfait.\nPour s’en convaincre, exécutons le code qui calcule le nombre d’exception et fait le test.\n\n\nCode\n### Pour lambda = 0.9\n\nvar_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], 0.9, alpha=0.99)\nLR_uc, p_value = functions.unconditional_coverage_test(test_data, var_ewma, significance_level=0.01)\nprint(\"-\"*40)\n\n\nPour λ = 0.9:\n  - μ̂(λ) = -0.0019\n  - σ̂(λ) = 0.0409\n----------------------------------------\nVaR gaussienne EWMA à 1 jour (α = 99.0%): -0.0971\n----------------------------------------\nNombre d'exceptions observées : 0\n--------------------------------------------------\nNombre total d'observations : 479\n--------------------------------------------------\nProbabilité de dépassement empirique : 0.0000\n--------------------------------------------------\nStatistique LR pour le test d'unconditional coverage : 9.6282\n--------------------------------------------------\nP-value du test d'unconditional coverage : 0.0019\n--------------------------------------------------\nL'hypothèse nulle est rejetée : Le modèle de VaR ne couvre pas correctement les exceptions.\n----------------------------------------\n\n\n\n\nCode\n### Pour lambda = 0.95\n\nvar_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], 0.95, alpha=0.99)\nLR_uc, p_value = functions.unconditional_coverage_test(test_data, var_ewma, significance_level=0.01)\nprint(\"-\"*40)\n\n\nPour λ = 0.95:\n  - μ̂(λ) = -0.0015\n  - σ̂(λ) = 0.0380\n----------------------------------------\nVaR gaussienne EWMA à 1 jour (α = 99.0%): -0.0898\n----------------------------------------\nNombre d'exceptions observées : 0\n--------------------------------------------------\nNombre total d'observations : 479\n--------------------------------------------------\nProbabilité de dépassement empirique : 0.0000\n--------------------------------------------------\nStatistique LR pour le test d'unconditional coverage : 9.6282\n--------------------------------------------------\nP-value du test d'unconditional coverage : 0.0019\n--------------------------------------------------\nL'hypothèse nulle est rejetée : Le modèle de VaR ne couvre pas correctement les exceptions.\n----------------------------------------\n\n\n\n\nCode\n### Pour lambda = 0.99\n\nvar_ewma = functions.calculate_var_gauss_ewma(train_data['Log Return'], 0.99, alpha=0.99)\nLR_uc, p_value = functions.unconditional_coverage_test(test_data, var_ewma, significance_level=0.01)\nprint(\"-\"*40)\n\n\nPour λ = 0.99:\n  - μ̂(λ) = -0.0005\n  - σ̂(λ) = 0.0273\n----------------------------------------\nVaR gaussienne EWMA à 1 jour (α = 99.0%): -0.0640\n----------------------------------------\nNombre d'exceptions observées : 0\n--------------------------------------------------\nNombre total d'observations : 479\n--------------------------------------------------\nProbabilité de dépassement empirique : 0.0000\n--------------------------------------------------\nStatistique LR pour le test d'unconditional coverage : 9.6282\n--------------------------------------------------\nP-value du test d'unconditional coverage : 0.0019\n--------------------------------------------------\nL'hypothèse nulle est rejetée : Le modèle de VaR ne couvre pas correctement les exceptions.\n----------------------------------------\n\n\n\n\nQue peut-on retenir à ce niveau?\nLa VaR gaussienne calculée sans pondération sous-estime le risque de perte. En effet, le taux de dépassement observé est de 2.21 %, bien supérieur au seuil théorique de 1 % attendu pour une VaR à 99 %.\nPour y remédier, le recours à une VaR gaussienne avec pondération EWMA pourrait être pertinent, car elle permet d’accorder plus d’importance aux observations récentes. Toutefois, lorsqu’on choisit un λ trop élevé, on observe une VaR excessivement conservatrice. Dans ce cas, la mesure de risque devient trop sensible aux rendements récents, au point de perdre en représentativité globale.\nConclusion : Il est donc essentiel de trouver un λ adapté, capable d’assurer un bon compromis entre réactivité et stabilité. Un λ trop faible dilue l’information récente, tandis qu’un λ trop élevé amplifie les variations court terme. Le choix optimal de λ permettrait ainsi une évaluation plus réaliste et efficace du risque de perte."
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#var-skew-student",
    "href": "risques/projets/stat_extreme_corrige.html#var-skew-student",
    "title": "Statistiques des risques extrêmes",
    "section": "VaR skew-Student",
    "text": "VaR skew-Student\n\nEstimation des paramètres d’une loi de Skew Student par maximum de vraisemblance\nNous allons estimer les paramètres d’une loi de Skew Student en utilisant la méthode du maximum de vraisemblance. Nous procéderons par étapes :\n\nDéfinition de la fonction de densité\nFonction de log-vraisemblance\nOptimisation de la log-vraisemblance\n\n\nRappel\nSoit \\(X\\) une variable aléatoire suivant une loi Skew Student, avec les paramètres \\(\\\\mu\\) (moyenne), \\(\\\\sigma\\) (écart-type), \\(\\\\gamma\\) (paramètre de skewness) et \\(\\\\nu\\) (degrés de liberté).\nLa fonction de densité est donnée par la formule suivante :\n\\[\nf_{\\\\{skew-student}}(x) = 2 \\times f(x) \\times F(x)\n\\]\noù :\n\n\\(f(x)\\) est la fonction de densité de la loi de Student avec \\(\\\\nu\\) degrés de liberté,\n\\(F(x)\\) est la fonction de répartition (CDF) de la loi de Student avec \\(\\\\nu + 1\\) degrés de liberté.\n\n\n\nEstimation des paramètres de loi Skew Student sur base d’apprentissage.\n\n\nCode\nimportlib.reload(functions)\nlog_returns = train_data['Log Return'].values  \n\n# Estimation des paramètres optimaux\nparams_optimaux = functions.optimize_parameters(log_returns)\n\nif params_optimaux is not None:\n    mu_opt, sigma_opt, gamma_opt, nu_opt = params_optimaux\n    print(f\"Paramètres estimés de la loi Skew Student :\")\n    print(f\" - Moyenne (mu) : {mu_opt}\")\n    print(f\" - Ecart-type (sigma) : {sigma_opt}\")\n    print(f\" - Skewness (gamma) : {gamma_opt}\")\n    print(f\" - Degrés de liberté (nu) : {nu_opt}\")\nelse:\n    print(\"L'optimisation a échoué.\")\n\n\nParamètres estimés de la loi Skew Student :\n - Moyenne (mu) : 0.002291665868771221\n - Ecart-type (sigma) : 0.008826135314738344\n - Skewness (gamma) : -0.22701817051144937\n - Degrés de liberté (nu) : 2.9803742684956065\n\n\n\nMise en place d’une validation ex-ante de cet ajustement\n\nComparaison des fonctions de densité\n\n\nCode\nimportlib.reload(functions)\nfunctions.plot_skew_student_fit(log_returns, mu_opt, sigma_opt, gamma_opt, nu_opt, functions.f_skew_student)\n\n\n\n\n\n\n\n\n\nLa loi skew-student semble assez bien s’ajuster à nos données. On peut cependant noter que, au niveau des queues de distribution, la densité théorique est légèrement en dessous de la densité empirique.\nMais, ici, la qualité de l’ajustement semble bien meilleure que celle avec la loi normale.\nOn s’intéresse au QQ-plot.\n\nEvaluation de la qualité de l’ajustement aux données via des QQ-plot\n\n\nCode\nimportlib.reload(functions)\nT = len(train_data['Log Return'])\ndf_simulated = functions.skew_student_sim(mu_opt, sigma_opt, gamma_opt, nu_opt, T)\ndf_observed = train_data['Log Return']\n\nfig = functions.qqplot(df_observed, df_simulated)\nplt.show()\n\n\n\n\n\n\n\n\n\nL’observation du QQ-plot, vient confirmer le constat fait précédemment avec les courbes de densités supperposées.\nLa loi skew student serait donc mieux adapté à nos données qu’une loi gaussienne.\nPour que ceci soit plus visuel, nous allons afficher côte à côte les QQ-plots pour ces deux lois.\n\n\n\nComparaison de la qualité de fit entre loi gaussienne et loi de skew Student par analyse graphique.\n\n\nCode\nimportlib.reload(functions)\n\n# Paramètres des lois\nmu_gauss, sigma_gauss = np.mean(train_data['Log Return']), np.std(train_data['Log Return'])\nmu_skew, sigma_skew, gamma_skew, nu_skew = mu_opt, sigma_opt, gamma_opt, nu_opt\n\n# Génération des QQ plots\nfunctions.qqplot_gaussian_skew(train_data['Log Return'], mu_gauss, sigma_gauss, mu_skew, sigma_skew, gamma_skew, nu_skew)\n\n\n\n\n\n\n\n\n\nLa loi de Skew Student colle beaucoup mieux aux rendements extrêmes que la loi normale. Elle s’adapte mieux à la forme réelle de nos données, surtout dans les queues.\n\n\n\nCalcul de la VaR Skew Student sur base d’apprentissage pour alpha = 99%.\n\n\nCode\nimportlib.reload(functions)\nVaR_skew = functions.Var_param_student(train_data['Log Return'], confidence_level=0.99)\n\n\nLa VaR Skewed Student au niveau de confiance 99.0% est de : -0.0370"
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#expected-shortfall",
    "href": "risques/projets/stat_extreme_corrige.html#expected-shortfall",
    "title": "Statistiques des risques extrêmes",
    "section": "Expected Shortfall",
    "text": "Expected Shortfall\nOn s’intéresse à présent à un indicateur plus prudent et informatif que la VaR afin de compléter notre analyse : l’Expected shortfall.\nIl réprésente la perte moyenne attendue en cas de dépassement de la VaR, c’est-à-dire la moyenne des pertes dans les pires cas\n\n\nCalcul de l’ES empirique associé à la mesure la VaR historique sur base d’apprentissage pour alpha = 99%.\n\n\nCode\nimportlib.reload(functions)\nprint(\"La VaR historique (niveau 99%) est : \", round(var_99_hist, 4))\nES_hist =functions.ES_Hist(train_data['Log Return'])\n\n\nLa VaR historique (niveau 99%) est :  -0.0405\nL'Expected Shortfall empirique historique au niveau de confiance 99.0% est : -0.053823\n\n\n\n\n\nCalcul de l’ES empirique associé à la VaR gaussienne sur base d’apprentissage pour alpha = 99%.\n\n\nCode\nimportlib.reload(functions)\nprint(\"La VaR gaussienne (niveau 99%) est : \", round(var_gauss, 4))\nES_emp_gauss = functions.ES_emp_gauss(train_data)\n\n\nLa VaR gaussienne (niveau 99%) est :  -0.0322\nL'Expected Shortfall empirique gaussien au niveau de confiance 99.0% est : -0.044325\n\n\n\n\nL’ES théorique basée sur la VaR gaussienne\n\n\nCode\nimportlib.reload(functions)\nprint(\"La VaR gaussienne (niveau 99%) est : \", round(var_gauss, 4))\nES_gauss = functions.ES_gauss(train_data, confidence_level=0.99)\n\n\nLa VaR gaussienne (niveau 99%) est :  -0.0322\nES théorique gaussien (niveau 99.0%) : -0.036884\n\n\n\n\n\nCalcul de l’ES empirique basé sur la VaR skewed-student à 99%\n\n\nCode\nimportlib.reload(functions)\nES_emp_skew_student = functions.ES_emp_skew_student(train_data)\n\n\nLa VaR Skewed Student au niveau de confiance 99.0% est de : -0.0370\nL'Expected Shortfall empirique skewed-student au niveau de confiance 99.0% est : -0.049202\n\n\n\n\nCalcul de l’ES théorique associée à la VaR Skew student\n\n\nCode\nimportlib.reload(functions)\nprint(\"La VaR skew-student (niveau 99%) est : \", round(VaR_skew, 4))\nES_skew_student = functions.ES_skew_student(mu_opt, sigma_opt, gamma_opt, nu_opt, train_data, VaR_skew)\n\n\nLa VaR skew-student (niveau 99%) est :  -0.037\nL'Expected Shortfall théorique skewed-student au niveau de confiance 99.0% est : -0.056012"
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#protocole-de-backtesting",
    "href": "risques/projets/stat_extreme_corrige.html#protocole-de-backtesting",
    "title": "Statistiques des risques extrêmes",
    "section": "Protocole de backtesting",
    "text": "Protocole de backtesting\n\n\nProposition d’un protocole de backtesting\nNous proposons ci dessous un protocole de backtesting adaptatif pour une VaR gaussienne\n\nProtocole de backtesting adaptatif :\nLe protocole de backtesting proposé vise à vérifier la robustesse de l’estimation de la VaR par une approche paramétrique gaussienne en suivant trois étapes principales :\n\nCalibration initiale :\nLe modèle est initialement calibré sur une période d’apprentissage fixe pour estimer la VaR gaussienne.\nBacktesting ex-post quotidien :\nLe backtesting commence à partir du 30ᵉ jour afin de s’assurer que les hypothèses sous-jacentes des tests utilisés (notamment l’approximation des lois) sont satisfaites. À partir de ce moment-là, la VaR estimée est testée chaque jour sur une période glissante de taille prédéfinie (window_size).\nDeux tests sont réalisés :\n\nUnconditional Coverage Test (UC Test) : Vérifie si la proportion d’exception observés est cohérente au seuil fixé, ici 1%.\n\nIndependence Test (IND Test) : Vérifie si les excès sont indépendants dans le temps.\n\nMécanisme de recalibrage par fenêtre glissante :\n\nRecalibrage déclenché par exception :\nSi l’un des tests échoue (UC Test ou IND Test), un recalibrage est immédiatement déclenché.\nLa période d’entraînement est alors mise à jour par une fenêtre glissante : les observations les plus anciennes sont supprimées tandis que les nouvelles observations récentes sont ajoutées.\nPar exemple, si une exception est détectée au 100ᵉ jour, les 100 premières observations de la période d’entraînement sont supprimées et remplacées par les 100 jours les plus récents des données de test. Cela permet d’actualiser les paramètres du modèle pour mieux refléter les conditions actuelles du marché.\nRecalibrage en absence prolongée d’exception :\nSi aucune exception n’est détectée sur une longue période (max_no_recalib = 252 jours), un recalibrage est également effectué par une fenêtre glissante.\nCela garantit que le modèle reste pertinent même lorsqu’il semble bien fonctionner sur une longue période, évitant ainsi un ajustement excessivement conservateur.\n\n\nCe protocole dynamique assure une adaptation continue du modèle, permettant d’estimer la VaR de manière fiable en s’adaptant aux nouvelles conditions de marché.\n\n\n\nMise en place ce protocole sur les données de test\nPour ce faire, dans la partie dediée au code, on retrouvera :\n\nla fonction : perform_backtest* qui fait conjointement le test d’unconditional coverage et le test d’independance\n\nEnsuite la fonction\n\nla fonction : adaptive_backtesting : Qui réalise le backtesting adaptatif\n\n\n\n\nCode\nimportlib.reload(functions)\n\nresult_var, result_date_recalib, jours_recalibrage = functions.adaptive_backtesting(train_data, test_data)\n\n\nLa VaR est recalibrée à la date 2023-03-16\naprès 165 jours.\n------------------------------------------------------------\nLa valeur de la nouvelle VaR recalibrée est : -0.029898077522017466\n------------------------------------------------------------\n\n\nLa VaR est recalibrée à la date 2023-07-07\naprès 78 jours.\n------------------------------------------------------------\nLa valeur de la nouvelle VaR recalibrée est : -0.02971677178973731\n------------------------------------------------------------\n\n\nLa VaR est recalibrée à la date 2024-04-09\naprès 192 jours.\n------------------------------------------------------------\nLa valeur de la nouvelle VaR recalibrée est : -0.02862514975428418\n------------------------------------------------------------\n\n\n\n\n\nLa théorie des valeurs extrêmes offre également d’importants insights pour le calcul de la Value at Risk.\nNous allons présenter ci après les VaR obtenues via les méthodes dites :\n- Generalized Pareto Distribution (GPD)  qui est une approche par bloc Maxima et \n-  Peak Over Threshold(POT)"
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#var-tve-approche-maxima-par-bloc",
    "href": "risques/projets/stat_extreme_corrige.html#var-tve-approche-maxima-par-bloc",
    "title": "Statistiques des risques extrêmes",
    "section": "VaR TVE : Approche Maxima par bloc",
    "text": "VaR TVE : Approche Maxima par bloc\n\n\nDétermination d’une taille de bloc s et construction d’un échantillon de maxima sur la base d’apprentissage.\nOn choisit s suffisamment grand pour satisfaire les conditions asymptotiques du théorème de Fisher - Tippet mais également suffisamment faible pour obtenir un échantillon de maxima de taille convenable\nDans notre cas, nous avons choisi comme taille de bloc 20\nCompte tenu du fait que nous travaillons sur des données financières, cette taille represente donc des blocs mensuels.\n\n\n\nTracé de Gumbel plot pour juger de l’hypothèse ξ=0 (i.e. GEV vs EV).\nLe Gumbel plot nous permettra de determiner si la distribution adaptée aux données est celle de Gumbel\n\n\nCode\nimportlib.reload(functions)\nfunctions.gumbel_plot(-train_data['Log Return'])\n\n\n\n\n\n\n\n\n\nLa courbe observée, présente une courbure mais, très peu prononcée. Pour être précautionaux, nous allons modéliser une GEV.\n#### Estimation des paramètres de loi GEV\n\nNous avons opté pour des blocs de taille 20, qui correspondent dans notre cas à une période mensuelle.\n\n\nCode\nimportlib.reload(functions)\n# Recuperons les paramètres de la GEV\nblock_max = functions.block_maxima(-train_data)\nshape, loc, scale, neg_log_likelihood_value = functions.fit_gev(block_max)\n\n# Affichons les résultats \nprint(f\"Shape (ξ): {shape}\\n{'-'*50}\")\nprint(f\"Location (μ): {loc}\\n{'-'*50}\")\nprint(f\"Scale (σ): {scale}\\n{'-'*50}\")\nprint(f\"Negative Log-Likelihood: {neg_log_likelihood_value}\\n{'-'*50}\")\n\n\nShape (ξ): -0.1340739024682368\n--------------------------------------------------\nLocation (μ): 0.017788373524147527\n--------------------------------------------------\nScale (σ): 0.009618125539477461\n--------------------------------------------------\nNegative Log-Likelihood: 529.2834672681624\n--------------------------------------------------\n\n\n\n\n\nFaisons une validation ex-ante de notre ajustement\nNous allons pour ce faire, tester l’ajustement de nos données à la loi ajustée en utilisant un QQ plot, un test d’adéquation de kolmogorov smirnov, et un LR test,\nPour voir si le fait d’avoir pris en compre cette courbure très prononcée apporte quelque chose à nos données approte quelque chose, nous allons fait un test de rapport de vraisemblance pour comparer la Gumble à la loi ajustée\n\nQQ-Plot\n\n\n\nCode\nimportlib.reload(functions)\nfunctions.gev_plot(block_max , shape, loc, scale)\n\n\n\n\n\n\n\n\n\nLe QQ-plot suggère que la loi GEV est globalement adaptée aux maxima extraits de nos blocs. Les points soint assez bien alignés sur la premiere bissectrice, ce qui indique que la distribution théorique capture bien le comportement empirique, y compris dans les queues, malgré quelques écarts en très haute valeur.\nNous allons valider celà statistiquement avec un test de Kolmogorov-smirnov au seuil de 5%\n\n\n\nCode\nimportlib.reload(functions)\nfunctions.ks_test_gev(block_max, shape, loc, scale, alpha=0.05)\n\n\n--------------------------------------------------\nTest de Kolmogorov-Smirnov pour la GEV\n\nStatistique KS : 0.0404\np-value        : 0.9238\n\nRésultat : on ne rejette pas H₀ au seuil 0.05.\nLa loi GEV est acceptable.\n\n--------------------------------------------------\n\n\n\nLa courbure du Gumbel plot présenté précédemment etait très peu marquée et nous avons choisi de modéliser une GEV pour être plus précautionneux.\nNous voulons à présent savoir si le gain qu’on obtient en faisant ce choix est significatif. Nous allons donc pour ce faire effectuer un test de comparaison de vraisemblance (LR test):\nOn compare les log vraisemblance de la GEV ajustée sur nos données à celle qu’aurait donné une Gumbel.\n\nPour ce faire, on ajuste d’abord une Gumbel\n\n\nCode\nimportlib.reload(functions)\nloc_g, scale_g, neg_log_likelihood_value_g = functions.fit_gumbel(block_max)\n\n\nOn réalise ensuite notre test de rapport de vraisemblance\n\n\nCode\nimportlib.reload(functions)\nfunctions.LR_test(neg_log_likelihood_value_g, neg_log_likelihood_value)\n\n\n--------------------------------------------------\nTest du rapport de vraisemblance : Gumbel (H₀) vs GEV (H₁)\n\nStatistique LRT : -7.1161\np-value         : 1.0000\n\nRésultat : pas d'amélioration significative par rapport au modèle Gumbel.\n--------------------------------------------------\n\n\nTrue\n\n\nStatistiquement, il n’y a pas d’amelioration d’ajustement qui soit significatif en ajustant une GEV plutôt qu’une Gumbel.\nL’effet d’une GEV est donc marginal.\nNous allons donc considerer la Gumbel pour le calcul de la VaR\n\n\n\nCalcul de la VaR\n\n\nCode\nimportlib.reload(functions)\nVaR_Gumbel = functions.calcul_VaR_Gumbel (loc_g, scale_g, block_size=20)\n\n\nLa VaR Gumbel est : -0.03492178637571426\n----------------------------------------"
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#var-tve-approche-peak-over-threshold",
    "href": "risques/projets/stat_extreme_corrige.html#var-tve-approche-peak-over-threshold",
    "title": "Statistiques des risques extrêmes",
    "section": "VaR TVE : Approche Peak over threshold",
    "text": "VaR TVE : Approche Peak over threshold\n\nEcrivons une fonction permettant d’obtenir le mean excess plot et determinons le seuil u par une analyse graphique\n\n\nCode\nimportlib.reload(functions)\nfunctions.mean_excess_plot(-train_data, u_min=0, u_max=None, step=0.001)\n\n\n\n\n\n\n\n\n\nEn observant le mean excess plot, on opte pour u = 0.028. Le comportement des mean excess au delà de u, sont plus ou moins linéaires. Mais il est important que u soit suffisamment élevé pour respecter les conditions asymptotiques.\nNous optons donc pour cette valeur et voyons les caractéristiques de la loi ajustée\n\n\nEstimation des paramètres de loi GPD\n\n\n\nCode\nimportlib.reload(functions)\nshape, loc, scale =functions.fit_gpd(-train_data['Log Return'].to_numpy(), u = 0.028)\nprint(\"-\" * 50)\nprint(\"Paramètres calibrés de la loi GPD\\n\")\nprint(f\"Shape (ξ)  : {shape:.4f}\")\nprint(f\"Loc        : {loc:.4f}\")\nprint(f\"Scale (σ)  : {scale:.4f}\")\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\nParamètres calibrés de la loi GPD\n\nShape (ξ)  : 0.0782\nLoc        : 0.0000\nScale (σ)  : 0.0115\n--------------------------------------------------\n\n\n\n\nFaire une validation ex-ante (analyse graphiques, QQ-plot, etc.)\nNous utilisons pour ce faire un QQ-plot, un PP-plot et un test de kolmogorov smirnov\n\n\n\nCode\nimportlib.reload(functions)\nu = 0.028\ngpd_validation = functions.gpd_validation(-train_data['Log Return'].to_numpy(), u, shape, scale)\n\n\n\n\n\n\n\n\n\nLe QQ plot et le PP plot affichent un plus ou moins bien ajustement de la loi GPD aux données. Bien qu’elle sous estime quelque peu les valeurs extrêmes.\nCet assez bon ajustement est corroborée par un test statistique de Kolmogorov smirnov qui soutient que la GPD arrive à bien modéliser les excès.\nTest de Kolmogorov_smirnov\n\n\nCode\nimportlib.reload(functions)\nfunctions.gpd_ks_test(-train_data['Log Return'], u, shape, scale, alpha=0.05)\n\n\n--------------------------------------------------\nTest de Kolmogorov-Smirnov pour la loi GPD\n\nStatistique KS : 0.0947\np-value        : 0.2852\n\nRésultat : H₀ non rejetée au seuil 0.05.\nConclusion : la loi GPD est acceptable pour les excès.\n\n--------------------------------------------------\n\n\n\n\nCalculer de la VaR TVE par PoT sur base d’apprentissage pour alpha = 99%.\n\n\nCode\nimportlib.reload(functions)\nvar_tve_pot_result = functions.var_tve_pot(-train_data['Log Return'].to_numpy(), u, shape, scale)\nprint(\"-\" * 50)\nprint(\"la VaR TVE par PoT à un niveau de confiance 99 % est :\\n\")\nprint(f\"VaR TVE  : {var_tve_pot_result :.4f}\")\nprint(\"-\" * 50)\n\n\n--------------------------------------------------\nla VaR TVE par PoT à un niveau de confiance 99 % est :\n\nVaR TVE  : -0.0412\n--------------------------------------------------\n\n\nLe choix du seuil u, étant très subjectif et généralement pas aisé, nous proposons ci après un protocole qui permet une selection automatique d’un bon seuil pour la méthode POT.\n\n\nProposition d’un protocole permettant de calibrer u de manière automatique, et le mettre en œuvre.\n\nAfin de déterminer le seuil optimal, il ne doit pas être trop bas pour éviter d’inclure des données trop fréquentes, ni trop élevé afin de conserver suffisamment d’observations pour une modélisation fiable. Pour cette raison, un seuil minimal est fixé au quantile d’ordre 90% et un seuil maximal au quantile 99%. Avec un pas ajustable de 0.0001, nous explorons la plage de valeurs comprises entre u_min et u_max. Pour chaque seuil, nous estimons les paramètres de la loi GPD. Nous calculons ensuite les écarts absolus entre les estimations successives des paramètres scale et shape. Ces écarts sont additionnés pour chaque seuil afin de mesurer la stabilité des estimations. Le seuil optimal correspond à celui où la somme de ces écarts est minimale, indiquant une stabilité maximale des paramètres estimés.\n\nImplémentation du protocole\n\n\n\nCode\nimportlib.reload(functions)\nu = functions.calibrate_u(-train_data['Log Return'].to_numpy(), alpha=0.99, step=0.0001)\n\n\n\n\nCode\nprint('-'*40)\nprint(\"Le paramètre u optimal dans le cadre de notre exercice présent est\", u)\nprint('-'*40)\n\n\n----------------------------------------\nLe paramètre u optimal dans le cadre de notre exercice présent est 0.02734745956723591\n----------------------------------------"
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#var-garch",
    "href": "risques/projets/stat_extreme_corrige.html#var-garch",
    "title": "Statistiques des risques extrêmes",
    "section": "VaR GARCH",
    "text": "VaR GARCH\n\n\nEtude de l’applicabilité d’un modèle AR[1]-GARCH[1,1] à la série des log-rendements historiques sur base d’apprentissage.\nOn procédera de manière séquentielle : plausibilité d’un AR[1], étude de l’homoscedasticité des résidus de l’AR[1]\n\n\nSpécification d’un AR(1) sur les rendements et analyse des résidus\n\n\nCode\n### Spécification d'un AR(1) et analyse des résidus\n\n\n# Analyse de  l'ACF et du PACF des rendements\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_acf(train_data['Log Return'], lags=20, ax=plt.gca())\nplt.title('ACF of Returns')\n\nplt.subplot(1, 2, 2)\nplot_pacf(train_data['Log Return'], lags=20, ax=plt.gca())\nplt.title('PACF of Returns')\nplt.show()\n\n# Test de stationnarité (ADF test)\nadf_result = adfuller(train_data['Log Return'])\nprint(f\"ADF Statistic: {adf_result[0]}\")\nprint(f\"p-value: {adf_result[1]}\")\nprint(f\"Critical Values: {adf_result[4]}\")\n\n# On ajuste un modèle AR(1) sur nos rendements\nar_model = ARIMA(train_data['Log Return'], order=(1, 0, 0)).fit()\nprint(ar_model.summary())\n\n\n\n\n\n\n\n\n\nADF Statistic: -60.09005708954505\np-value: 0.0\nCritical Values: {'1%': np.float64(-3.4322085838690244), '5%': np.float64(-2.862361217190852), '10%': np.float64(-2.567207147908185)}\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             Log Return   No. Observations:                 3522\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10061.836\nDate:                Wed, 19 Nov 2025   AIC                         -20117.673\nTime:                        09:53:34   BIC                         -20099.173\nSample:                             0   HQIC                        -20111.072\n                               - 3522                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0002      0.000      0.711      0.477      -0.000       0.001\nar.L1         -0.0101      0.012     -0.848      0.397      -0.034       0.013\nsigma2         0.0002   2.15e-06     89.670      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7841.68\nProb(Q):                              0.99   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.28\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.29\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nLes coefficients const et ar.L1 ne sont pas significatifs, ce qui indique qu’un modèle AR(1) ne capture pas bien la dynamique de la série.\nEn s’intéressant notamment aux residus de cette modélisation AR(1) (présentés ci-dessous), il en ressort qu’en effet, ce modèle n’est pas adapté à la dynamique des rendements\n\n\nCode\nresiduals = ar_model.resid\n\n# Plot ACF and PACF of squared residuals\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_acf(residuals**2, lags=20, ax=plt.gca())\nplt.title('ACF of Squared Residuals')\n\nplt.subplot(1, 2, 2)\nplot_pacf(residuals**2, lags=20, ax=plt.gca())\nplt.title('PACF of Squared Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\nL’ACF ci dessus suggère une présence subtantielle d’autocorellation dans les residus du modèle AR(1). Cette observation est validée statistiquement par un test statistique : le test de Ljung box.\n\n\nCode\nljung_box_pvalues_squared = acorr_ljungbox(residuals, lags=[10], return_df=True)\nprint(\"Résultats du test de Ljung-Box sur les résidus  :\")\nprint(ljung_box_pvalues_squared)\n\n\nRésultats du test de Ljung-Box sur les résidus  :\n      lb_stat  lb_pvalue\n10  28.151382   0.001707\n\n\nConclusion 1 :\nLe modèle AR(1) n’est clairement pas celui adapté pour modéliser la dynamique de nos log-rendements. Nous allons explorer d’autres approches.\n\nNous allons faire un ajustement GARCH[1,1] sur les residus du AR(1) précédent et allons analyser le comportement des residus obtenus\n\n\nAjustement d’un GARCH(1,1) sur les résidus de l’AR(1)\n\n\n\nCode\ngarch_model = arch_model(residuals, vol='Garch', p=1, q=1)\ngarch_fit = garch_model.fit()\nprint(garch_fit.summary())\n\n# Plot ACF of GARCH residuals\ngarch_residuals = garch_fit.resid\nplot_acf(garch_residuals, lags=20)\nplt.title('ACF of GARCH Residuals')\nplt.show()\n\n\nIteration:      1,   Func. Count:      6,   Neg. LLF: 3.1059466350498745e+21\nIteration:      2,   Func. Count:     20,   Neg. LLF: 9.739502727265805e+20\nIteration:      3,   Func. Count:     34,   Neg. LLF: 55158141618.326485\nIteration:      4,   Func. Count:     49,   Neg. LLF: 8.03930454804086e+16\nIteration:      5,   Func. Count:     62,   Neg. LLF: 612489316018526.9\nIteration:      6,   Func. Count:     73,   Neg. LLF: -10642.68780877725\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -10642.687814659435\n            Iterations: 10\n            Function evaluations: 73\n            Gradient evaluations: 6\n                     Constant Mean - GARCH Model Results                      \n==============================================================================\nDep. Variable:                   None   R-squared:                       0.000\nMean Model:             Constant Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10642.7\nDistribution:                  Normal   AIC:                          -21277.4\nMethod:            Maximum Likelihood   BIC:                          -21252.7\n                                        No. Observations:                 3522\nDate:                Wed, Nov 19 2025   Df Residuals:                     3521\nTime:                        09:53:34   Df Model:                            1\n                                 Mean Model                                 \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nmu         3.8802e-04  1.491e-04      2.602  9.262e-03 [9.577e-05,6.803e-04]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8648e-06  5.659e-12  6.829e+05      0.000 [3.865e-06,3.865e-06]\nalpha[1]       0.1000  8.159e-03     12.257  1.542e-34   [8.401e-02,  0.116]\nbeta[1]        0.8800  4.814e-03    182.791      0.000     [  0.871,  0.889]\n============================================================================\n\nCovariance estimator: robust\n\n\n\n\n\n\n\n\n\nLes coefficients du GARCH(1,1) sont tous significatifs, de plus , et, en observant l’ACF, on n’observe pas d’autocorellation dans les residus du modèle mis en place.\nCeci, nous amène donc à postuler pour un modèle de type AR(1)- GARCH(1,1) pour modéliser la dynamique des log rendements\n\n\n\nEstimation des paramètres du modèle AR[1]-GARCH[1,1] sur base d’apprentissage\n\n\n\nCode\ncombined_model = arch_model(train_data['Log Return'], mean='AR', lags=1, vol='Garch', p=1, q=1)\ncombined_fit = combined_model.fit()\nprint(combined_fit.summary())\n\n\nIteration:      1,   Func. Count:      7,   Neg. LLF: 2.8568044842874537e+21\nIteration:      2,   Func. Count:     22,   Neg. LLF: 89568496818.99553\nIteration:      3,   Func. Count:     36,   Neg. LLF: -10641.441136322941\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -10641.441141791593\n            Iterations: 7\n            Function evaluations: 36\n            Gradient evaluations: 3\n                           AR - GARCH Model Results                           \n==============================================================================\nDep. Variable:             Log Return   R-squared:                      -0.001\nMean Model:                        AR   Adj. R-squared:                 -0.001\nVol Model:                      GARCH   Log-Likelihood:                10641.4\nDistribution:                  Normal   AIC:                          -21272.9\nMethod:            Maximum Likelihood   BIC:                          -21242.0\n                                        No. Observations:                 3521\nDate:                Wed, Nov 19 2025   Df Residuals:                     3519\nTime:                        09:53:35   Df Model:                            2\n                                   Mean Model                                   \n================================================================================\n                    coef    std err          t      P&gt;|t|       95.0% Conf. Int.\n--------------------------------------------------------------------------------\nConst         5.5801e-04  1.614e-04      3.458  5.448e-04  [2.417e-04,8.743e-04]\nLog Return[1]    -0.0101  1.928e-02     -0.526      0.599 [-4.792e-02,2.765e-02]\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8446e-06  3.945e-12  9.746e+05      0.000 [3.845e-06,3.845e-06]\nalpha[1]       0.1000  1.273e-03     78.545      0.000   [9.750e-02,  0.102]\nbeta[1]        0.8800  3.991e-03    220.516      0.000     [  0.872,  0.888]\n============================================================================\n\nCovariance estimator: robust\n\n\n\n\nVérifions que les résidus de l’AR[1]-GARCH[1,1] sont bien représentatifs d’un bruit blanc i.i.d.\n\n\nOn verifie si les résidus sont décorrelés\n\n\n\nCode\n# Récupérons les résidus standardisés du modèle AR(1)-GARCH(1,1)\n\nresiduals = combined_fit.std_resid  \nresiduals = residuals.dropna()\n# Test d'autocorrélation avec Ljung-Box\nljung_box_pvalues = acorr_ljungbox(residuals, lags=[10], return_df=True)\nljung_box_pvalues_squared = acorr_ljungbox(residuals**2, lags=[10], return_df=True)\nprint(\"Résultats du test de Ljung-Box sur les résidus :\")\nprint(ljung_box_pvalues)\n\n\nRésultats du test de Ljung-Box sur les résidus :\n     lb_stat  lb_pvalue\n10  5.031137   0.889088\n\n\nLa p-value du test de Ljung-box est superieure à 0.05, ce qui signifie que au seuil de 5%, on peut rejetter l’hypothèse d’autocorrelation des residus du modèle postulé.\n\n\nOn s’assure aussi que la volatilité conditionnelle est bien modélisée par le GARCH.\n\nPour ce faire, on va tester l’auto-correlation des residus studentisés élévés au carré\n\n\n\nCode\nljung_box_pvalues_squared = acorr_ljungbox(residuals**2, lags=[10], return_df=True)\nprint(\"Résultats du test de Ljung-Box sur les résidus au carré :\")\nprint(ljung_box_pvalues_squared)\n\n\nRésultats du test de Ljung-Box sur les résidus au carré :\n      lb_stat  lb_pvalue\n10  11.546459   0.316556\n\n\nLa volatilité conditionnelle est bien donc modélisée par le GARCH\n\n\nOn s’assure que ces residus sont identiquement distribués\n\nPour ce faire, on essaie de modéliser la loi qui les régit : On effectue des test de normalité à savoir le test de forme de Jacques Bera et le test de Shapiro-wilk\n\n\n\nCode\n# Tracer la densité (KDE) des résidus\nsns.kdeplot(residuals, shade=True, color='blue')\nplt.title(\"Densité des résidus\")\nplt.xlabel(\"Valeurs des résidus\")\nplt.ylabel(\"Densité\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Tracer le QQ plot\nplt.figure(figsize=(6, 6))\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title(\"QQ Plot des résidus vs Distribution Normale\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#Test de normalité avec Jarque-Bera\njb_stat, jb_pvalue = jarque_bera(residuals)\n\nprint(\"\\nRésultat du test de Jarque-Bera :\")\nprint(f\"JB-stat = {jb_stat:.3f}, p-value = {jb_pvalue:.3f}\")\n\n\n\nRésultat du test de Jarque-Bera :\nJB-stat = 861.889, p-value = 0.000\n\n\n\n\nCode\n# Test de Shapiro-Wilk\nshapiro_stat, shapiro_pvalue = shapiro(residuals)\n\nprint(\"\\nRésultat du test de Shapiro-Wilk :\")\nprint(f\"Statistique W = {shapiro_stat:.3f}, p-value = {shapiro_pvalue:.3f}\")\n\n\n\nRésultat du test de Shapiro-Wilk :\nStatistique W = 0.978, p-value = 0.000\n\n\nAnalyse des resultats relatifs à la distribution des residus\nBien que les outils graphiques suggèrent une distribution normale des residus, les tests statistiques effectuées ont condduit au rejet de cette hypothèse. Ceci suggère que, a priori, les residus ne sont pas identiquement distribués.\nMalgré cela, nous conservons le modèle AR(1)-GARCH(1,1) en raison de sa simplicité d’interprétation et de sa pertinence pédagogique.\nCe modèle constitue un bon point de départ pour la modélisation de la VaR, avec une structure claire où :\n- 𝜔 (omega) représente la volatilité de long terme,\n- 𝛼 (alpha) mesure la réaction immédiate aux chocs récents,\n- 𝛽 (beta) reflète la persistance de la volatilité à travers le temps.\n\n\n\nCode\n### On regarde la forme du combined plot pour notre modèle.\ncombined_fit.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproduisons sur l’ensemble de la période (apprentissage + test) la dynamique historique de μ_t et σ_t selon la dynamique et les valeurs initiales fournies dans le support de cours.\n\n\n\nCode\nmu, phi, omega, a, b = combined_fit.params\ndata = pd.concat([train_data, test_data])\nstd_residuals = combined_fit.std_resid \nalpha = 0.99\nq = std_residuals.quantile(1-alpha) \n\ndata[\"mu\"] = mu + phi*data['Log Return'].shift()\ndata[\"mu\"][0] = mu\n\ndata[\"vol\"] = np.sqrt(omega/(1-a-b))\n\nfor t in range(1, len(data)):\n    data[\"vol\"][t] = np.sqrt(omega + \n                             a*(data['Log Return'][t-1] - data[\"mu\"][t-1])**2 \n                            + b*data[\"vol\"][t-1]**2\n                               )\ndata[\"VaR\"] = data[\"mu\"] + data[\"vol\"]*q\ndata.head(2)\n\n\n\n\n\n\n\n\n\nClose\nLog Return\nVaR_exceedance\nException\nmu\nvol\nVaR\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2008-10-16\n3181.000000\n-0.060997\nTrue\nNaN\n0.000558\n0.013865\n-0.036497\n\n\n2008-10-17\n3329.919922\n0.045753\nFalse\nNaN\n0.001176\n0.023493\n-0.061610\n\n\n\n\n\n\n\n\n\nPour les méthodes historique et GPD de calcul de VaR, nous allons\n\nEstimer la VaR sur les résidus\n  Calculer la VaR dynamique et les exceptions associées sur la base de test\n\n\n\n\nCas de la VaR historique\n\n\n\nCode\nplt.figure(figsize=(12, 6))\n\n# Tracer la VaR et les rendements\nplt.plot(data.index, data[\"VaR\"], label=\"VaR\", color=\"red\", linestyle=\"--\")\nplt.plot(data.index, data['Log Return'], label=\"Rendements\", color=\"blue\")\n\n# Identifier les points où la VaR excède les rendements\nexceedance_points = data[data[\"VaR\"] &gt; data['Log Return']]\n\n# Marquer ces points avec des points rouges\nplt.scatter(exceedance_points.index, exceedance_points['Log Return'], color=\"red\", label=\"Exception\", zorder=5)\n\n# Annoter les points critiques\nfor date, return_value in exceedance_points['Log Return'].items():\n    plt.annotate(date.strftime('%Y-%m-%d'), (date, return_value), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\n# Ajouter des labels et une légende\nplt.title(\"VaR vs Rendements\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Valeur\")\nplt.legend()\nplt.grid()\n\n# Afficher le graphique\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCas de la VaR GPD\n\n\nCode\n# On ajuste un AR(1)_GARCH(1,1)\ncombined_model = arch_model(train_data['Log Return'], mean='AR', lags=1, vol='Garch', p=1, q=1)\ncombined_fit = combined_model.fit()\n\n# On recupère les residus\nstd_residuals = combined_fit.std_resid.dropna().to_numpy()\nstd_residuals = -std_residuals\n\n\nIteration:      1,   Func. Count:      7,   Neg. LLF: 2.8568044842874537e+21\nIteration:      2,   Func. Count:     22,   Neg. LLF: 89568496818.99553\nIteration:      3,   Func. Count:     36,   Neg. LLF: -10641.441136322941\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: -10641.441141791593\n            Iterations: 7\n            Function evaluations: 36\n            Gradient evaluations: 3\n\n\n\n\nCode\nimportlib.reload(functions)\nfunctions.compute_and_plot_var_GPD(std_residuals, alpha, combined_fit, train_data, test_data)"
  },
  {
    "objectID": "risques/projets/stat_extreme_corrige.html#conclusion",
    "href": "risques/projets/stat_extreme_corrige.html#conclusion",
    "title": "Statistiques des risques extrêmes",
    "section": "Conclusion",
    "text": "Conclusion\n\nL’estimation de la Value at Risk (VaR) est une tâche essentielle pour nous qui sommes appelés à être des modélisateur, car elle constitue un indicateur clé qui oriente la prise de decision, des banques, des prestataires de services d’investissement, et même des sociétés de gestion de portefeuilles.\nDans la littérature, plusieurs méthodes permettent d’estimer la Value at Risk. Dans le cadre de ce cours, et plus particulièrement de ce projet, nous avons exploré diverses approches, allant de la méthode gaussienne classique à des modèles plus sophistiqués tels que le modèle AR(1)-GARCH(1,1).\nChacune de ces méthodes a montré ses limites, tant au niveau du calcul du nombre d’exceptions que de l’application du protocole de backtesting.\nEn se concentrant plus spécifiquement sur les modèles de type série temporelle, les tests statistiques réalisés dans le cadre de la modélisation AR(1) — notamment sur les résidus, la distribution et la volatilité — ont mis en évidence deux points clés : les résidus ne suivent pas la même loi de distribution, et la volatilité n’est pas constante.\nCes observations nous ont naturellement conduit au passage à un modèle conditionnel de type AR(1)-GARCH(1,1), plus adapté pour modéliser les variations de la volatilité dans le temps.\nEn combinant ce modèle avec une loi de queue plus souple (la GPD dans notre cas), nous avons pu affiner l’estimation de la VaR, notamment dans les zones extrêmes.\nLa version dynamique de la VaR, fondée sur ce modèle, s’est révélée la plus pertinente : elle s’ajuste mieux aux fluctuations des données et permet d’identifier plus efficacement les périodes de stress.\nCela montre que, bien que ces modèles soient plus sophistiqués et donc plus complexes à implémenter, leur utilisation permet d’obtenir un gain significatif en termes de précision.\nPar ailleurs, ce projet nous a permis d’approfondir notre compréhension des défis associés à l’estimation de la VaR, ainsi que des difficultés rencontrées avec les différentes approches. Cela inclut, par exemple, la détermination de la taille des blocs pour l’approche des blocs maxima, ou encore le choix du seuil optimal dans le cadre de l’approche POT. Nous avons été amenés à pousser la réflexion de manière plus approfondie afin de concevoir nous même des mécanismes permettant de sélectionner automatiquement certains paramètres, optimisant ainsi le processus d’estimation."
  }
]